{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c83d75-8e21-4c49-ae23-004e8f14cc32",
   "metadata": {},
   "source": [
    "Note code is heavily influenced by Sebastian Raschka's [repo](https://github.com/rasbt/LLMs-from-scratch) covering LLMs from scratch with some modifications.\n",
    "Here are some other useful blog posts:\n",
    "- https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07\n",
    "- https://medium.com/@dillipprasad60/qlora-explained-a-deep-dive-into-parametric-efficient-fine-tuning-in-large-language-models-llms-c1a4794b1766 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23037aab-769a-467c-b4c6-a689d3ab51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import parse_sgm_to_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38145ed6-c229-424f-a20b-b224be98667a",
   "metadata": {},
   "source": [
    "## Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4fa00-662c-44e3-9da1-62fc80ddae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_decode_example(list_of_strings):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Get the token ID for <|endoftext|>\n",
    "    endoftext_token = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "    all_tokens = []\n",
    "    for text in list_of_strings:\n",
    "        # Encode the text\n",
    "        encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        all_tokens.extend(encoded + [endoftext_token])\n",
    "\n",
    "    # Decode the tokens\n",
    "    decoded = tokenizer.decode(all_tokens)\n",
    "\n",
    "    return all_tokens, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53cb81-7240-4cfd-9d45-dc3aa0fbe53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, articles, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Get the token ID for <|endoftext|>\n",
    "        endoftext_token = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "        # Tokenize all articles with end-of-text token\n",
    "        all_tokens = []\n",
    "        for article in articles:\n",
    "            article_tokens = tokenizer.encode(article, allowed_special={\"<|endoftext|>\"})\n",
    "            all_tokens.extend(article_tokens + [endoftext_token])\n",
    "\n",
    "        # Use a sliding window to chunk the tokens into overlapping sequences of max_length\n",
    "        for i in range(0, len(all_tokens) - max_length, stride):\n",
    "            input_chunk = all_tokens[i:i + max_length]\n",
    "            target_chunk = all_tokens[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3796d3da-d0c2-48e9-b141-da4277d40833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(articles, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(articles, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e2d9a-fbbc-47b7-a8ee-efc5117dcfca",
   "metadata": {},
   "source": [
    "## Load Reuters dataset\n",
    "\n",
    "#### TODO\n",
    "- Deduplicate articles so that there is 1 per ID, not 1 per topic\n",
    "- Load all the articles, not just the single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0defca-03d3-4a10-ba91-f027bb2e7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_sgm_to_dataframe('../../data/reuters21578/reut2-000.sgm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be9a4a-ec82-48c6-9bec-16c4ff640654",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = df['Body'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2db3b1-0211-40d0-8d01-fe9d9c555f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bbb7a2-529f-438b-bdf1-28ed5b4e98bd",
   "metadata": {},
   "source": [
    "# Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea744551-39ab-4604-a79e-944d9fef760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"google/gemma-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# Prepare your dataset\n",
    "your_string_list = [\"Your first string\", \"Your second string\", \"Your third string\"]  # Replace with your actual data\n",
    "eos_token = tokenizer.eos_token\n",
    "\n",
    "# Join strings with EOS token\n",
    "combined_text = eos_token.join(your_string_list)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": [combined_text]})\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare model for QLoRA fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-2b-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,  # Using the same dataset for simplicity; consider using a separate validation set\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./gemma-2b-finetuned-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28363559-3f43-4eda-9105-d41e1fdc8acb",
   "metadata": {},
   "source": [
    "## Create data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738db129-e6c7-4682-9a14-6be51c9076c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "max_len = 1024\n",
    "context_length = max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e6bb14-7b1e-429d-b728-0e6115451c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "token_embedding_layer = nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc49c66-779f-4194-a21c-a9c838652d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(articles, batch_size=8, max_length=max_length, stride=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0c24a-88b4-4e52-b989-478c666685d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_batch(x, y, n_samples=2):\n",
    "    for i in range(min(n_samples, len(x))):\n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        \n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        \n",
    "        # Decode and print the input sequence\n",
    "        input_text = tokenizer.decode(x[i].tolist())\n",
    "        print(f\"Input text: {input_text}\")\n",
    "        print(f\"Input encoding: {x[i].tolist()}\")\n",
    "        \n",
    "        # Decode and print the target sequence\n",
    "        target_text = tokenizer.decode(y[i].tolist())\n",
    "        print(f\"Target text: {target_text}\")\n",
    "        print(f\"Target encoding: {y[i].tolist()}\")\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e2634e-79b0-4bab-b53b-3d6aa9c584d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSPECT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84c4a1-cfbd-4b73-a397-2a7f2929e8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "\n",
    "    if INSPECT:\n",
    "        # Visual inspection\n",
    "        inspect_batch(x, y)\n",
    "\n",
    "    token_embeddings = token_embedding_layer(x)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d1d95-18b8-4c2f-85b9-4617dfc2c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76117223-25ba-4f78-bcaa-dab91b150584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c7074-4ab3-460f-b42f-c8e6ba0fa6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4175b-dfe3-4846-ba49-08dca54ebab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fd660-e1c0-484e-9c9e-900f7ab13083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d80830e-3399-4600-b85d-a86858a50ece",
   "metadata": {},
   "source": [
    "# Calculating metrics\n",
    "\n",
    "We will use cross-entropy and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268da083-34fa-4318-a918-2e400457b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A couple examples of inputs and targets\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a01657c-dd17-4538-b753-331d3cca3625",
   "metadata": {},
   "source": [
    "Feeding the `inputs` to the model, we obtain the logits vector for the 2 input examples that consist of 3 tokens each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a0021-024a-4992-b12c-65b084b1fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55bd0bf-4472-4662-83ca-95a09d789e85",
   "metadata": {},
   "source": [
    "Since we have 2 input batches with 3 tokens each, we obtain 2 by 3 predicted token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef7fa9-f62c-4516-a07b-c70e9c582525",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ca6c7-a4e9-4882-976b-13a166839c0f",
   "metadata": {},
   "source": [
    "If we decode these tokens, we find that these are quite different from the tokens we want the model to predict, namely the target tokens. This reflects that the model hasn't been trained yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3c6ad-2b54-4f6b-95c3-8925821a965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6229df5c-86b6-4cab-8570-6567c1f3e50b",
   "metadata": {},
   "source": [
    "The token probabilities corresponding to the target indices are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83d452-5626-4432-ab07-507a7dde9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb007f-d921-41ca-a1bf-728ec222fa00",
   "metadata": {},
   "source": [
    "We want to maximize all these values, bringing them close to a probability of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda3aad9-2f61-4ea4-a8d4-de09c82e0a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08825b-af8d-49ca-a4e8-2f7406b8db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a0f2f-314d-4c13-937c-10160ca33785",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be339ccd-44a2-4e62-b9f5-4e5d8eda4870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db571088-59d0-4a87-8605-f20ebbbff971",
   "metadata": {},
   "source": [
    "For the `cross_entropy` function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae19354f-8c40-4362-8c3c-cfcbbe6b0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8688f85d-bebc-48fa-9759-f6904e9b7da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed6b10-6f2f-4a09-808c-3661fce69e9f",
   "metadata": {},
   "source": [
    "- A concept related to the cross-entropy loss is the perplexity of an LLM\n",
    "- The perplexity is simply the exponential of the cross-entropy loss\n",
    "- The perplexity is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that'd be 48,725 words or tokens)\n",
    "- In other words, perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset\n",
    "- Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96059422-6ba1-410e-9375-aab98da1ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78162dc4-c5e9-472b-8ef2-1a56831179a4",
   "metadata": {},
   "source": [
    "## Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af2868c-07f7-4855-964f-e312a3d4c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, text_data = encode_and_decode_example(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ce3b2-cdba-472a-a184-54dce5522d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df3695-86bf-4629-8fd0-57170c27a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559b5f1-f97e-4dc3-9486-0c769012dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 100 characters\n",
    "print(text_data[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb773da0-b1f8-473b-b15e-40aaaa99e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokens)\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0feabc-52f2-43bd-b7a3-39c06b7d0c68",
   "metadata": {},
   "source": [
    "Create the training and validation data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd266ec-bac6-46f1-9cb0-64e83ed1b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(articles))\n",
    "train_data = articles[:split_idx]\n",
    "val_data = articles[split_idx:]\n",
    "\n",
    "# split_idx = int(train_ratio * len(text_data))\n",
    "# train_data = text_data[:split_idx]\n",
    "# val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eaa226-ebd5-4f4a-b6db-754750ffe22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1245aa-1027-4e11-9ce9-c046856272e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748726e-b95c-4d06-bb6b-c0c32b13ea79",
   "metadata": {},
   "source": [
    "An optional check that the data was loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8b93a-8694-493c-9b79-8d74ce031cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce26fe-b73f-4681-95d3-e75139fd5fcd",
   "metadata": {},
   "source": [
    "Another optional check that the token sizes are in the expected ballpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac57645-f908-4b28-930a-53d403df378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95558a-21a1-438e-962c-40c277b1237d",
   "metadata": {},
   "source": [
    "- Next, we implement a utility function to calculate the cross-entropy loss of a given batch\n",
    "- In addition, we implement a second utility function to compute the loss for a user-specified number of batches in a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c1d344-4278-486f-b8d4-dfee81606dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b872e1-f3d5-42ba-9ab4-dd3cf91a2307",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6866283-dc19-42a2-8d36-da8ce0370677",
   "metadata": {},
   "source": [
    "# LLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f58dbb-c3a3-41c1-a354-aedf456e07c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223f4aa-5337-4a91-8711-2aadf17e6006",
   "metadata": {},
   "source": [
    "With the current dataset and settings there are ~350 steps per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ed61c-9f61-4cf9-b8e8-b02cd6aaf383",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58abed9c-21fc-4da4-a958-74f17b3d8793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc53b6-d9d9-41b1-aed7-f021f61dccf4",
   "metadata": {},
   "source": [
    "# Controlling randomness in the text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceab935-0231-4870-8f05-57d99b64562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5043d3-cba3-4f16-bbab-106da11c4024",
   "metadata": {},
   "source": [
    "- Even if we execute the `generate_text_simple` function above multiple times, the LLM will always generate the same outputs\n",
    "- We now introduce two concepts, so-called decoding strategies, to modify the `generate_text_simple`: *temperature scaling* and *top-k* sampling\n",
    "- These will allow the model to control the randomness and diversity of the generated text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4e833-79f5-4eb8-a5e9-d6d2e2a4c863",
   "metadata": {},
   "source": [
    "## Temperature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60efaba-b972-4113-8cef-f6d335fdb977",
   "metadata": {},
   "source": [
    "- Previously, we always sampled the token with the highest probability as the next token using `torch.argmax`\n",
    "- To add variety, we can sample the next token using The `torch.multinomial(probs, num_samples=1)`, sampling from a probability distribution\n",
    "- Here, each index's chance of being picked corresponds to its probability in the input tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4374da-38b5-4e60-9dcc-e7d3587f7527",
   "metadata": {},
   "source": [
    "- Here's a little recap of generating the next token, assuming a very small vocabulary for illustration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3043c278-6240-4b7a-99a8-9617ebb51e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a786d62d-cffa-4d3f-8612-0639133de230",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc8c8d-7a20-4ee0-a291-3bf262131b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f173f-122f-4e2b-b136-bd1595a818eb",
   "metadata": {},
   "source": [
    "- We can control the distribution and selection process via a concept called temperature scaling\n",
    "- \"Temperature scaling\" is just a fancy word for dividing the logits by a number greater than 0\n",
    "- Temperatures greater than 1 will result in more uniformly distributed token probabilities after applying the softmax\n",
    "- Temperatures smaller than 1 will result in more confident (sharper or more peaky) distributions after applying the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a62b1-64f9-43c8-b6b7-11c5fc8d4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dbfccd-ff42-437f-8699-7e341878c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be97fb-8db1-4e6c-b43f-53854888062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc4a61-8f82-44d5-a3d6-013d97f6e64c",
   "metadata": {},
   "source": [
    "The rescaled probabilities via temperature 5 are more uniformly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17fea1a-c9ae-4f24-9397-ccbd77fee9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3c9fd-866a-41fc-aa16-875b1bec695a",
   "metadata": {},
   "source": [
    "## Top k sampling\n",
    "\n",
    "To be able to use higher temperatures to increase output diversity and to reduce the probability of nonsensical sentences, we can restrict the sampled tokens to the top-k most likely tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335251bc-3926-4207-91d6-68e43e09bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fa381-2360-49ed-92d2-146eb6eb578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float('-inf')), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f648c9-edac-4e80-af11-2e39fdc4a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9720a9-e163-40b8-8240-c168770d9683",
   "metadata": {},
   "source": [
    "## Modifying the text generation function\n",
    "\n",
    "- The previous two subsections introduced temperature sampling and top-k sampling\n",
    "- Let's use these two concepts to modify the `generate_simple` function we used to generate text via the LLM earlier, creating a new `generate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f587008-1bb7-46c3-9435-1042e595d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3250ce-b65a-4b58-a6d6-2d93fc906473",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ea726-bd1c-4f51-9960-5d5ea0d24211",
   "metadata": {},
   "source": [
    "## Loading and saving model weights in PyTorch\n",
    "\n",
    "The recommended way in PyTorch is to save the model weights, the so-called `state_dict` via by applying the `torch.save` function to the `.state_dict()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8f544-65b8-4586-af18-85e5d98bccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../models/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42211c64-eb7e-4daf-86b3-f3a596a9cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"../models/model.pth\", map_location=device, weights_only=True))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd4623-325c-4479-91df-5430847500ba",
   "metadata": {},
   "source": [
    "- It's common to train LLMs with adaptive optimizers like Adam or AdamW instead of regular SGD\n",
    "- These adaptive optimizers store additional parameters for each model weight, so it makes sense to save them as well in case we plan to continue the pretraining later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c608bf-fe1c-45a2-8e92-c22d41921ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"../models/model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0dfd7-6a24-4ba2-9a15-c6e606b64f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../models/model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
