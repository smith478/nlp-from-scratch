{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c83d75-8e21-4c49-ae23-004e8f14cc32",
   "metadata": {},
   "source": [
    "Note code is coming from Sebastian Raschka's [repo](https://github.com/rasbt/LLMs-from-scratch) covering LLMs from scratch with slight modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23037aab-769a-467c-b4c6-a689d3ab51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import json_to_dataframe, json_to_string_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38145ed6-c229-424f-a20b-b224be98667a",
   "metadata": {},
   "source": [
    "## Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e4fa00-662c-44e3-9da1-62fc80ddae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_decode_example(list_of_strings):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Get the token ID for <|endoftext|>\n",
    "    endoftext_token = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "    all_tokens = []\n",
    "    for text in list_of_strings:\n",
    "        # Encode the text\n",
    "        encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        all_tokens.extend(encoded + [endoftext_token])\n",
    "\n",
    "    # Decode the tokens\n",
    "    decoded = tokenizer.decode(all_tokens)\n",
    "\n",
    "    return all_tokens, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a53cb81-7240-4cfd-9d45-dc3aa0fbe53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, articles, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Get the token ID for <|endoftext|>\n",
    "        endoftext_token = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "        # Tokenize all articles with end-of-text token\n",
    "        all_tokens = []\n",
    "        for article in articles:\n",
    "            article_tokens = tokenizer.encode(article, allowed_special={\"<|endoftext|>\"})\n",
    "            all_tokens.extend(article_tokens + [endoftext_token])\n",
    "\n",
    "        # Use a sliding window to chunk the tokens into overlapping sequences of max_length\n",
    "        for i in range(0, len(all_tokens) - max_length, stride):\n",
    "            input_chunk = all_tokens[i:i + max_length]\n",
    "            target_chunk = all_tokens[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3796d3da-d0c2-48e9-b141-da4277d40833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(articles, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(articles, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e2d9a-fbbc-47b7-a8ee-efc5117dcfca",
   "metadata": {},
   "source": [
    "## Load radiology reports dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b10af37b-6b85-4e45-9601-34b612fb6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/vector_veterinary_imaging_2.json'\n",
    "\n",
    "df = json_to_dataframe(filepath) \n",
    "rad_strings = json_to_string_list(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28ebf5d3-b68f-4864-b00e-6aaa73ac29c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_identifier</th>\n",
       "      <th>findings</th>\n",
       "      <th>conclusions_and_recommendations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181153</td>\n",
       "      <td>Orthogonal pelvis and orthogonal right shoulde...</td>\n",
       "      <td>1. Medial right mildly comminuted acetabular f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181413</td>\n",
       "      <td>Three view whole body images dated April 14, 2...</td>\n",
       "      <td>The material within the stomach and small inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181821</td>\n",
       "      <td>Three view thoracic radiographs (total of 5 th...</td>\n",
       "      <td>No aggressive osseous changes are noted. The b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181886</td>\n",
       "      <td>Orthogonal images of the right pelvic limb are...</td>\n",
       "      <td>1. Chronic right calcaneal tendonopathy, with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>181911</td>\n",
       "      <td>Lateral abdomen and pelvis images are provided...</td>\n",
       "      <td>1. Numerous small urinary cystoliths, non-obst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>274208</td>\n",
       "      <td>Three view thorax and three view abdomen image...</td>\n",
       "      <td>Aggressive osseous change of the L6 vertebral ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>274229</td>\n",
       "      <td>Orthogonal thorax and three view abdomen image...</td>\n",
       "      <td>Right cranial pulmonary mass. This is most lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>274244</td>\n",
       "      <td>Liver: Diffusely homogenously hyperechoic, oth...</td>\n",
       "      <td>At least one gastric mural nodule extending in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>274249</td>\n",
       "      <td>Ventrodorsal pelvis and orthogonal stifles ima...</td>\n",
       "      <td>Right coxofemoral subluxation, progressive fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>274264</td>\n",
       "      <td>Lateral left and right thoracic limbs, cranioc...</td>\n",
       "      <td>Normal pelvis and thoracic and pelvic limbs. A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1761 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      case_identifier                                           findings  \\\n",
       "0              181153  Orthogonal pelvis and orthogonal right shoulde...   \n",
       "1              181413  Three view whole body images dated April 14, 2...   \n",
       "2              181821  Three view thoracic radiographs (total of 5 th...   \n",
       "3              181886  Orthogonal images of the right pelvic limb are...   \n",
       "4              181911  Lateral abdomen and pelvis images are provided...   \n",
       "...               ...                                                ...   \n",
       "1756           274208  Three view thorax and three view abdomen image...   \n",
       "1757           274229  Orthogonal thorax and three view abdomen image...   \n",
       "1758           274244  Liver: Diffusely homogenously hyperechoic, oth...   \n",
       "1759           274249  Ventrodorsal pelvis and orthogonal stifles ima...   \n",
       "1760           274264  Lateral left and right thoracic limbs, cranioc...   \n",
       "\n",
       "                        conclusions_and_recommendations  \n",
       "0     1. Medial right mildly comminuted acetabular f...  \n",
       "1     The material within the stomach and small inte...  \n",
       "2     No aggressive osseous changes are noted. The b...  \n",
       "3     1. Chronic right calcaneal tendonopathy, with ...  \n",
       "4     1. Numerous small urinary cystoliths, non-obst...  \n",
       "...                                                 ...  \n",
       "1756  Aggressive osseous change of the L6 vertebral ...  \n",
       "1757  Right cranial pulmonary mass. This is most lik...  \n",
       "1758  At least one gastric mural nodule extending in...  \n",
       "1759  Right coxofemoral subluxation, progressive fro...  \n",
       "1760  Normal pelvis and thoracic and pelvic limbs. A...  \n",
       "\n",
       "[1761 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0680ab3c-5529-47bf-ab21-a343548b6722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1761"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rad_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c662dd88-fcb6-4729-8678-499012d60e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Findings: Orthogonal pelvis and orthogonal right shoulder and lateral left shoulder images dated April 17, 2023 are provided for review (total of 5 images). Shoulders: A sagittal plane fracture is present through the right scapular body, where the spine meets the body, extending cranially through the cranial margin of the acromion. this fracture does not articulate with the glenoid rims or the scapulohumeral joint. This fracture is visualized on the craniocaudal image, not visualized on the lateral image, thought due to superimposition. The fracture is non-displaced. Small fissures are suspected extending into the scapular spine. The right first rib is fractured in the body. A non-displaced fracture is also suspected in the body of the right second rib. The visible scapula, scapulohumeral margins, and humerus of the left shoulder are normal. The included cervical and thoracic spine is normal. Pelvis: A mildly comminuted segment fracture is present through the medial and cranial third of the right acetabulum. This fracture is mildly medially displaced and overriding, causing widening of the coxofemoral joint space. A transverse fracture is present through the right pubis. The medial aspect of the right acetabulum is mildly heterogenous and poorly defined though no distinct fracture is identified. The left ischial apophysis is fractured and caudally displaced. The remainder of the pelvis is normal. The femoral heads remain smooth and rounded. The sacroiliac joints are normal. The included lumbar and caudal spine is normal. Conclusions and recommendations: 1. Medial right mildly comminuted acetabular fracture, affecting the weight-bearing surface of the acetabulum. Consultation with an orthopedist is warranted regarding surgical fixation. 2. Longitudinal right scapular fracture, affecting the spine where it inserts onto the body. Surgical fixation may also be warranted of this fracture. 3. Left ischial apophysis avulsion fracture, likely involving avulsion of the left quadriceps origin. 4. Right pubis fracture. 5. Possible right non-displaced ischial fracture. 6. Right first and probable second rib fractures.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rad_strings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28363559-3f43-4eda-9105-d41e1fdc8acb",
   "metadata": {},
   "source": [
    "## Create data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738db129-e6c7-4682-9a14-6be51c9076c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "max_len = 1024\n",
    "context_length = max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38e6bb14-7b1e-429d-b728-0e6115451c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "token_embedding_layer = nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbc49c66-779f-4194-a21c-a9c838652d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(rad_strings, batch_size=8, max_length=max_length, stride=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39d0c24a-88b4-4e52-b989-478c666685d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_batch(x, y, n_samples=2):\n",
    "    for i in range(min(n_samples, len(x))):\n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        \n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        \n",
    "        # Decode and print the input sequence\n",
    "        input_text = tokenizer.decode(x[i].tolist())\n",
    "        print(f\"Input text: {input_text}\")\n",
    "        print(f\"Input encoding: {x[i].tolist()}\")\n",
    "        \n",
    "        # Decode and print the target sequence\n",
    "        target_text = tokenizer.decode(y[i].tolist())\n",
    "        print(f\"Target text: {target_text}\")\n",
    "        print(f\"Target encoding: {y[i].tolist()}\")\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65e2634e-79b0-4bab-b53b-3d6aa9c584d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSPECT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a84c4a1-cfbd-4b73-a397-2a7f2929e8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "Input text: ial carpal bones\n",
      "Input encoding: [498, 1097, 18596, 11945]\n",
      "Target text:  carpal bones and\n",
      "Target encoding: [1097, 18596, 11945, 290]\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 2:\n",
      "Input text:  well as along the\n",
      "Input encoding: [880, 355, 1863, 262]\n",
      "Target text:  as along the lateral\n",
      "Target encoding: [355, 1863, 262, 25653]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "\n",
    "    if INSPECT:\n",
    "        # Visual inspection\n",
    "        inspect_batch(x, y)\n",
    "\n",
    "    token_embeddings = token_embedding_layer(x)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c6d1d95-18b8-4c2f-85b9-4617dfc2c260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c2383-dc43-4f77-be49-560b3d7c5dcb",
   "metadata": {},
   "source": [
    "# Define GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2069516d-cbea-4019-9d5f-b207b5d5f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89bebd56-46e4-4c35-ab21-56587819f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9726f58c-6849-484e-a481-4ed97bb68bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb076600-0858-4476-b2a7-81aa3a474225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bceff59-276c-4501-bd01-c38d6a435579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0dca6-ba38-41f1-aa4c-e7029ac313e7",
   "metadata": {},
   "source": [
    "# Initialize GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1a9ca52-ad89-4762-8d00-732dd1eece7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LENGTH = 256 # default is 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80641b2c-e2f0-4d8a-8a6f-14fa93cf1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": CONTEXT_LENGTH,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "GPT2_CONFIG_355M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": CONTEXT_LENGTH,\n",
    "    \"emb_dim\": 1024,\n",
    "    \"n_heads\": 16,\n",
    "    \"n_layers\": 24,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "GPT2_CONFIG_774M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": CONTEXT_LENGTH,\n",
    "    \"emb_dim\": 1280,\n",
    "    \"n_heads\": 20,\n",
    "    \"n_layers\": 36,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "GPT2_CONFIG_1558M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": CONTEXT_LENGTH,\n",
    "    \"emb_dim\": 1600,\n",
    "    \"n_heads\": 25,\n",
    "    \"n_layers\": 48,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d488eb2d-e5c7-406e-a2dc-828e5a32b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE = '774M'\n",
    "\n",
    "if MODEL_SIZE == '124M':\n",
    "    CONFIG = GPT2_CONFIG_124M\n",
    "    training_batch_size = 8\n",
    "elif MODEL_SIZE == '355M':\n",
    "    CONFIG = GPT2_CONFIG_355M\n",
    "    training_batch_size = 4\n",
    "elif MODEL_SIZE == '774M':\n",
    "    CONFIG = GPT2_CONFIG_774M\n",
    "    training_batch_size = 2\n",
    "elif MODEL_SIZE == '1558M':\n",
    "    CONFIG = GPT2_CONFIG_1558M\n",
    "    training_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "99aca8ce-4745-4150-8f91-000e13b9a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(CONFIG)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "07d5d5d1-7391-4207-95de-9d99768ef67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "892aa126-3dc3-4602-8469-d9d540a1b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"The included lumbar and\"\n",
    "# start_context = \"extending cranially through\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c8a7d217-ce44-48de-908e-25f27cdcb888",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=CONFIG[\"context_length\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc5099-3068-4410-ab8c-87c7fbd0eefc",
   "metadata": {},
   "source": [
    "The model will generate random text prior to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "69ee08cc-6aa7-42b0-9c9b-4230018e39fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " The included lumbar and implementationspack girlfriends 560lishes WSFinding Evertonannawww\n"
     ]
    }
   ],
   "source": [
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed82de-54ce-4462-9084-f10a960dd175",
   "metadata": {},
   "source": [
    "## Explore dataset\n",
    "\n",
    "Look at distribution of token lengths of reports, also look at token length of the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9d596963-2d66-4ede-8289-891bf578928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c0cf8230-b5fd-4b0c-b368-478978a33db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the entire dataset: 650960\n"
     ]
    }
   ],
   "source": [
    "# Calculate token length of entire dataset\n",
    "entire_text = \" \".join(rad_strings)\n",
    "total_tokens = len(tokenizer.encode(entire_text))\n",
    "\n",
    "print(f\"Total tokens in the entire dataset: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5ca61c9f-97db-4d41-8c11-8d85ec39b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token length statistics for individual strings:\n",
      "Mean: 369.65\n",
      "Median: 336.00\n",
      "Minimum: 114\n",
      "Maximum: 1554\n",
      "Standard Deviation: 167.03\n",
      "\n",
      "Percentiles:\n",
      "25th: 255.00\n",
      "50th: 336.00\n",
      "75th: 441.00\n",
      "90th: 565.00\n",
      "95th: 682.00\n",
      "99th: 979.60\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate token lengths for individual strings\n",
    "token_lengths = [len(tokenizer.encode(s)) for s in rad_strings]\n",
    "\n",
    "# Calculate statistics\n",
    "mean_length = np.mean(token_lengths)\n",
    "median_length = np.median(token_lengths)\n",
    "min_length = np.min(token_lengths)\n",
    "max_length = np.max(token_lengths)\n",
    "std_dev = np.std(token_lengths)\n",
    "\n",
    "# Calculate percentiles\n",
    "percentiles = np.percentile(token_lengths, [25, 50, 75, 90, 95, 99])\n",
    "\n",
    "print(\"\\nToken length statistics for individual strings:\")\n",
    "print(f\"Mean: {mean_length:.2f}\")\n",
    "print(f\"Median: {median_length:.2f}\")\n",
    "print(f\"Minimum: {min_length}\")\n",
    "print(f\"Maximum: {max_length}\")\n",
    "print(f\"Standard Deviation: {std_dev:.2f}\")\n",
    "print(\"\\nPercentiles:\")\n",
    "print(f\"25th: {percentiles[0]:.2f}\")\n",
    "print(f\"50th: {percentiles[1]:.2f}\")\n",
    "print(f\"75th: {percentiles[2]:.2f}\")\n",
    "print(f\"90th: {percentiles[3]:.2f}\")\n",
    "print(f\"95th: {percentiles[4]:.2f}\")\n",
    "print(f\"99th: {percentiles[5]:.2f}\")\n",
    "\n",
    "# Optionally, you can create a histogram of token lengths\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(token_lengths, bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Token Lengths')\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80830e-3399-4600-b85d-a86858a50ece",
   "metadata": {},
   "source": [
    "# Calculating metrics\n",
    "\n",
    "We will use cross-entropy and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "268da083-34fa-4318-a918-2e400457b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A couple examples of inputs and targets\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a01657c-dd17-4538-b753-331d3cca3625",
   "metadata": {},
   "source": [
    "Feeding the `inputs` to the model, we obtain the logits vector for the 2 input examples that consist of 3 tokens each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d96a0021-024a-4992-b12c-65b084b1fc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55bd0bf-4472-4662-83ca-95a09d789e85",
   "metadata": {},
   "source": [
    "Since we have 2 input batches with 3 tokens each, we obtain 2 by 3 predicted token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ffef7fa9-f62c-4516-a07b-c70e9c582525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[39172],\n",
      "         [46382],\n",
      "         [44404]],\n",
      "\n",
      "        [[  839],\n",
      "         [38524],\n",
      "         [44668]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ca6c7-a4e9-4882-976b-13a166839c0f",
   "metadata": {},
   "source": [
    "If we decode these tokens, we find that these are quite different from the tokens we want the model to predict, namely the target tokens. This reflects that the model hasn't been trained yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e2c3c6ad-2b54-4f6b-95c3-8925821a965c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1: Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  shaming motel\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6229df5c-86b6-4cab-8570-6567c1f3e50b",
   "metadata": {},
   "source": [
    "The token probabilities corresponding to the target indices are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4f83d452-5626-4432-ab07-507a7dde9133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([1.1897e-05, 9.4235e-06, 1.3234e-05])\n",
      "Text 2: tensor([2.3844e-05, 1.2956e-05, 2.6180e-05])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb007f-d921-41ca-a1bf-728ec222fa00",
   "metadata": {},
   "source": [
    "We want to maximize all these values, bringing them close to a probability of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bda3aad9-2f61-4ea4-a8d4-de09c82e0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-11.3392, -11.5723, -11.2327, -10.6440, -11.2539, -10.5505])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2f08825b-af8d-49ca-a4e8-2f7406b8db0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-11.0988)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3f5a0f2f-314d-4c13-937c-10160ca33785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.0988)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "be339ccd-44a2-4e62-b9f5-4e5d8eda4870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db571088-59d0-4a87-8605-f20ebbbff971",
   "metadata": {},
   "source": [
    "For the `cross_entropy` function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ae19354f-8c40-4362-8c3c-cfcbbe6b0420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8688f85d-bebc-48fa-9759-f6904e9b7da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.0988)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed6b10-6f2f-4a09-808c-3661fce69e9f",
   "metadata": {},
   "source": [
    "- A concept related to the cross-entropy loss is the perplexity of an LLM\n",
    "- The perplexity is simply the exponential of the cross-entropy loss\n",
    "- The perplexity is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that'd be 48,725 words or tokens)\n",
    "- In other words, perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset\n",
    "- Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "96059422-6ba1-410e-9375-aab98da1ff2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(66090.0781)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78162dc4-c5e9-472b-8ef2-1a56831179a4",
   "metadata": {},
   "source": [
    "## Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5af2868c-07f7-4855-964f-e312a3d4c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, text_data = encode_and_decode_example(rad_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d88ce3b2-cdba-472a-a184-54dce5522d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3027781"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "315ceaf5-00b4-4862-b2c0-0da67487cf5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "652721"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c0df3695-86bf-4629-8fd0-57170c27a8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Findings: Orthogonal pelvis and orthogonal right shoulder and lateral left shoulder images dated Apr\n"
     ]
    }
   ],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c559b5f1-f97e-4dc3-9486-0c769012dc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is not appreciated on imaging due to manipulation of the limbs for proper positioning.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Last 100 characters\n",
    "print(text_data[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fb773da0-b1f8-473b-b15e-40aaaa99e86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 3027781\n",
      "Tokens: 652721\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokens)\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0feabc-52f2-43bd-b7a3-39c06b7d0c68",
   "metadata": {},
   "source": [
    "Create the training and validation data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9bd266ec-bac6-46f1-9cb0-64e83ed1b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(rad_strings))\n",
    "train_data = rad_strings[:split_idx]\n",
    "val_data = rad_strings[split_idx:]\n",
    "\n",
    "# split_idx = int(train_ratio * len(text_data))\n",
    "# train_data = text_data[:split_idx]\n",
    "# val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "96eaa226-ebd5-4f4a-b6db-754750ffe22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=training_batch_size,\n",
    "    max_length=CONFIG[\"context_length\"],\n",
    "    stride=CONFIG[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=training_batch_size,\n",
    "    max_length=CONFIG[\"context_length\"],\n",
    "    stride=CONFIG[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4a1245aa-1027-4e11-9ce9-c046856272e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < CONFIG[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `CONFIG['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < CONFIG[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `CONFIG['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748726e-b95c-4d06-bb6b-c0c32b13ea79",
   "metadata": {},
   "source": [
    "An optional check that the data was loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fdb8b93a-8694-493c-9b79-8d74ce031cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Train loader:\")\n",
    "# for x, y in train_loader:\n",
    "#     print(x.shape, y.shape)\n",
    "\n",
    "# print(\"\\nValidation loader:\")\n",
    "# for x, y in val_loader:\n",
    "#     print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce26fe-b73f-4681-95d3-e75139fd5fcd",
   "metadata": {},
   "source": [
    "Another optional check that the token sizes are in the expected ballpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1ac57645-f908-4b28-930a-53d403df378c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 587264\n",
      "Validation tokens: 65024\n",
      "All tokens: 652288\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95558a-21a1-438e-962c-40c277b1237d",
   "metadata": {},
   "source": [
    "- Next, we implement a utility function to calculate the cross-entropy loss of a given batch\n",
    "- In addition, we implement a second utility function to compute the loss for a user-specified number of batches in a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f8c1d344-4278-486f-b8d4-dfee81606dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c8b872e1-f3d5-42ba-9ab4-dd3cf91a2307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.997928215299572\n",
      "Validation loss: 10.994832436869464\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6866283-dc19-42a2-8d36-da8ce0370677",
   "metadata": {},
   "source": [
    "# LLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "04f58dbb-c3a3-41c1-a354-aedf456e07c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223f4aa-5337-4a91-8711-2aadf17e6006",
   "metadata": {},
   "source": [
    "With the current dataset and settings there are ~350 steps per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a33ed61c-9f61-4cf9-b8e8-b02cd6aaf383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.889, Val loss 9.873\n",
      "Ep 1 (Step 000010): Train loss 7.441, Val loss 7.265\n",
      "Ep 1 (Step 000020): Train loss 17.099, Val loss 16.537\n",
      "Ep 1 (Step 000030): Train loss 6.906, Val loss 6.867\n",
      "Ep 1 (Step 000040): Train loss 6.738, Val loss 6.727\n",
      "Ep 1 (Step 000050): Train loss 6.505, Val loss 6.662\n",
      "Ep 1 (Step 000060): Train loss 6.406, Val loss 6.518\n",
      "Ep 1 (Step 000070): Train loss 6.270, Val loss 6.413\n",
      "Ep 1 (Step 000080): Train loss 6.288, Val loss 6.384\n",
      "Ep 1 (Step 000090): Train loss 6.166, Val loss 6.308\n",
      "Ep 1 (Step 000100): Train loss 6.009, Val loss 6.185\n",
      "Ep 1 (Step 000110): Train loss 6.084, Val loss 6.107\n",
      "Ep 1 (Step 000120): Train loss 5.858, Val loss 5.981\n",
      "Ep 1 (Step 000130): Train loss 5.654, Val loss 5.925\n",
      "Ep 1 (Step 000140): Train loss 5.682, Val loss 5.802\n",
      "Ep 1 (Step 000150): Train loss 5.518, Val loss 5.658\n",
      "Ep 1 (Step 000160): Train loss 5.170, Val loss 5.563\n",
      "Ep 1 (Step 000170): Train loss 5.077, Val loss 5.446\n",
      "Ep 1 (Step 000180): Train loss 5.072, Val loss 5.375\n",
      "Ep 1 (Step 000190): Train loss 5.250, Val loss 5.278\n",
      "Ep 1 (Step 000200): Train loss 4.918, Val loss 5.163\n",
      "Ep 1 (Step 000210): Train loss 4.722, Val loss 5.138\n",
      "Ep 1 (Step 000220): Train loss 4.505, Val loss 4.989\n",
      "Ep 1 (Step 000230): Train loss 4.644, Val loss 4.931\n",
      "Ep 1 (Step 000240): Train loss 4.591, Val loss 4.812\n",
      "Ep 1 (Step 000250): Train loss 4.625, Val loss 4.685\n",
      "Ep 1 (Step 000260): Train loss 4.330, Val loss 4.619\n",
      "Ep 1 (Step 000270): Train loss 4.336, Val loss 4.551\n",
      "Ep 1 (Step 000280): Train loss 4.276, Val loss 4.446\n",
      "Ep 1 (Step 000290): Train loss 4.067, Val loss 4.437\n",
      "Ep 1 (Step 000300): Train loss 3.877, Val loss 4.329\n",
      "Ep 1 (Step 000310): Train loss 4.080, Val loss 4.268\n",
      "Ep 1 (Step 000320): Train loss 3.953, Val loss 4.238\n",
      "Ep 1 (Step 000330): Train loss 3.880, Val loss 4.219\n",
      "Ep 1 (Step 000340): Train loss 3.738, Val loss 4.126\n",
      "Ep 1 (Step 000350): Train loss 3.889, Val loss 4.117\n",
      "Ep 1 (Step 000360): Train loss 3.822, Val loss 4.074\n",
      "Ep 1 (Step 000370): Train loss 3.965, Val loss 3.992\n",
      "Ep 1 (Step 000380): Train loss 3.500, Val loss 3.960\n",
      "Ep 1 (Step 000390): Train loss 3.669, Val loss 3.963\n",
      "Ep 1 (Step 000400): Train loss 3.513, Val loss 3.879\n",
      "Ep 1 (Step 000410): Train loss 3.521, Val loss 3.846\n",
      "Ep 1 (Step 000420): Train loss 3.322, Val loss 3.784\n",
      "Ep 1 (Step 000430): Train loss 3.820, Val loss 3.763\n",
      "Ep 1 (Step 000440): Train loss 3.413, Val loss 3.699\n",
      "Ep 1 (Step 000450): Train loss 3.358, Val loss 3.604\n",
      "Ep 1 (Step 000460): Train loss 3.583, Val loss 3.616\n",
      "Ep 1 (Step 000470): Train loss 3.513, Val loss 3.641\n",
      "Ep 1 (Step 000480): Train loss 3.192, Val loss 3.568\n",
      "Ep 1 (Step 000490): Train loss 3.436, Val loss 3.584\n",
      "Ep 1 (Step 000500): Train loss 3.495, Val loss 3.637\n",
      "Ep 1 (Step 000510): Train loss 3.330, Val loss 3.554\n",
      "Ep 1 (Step 000520): Train loss 3.103, Val loss 3.509\n",
      "Ep 1 (Step 000530): Train loss 3.490, Val loss 3.464\n",
      "Ep 1 (Step 000540): Train loss 3.335, Val loss 3.440\n",
      "Ep 1 (Step 000550): Train loss 3.092, Val loss 3.480\n",
      "Ep 1 (Step 000560): Train loss 3.326, Val loss 3.430\n",
      "Ep 1 (Step 000570): Train loss 3.154, Val loss 3.434\n",
      "Ep 1 (Step 000580): Train loss 3.321, Val loss 3.434\n",
      "Ep 1 (Step 000590): Train loss 3.315, Val loss 3.421\n",
      "Ep 1 (Step 000600): Train loss 3.083, Val loss 3.372\n",
      "Ep 1 (Step 000610): Train loss 3.141, Val loss 3.357\n",
      "Ep 1 (Step 000620): Train loss 3.249, Val loss 3.374\n",
      "Ep 1 (Step 000630): Train loss 3.372, Val loss 3.343\n",
      "Ep 1 (Step 000640): Train loss 3.109, Val loss 3.358\n",
      "Ep 1 (Step 000650): Train loss 3.113, Val loss 3.342\n",
      "Ep 1 (Step 000660): Train loss 2.983, Val loss 3.295\n",
      "Ep 1 (Step 000670): Train loss 3.456, Val loss 3.335\n",
      "Ep 1 (Step 000680): Train loss 3.032, Val loss 3.306\n",
      "Ep 1 (Step 000690): Train loss 2.962, Val loss 3.252\n",
      "Ep 1 (Step 000700): Train loss 2.867, Val loss 3.284\n",
      "Ep 1 (Step 000710): Train loss 3.184, Val loss 3.226\n",
      "Ep 1 (Step 000720): Train loss 2.944, Val loss 3.216\n",
      "Ep 1 (Step 000730): Train loss 3.162, Val loss 3.233\n",
      "Ep 1 (Step 000740): Train loss 2.827, Val loss 3.240\n",
      "Ep 1 (Step 000750): Train loss 2.899, Val loss 3.200\n",
      "Ep 1 (Step 000760): Train loss 3.068, Val loss 3.182\n",
      "Ep 1 (Step 000770): Train loss 2.982, Val loss 3.154\n",
      "Ep 1 (Step 000780): Train loss 2.632, Val loss 3.131\n",
      "Ep 1 (Step 000790): Train loss 3.241, Val loss 3.132\n",
      "Ep 1 (Step 000800): Train loss 2.962, Val loss 3.171\n",
      "Ep 1 (Step 000810): Train loss 2.924, Val loss 3.124\n",
      "Ep 1 (Step 000820): Train loss 2.852, Val loss 3.142\n",
      "Ep 1 (Step 000830): Train loss 2.972, Val loss 3.206\n",
      "Ep 1 (Step 000840): Train loss 2.790, Val loss 3.181\n",
      "Ep 1 (Step 000850): Train loss 2.794, Val loss 3.152\n",
      "Ep 1 (Step 000860): Train loss 2.750, Val loss 3.134\n",
      "Ep 1 (Step 000870): Train loss 2.993, Val loss 3.113\n",
      "Ep 1 (Step 000880): Train loss 3.076, Val loss 3.091\n",
      "Ep 1 (Step 000890): Train loss 2.997, Val loss 3.143\n",
      "Ep 1 (Step 000900): Train loss 2.935, Val loss 3.108\n",
      "Ep 1 (Step 000910): Train loss 2.983, Val loss 3.093\n",
      "Ep 1 (Step 000920): Train loss 2.756, Val loss 3.095\n",
      "Ep 1 (Step 000930): Train loss 2.801, Val loss 3.099\n",
      "Ep 1 (Step 000940): Train loss 2.812, Val loss 3.052\n",
      "Ep 1 (Step 000950): Train loss 3.051, Val loss 3.091\n",
      "Ep 1 (Step 000960): Train loss 2.799, Val loss 3.121\n",
      "Ep 1 (Step 000970): Train loss 2.896, Val loss 3.130\n",
      "Ep 1 (Step 000980): Train loss 2.821, Val loss 3.140\n",
      "Ep 1 (Step 000990): Train loss 2.997, Val loss 3.121\n",
      "Ep 1 (Step 001000): Train loss 3.046, Val loss 3.192\n",
      "Ep 1 (Step 001010): Train loss 2.971, Val loss 3.108\n",
      "Ep 1 (Step 001020): Train loss 2.902, Val loss 3.127\n",
      "Ep 1 (Step 001030): Train loss 2.858, Val loss 3.089\n",
      "Ep 1 (Step 001040): Train loss 2.861, Val loss 3.067\n",
      "Ep 1 (Step 001050): Train loss 2.934, Val loss 3.144\n",
      "Ep 1 (Step 001060): Train loss 3.137, Val loss 3.116\n",
      "Ep 1 (Step 001070): Train loss 2.637, Val loss 3.085\n",
      "Ep 1 (Step 001080): Train loss 2.863, Val loss 3.074\n",
      "Ep 1 (Step 001090): Train loss 2.715, Val loss 3.031\n",
      "Ep 1 (Step 001100): Train loss 2.937, Val loss 3.009\n",
      "Ep 1 (Step 001110): Train loss 2.947, Val loss 3.060\n",
      "Ep 1 (Step 001120): Train loss 3.048, Val loss 3.079\n",
      "Ep 1 (Step 001130): Train loss 2.971, Val loss 3.121\n",
      "Ep 1 (Step 001140): Train loss 2.990, Val loss 3.096\n",
      "The included lumbar and car displacement of the left and carpal instability. The cardiac silhouette and pulmonary vasculature is normal in size and pulmonary vasculature are normal in size and mediastinum are normal in size and pulmonary vasculature are normal in size of\n",
      "Ep 2 (Step 001150): Train loss 2.741, Val loss 3.044\n",
      "Ep 2 (Step 001160): Train loss 2.989, Val loss 3.037\n",
      "Ep 2 (Step 001170): Train loss 2.744, Val loss 3.118\n",
      "Ep 2 (Step 001180): Train loss 2.770, Val loss 3.047\n",
      "Ep 2 (Step 001190): Train loss 2.812, Val loss 3.019\n",
      "Ep 2 (Step 001200): Train loss 2.800, Val loss 3.021\n",
      "Ep 2 (Step 001210): Train loss 2.930, Val loss 3.047\n",
      "Ep 2 (Step 001220): Train loss 3.035, Val loss 3.063\n",
      "Ep 2 (Step 001230): Train loss 2.805, Val loss 3.069\n",
      "Ep 2 (Step 001240): Train loss 2.683, Val loss 3.059\n",
      "Ep 2 (Step 001250): Train loss 2.838, Val loss 3.065\n",
      "Ep 2 (Step 001260): Train loss 2.881, Val loss 3.045\n",
      "Ep 2 (Step 001270): Train loss 2.904, Val loss 3.073\n",
      "Ep 2 (Step 001280): Train loss 2.696, Val loss 3.102\n",
      "Ep 2 (Step 001290): Train loss 2.850, Val loss 3.146\n",
      "Ep 2 (Step 001300): Train loss 2.876, Val loss 3.119\n",
      "Ep 2 (Step 001310): Train loss 3.022, Val loss 3.108\n",
      "Ep 2 (Step 001320): Train loss 2.980, Val loss 3.159\n",
      "Ep 2 (Step 001330): Train loss 3.126, Val loss 3.089\n",
      "Ep 2 (Step 001340): Train loss 3.076, Val loss 3.133\n",
      "Ep 2 (Step 001350): Train loss 2.633, Val loss 3.154\n",
      "Ep 2 (Step 001360): Train loss 2.947, Val loss 3.256\n",
      "Ep 2 (Step 001370): Train loss 3.123, Val loss 3.272\n",
      "Ep 2 (Step 001380): Train loss 2.925, Val loss 3.267\n",
      "Ep 2 (Step 001390): Train loss 3.058, Val loss 3.252\n",
      "Ep 2 (Step 001400): Train loss 2.949, Val loss 3.249\n",
      "Ep 2 (Step 001410): Train loss 3.125, Val loss 3.202\n",
      "Ep 2 (Step 001420): Train loss 3.168, Val loss 3.278\n",
      "Ep 2 (Step 001430): Train loss 3.028, Val loss 3.289\n",
      "Ep 2 (Step 001440): Train loss 2.912, Val loss 3.287\n",
      "Ep 2 (Step 001450): Train loss 3.213, Val loss 3.311\n",
      "Ep 2 (Step 001460): Train loss 2.896, Val loss 3.311\n",
      "Ep 2 (Step 001470): Train loss 3.041, Val loss 3.339\n",
      "Ep 2 (Step 001480): Train loss 2.948, Val loss 3.305\n",
      "Ep 2 (Step 001490): Train loss 3.248, Val loss 3.344\n",
      "Ep 2 (Step 001500): Train loss 3.069, Val loss 3.290\n",
      "Ep 2 (Step 001510): Train loss 3.297, Val loss 3.370\n",
      "Ep 2 (Step 001520): Train loss 3.336, Val loss 3.405\n",
      "Ep 2 (Step 001530): Train loss 2.937, Val loss 3.323\n",
      "Ep 2 (Step 001540): Train loss 3.158, Val loss 3.297\n",
      "Ep 2 (Step 001550): Train loss 3.056, Val loss 3.387\n",
      "Ep 2 (Step 001560): Train loss 3.130, Val loss 3.453\n",
      "Ep 2 (Step 001570): Train loss 3.139, Val loss 3.475\n",
      "Ep 2 (Step 001580): Train loss 3.222, Val loss 3.488\n",
      "Ep 2 (Step 001590): Train loss 3.167, Val loss 3.509\n",
      "Ep 2 (Step 001600): Train loss 3.278, Val loss 3.509\n",
      "Ep 2 (Step 001610): Train loss 3.125, Val loss 3.415\n",
      "Ep 2 (Step 001620): Train loss 3.222, Val loss 3.389\n",
      "Ep 2 (Step 001630): Train loss 3.141, Val loss 3.436\n",
      "Ep 2 (Step 001640): Train loss 2.990, Val loss 3.388\n",
      "Ep 2 (Step 001650): Train loss 3.122, Val loss 3.351\n",
      "Ep 2 (Step 001660): Train loss 3.161, Val loss 3.380\n",
      "Ep 2 (Step 001670): Train loss 2.998, Val loss 3.374\n",
      "Ep 2 (Step 001680): Train loss 3.283, Val loss 3.307\n",
      "Ep 2 (Step 001690): Train loss 3.259, Val loss 3.383\n",
      "Ep 2 (Step 001700): Train loss 3.557, Val loss 3.481\n",
      "Ep 2 (Step 001710): Train loss 3.281, Val loss 3.531\n",
      "Ep 2 (Step 001720): Train loss 3.262, Val loss 3.505\n",
      "Ep 2 (Step 001730): Train loss 3.117, Val loss 3.468\n",
      "Ep 2 (Step 001740): Train loss 3.189, Val loss 3.385\n",
      "Ep 2 (Step 001750): Train loss 3.206, Val loss 3.380\n",
      "Ep 2 (Step 001760): Train loss 3.204, Val loss 3.486\n",
      "Ep 2 (Step 001770): Train loss 3.321, Val loss 3.532\n",
      "Ep 2 (Step 001780): Train loss 3.291, Val loss 3.470\n",
      "Ep 2 (Step 001790): Train loss 3.236, Val loss 3.545\n",
      "Ep 2 (Step 001800): Train loss 3.452, Val loss 3.557\n",
      "Ep 2 (Step 001810): Train loss 3.358, Val loss 3.520\n",
      "Ep 2 (Step 001820): Train loss 3.393, Val loss 3.654\n",
      "Ep 2 (Step 001830): Train loss 3.412, Val loss 3.646\n",
      "Ep 2 (Step 001840): Train loss 3.323, Val loss 3.603\n",
      "Ep 2 (Step 001850): Train loss 3.444, Val loss 3.726\n",
      "Ep 2 (Step 001860): Train loss 3.461, Val loss 3.566\n",
      "Ep 2 (Step 001870): Train loss 3.387, Val loss 3.556\n",
      "Ep 2 (Step 001880): Train loss 3.277, Val loss 3.511\n",
      "Ep 2 (Step 001890): Train loss 3.238, Val loss 3.535\n",
      "Ep 2 (Step 001900): Train loss 3.432, Val loss 3.557\n",
      "Ep 2 (Step 001910): Train loss 3.290, Val loss 3.610\n",
      "Ep 2 (Step 001920): Train loss 3.343, Val loss 3.599\n",
      "Ep 2 (Step 001930): Train loss 3.371, Val loss 3.593\n",
      "Ep 2 (Step 001940): Train loss 3.025, Val loss 3.518\n",
      "Ep 2 (Step 001950): Train loss 3.405, Val loss 3.488\n",
      "Ep 2 (Step 001960): Train loss 3.322, Val loss 3.544\n",
      "Ep 2 (Step 001970): Train loss 3.477, Val loss 3.578\n",
      "Ep 2 (Step 001980): Train loss 3.270, Val loss 3.479\n",
      "Ep 2 (Step 001990): Train loss 3.386, Val loss 3.508\n",
      "Ep 2 (Step 002000): Train loss 3.215, Val loss 3.515\n",
      "Ep 2 (Step 002010): Train loss 2.987, Val loss 3.549\n",
      "Ep 2 (Step 002020): Train loss 3.486, Val loss 3.531\n",
      "Ep 2 (Step 002030): Train loss 3.041, Val loss 3.486\n",
      "Ep 2 (Step 002040): Train loss 3.196, Val loss 3.428\n",
      "Ep 2 (Step 002050): Train loss 3.279, Val loss 3.426\n",
      "Ep 2 (Step 002060): Train loss 3.283, Val loss 3.511\n",
      "Ep 2 (Step 002070): Train loss 3.251, Val loss 3.451\n",
      "Ep 2 (Step 002080): Train loss 3.302, Val loss 3.496\n",
      "Ep 2 (Step 002090): Train loss 3.058, Val loss 3.395\n",
      "Ep 2 (Step 002100): Train loss 3.272, Val loss 3.388\n",
      "Ep 2 (Step 002110): Train loss 3.253, Val loss 3.371\n",
      "Ep 2 (Step 002120): Train loss 3.111, Val loss 3.402\n",
      "Ep 2 (Step 002130): Train loss 3.035, Val loss 3.329\n",
      "Ep 2 (Step 002140): Train loss 3.160, Val loss 3.348\n",
      "Ep 2 (Step 002150): Train loss 2.753, Val loss 3.378\n",
      "Ep 2 (Step 002160): Train loss 3.036, Val loss 3.363\n",
      "Ep 2 (Step 002170): Train loss 3.186, Val loss 3.425\n",
      "Ep 2 (Step 002180): Train loss 3.084, Val loss 3.456\n",
      "Ep 2 (Step 002190): Train loss 3.071, Val loss 3.399\n",
      "Ep 2 (Step 002200): Train loss 3.279, Val loss 3.423\n",
      "Ep 2 (Step 002210): Train loss 3.188, Val loss 3.423\n",
      "Ep 2 (Step 002220): Train loss 3.095, Val loss 3.534\n",
      "Ep 2 (Step 002230): Train loss 3.229, Val loss 3.479\n",
      "Ep 2 (Step 002240): Train loss 3.374, Val loss 3.457\n",
      "Ep 2 (Step 002250): Train loss 3.065, Val loss 3.474\n",
      "Ep 2 (Step 002260): Train loss 3.553, Val loss 3.503\n",
      "Ep 2 (Step 002270): Train loss 3.723, Val loss 3.982\n",
      "Ep 2 (Step 002280): Train loss 3.519, Val loss 3.838\n",
      "Ep 2 (Step 002290): Train loss 3.233, Val loss 3.670\n",
      "The included lumbar and., and recommendations: Normal and/orive is. The appearance is normal. The small intestine is likely benign lessasia. The small intestinelyrechoic. The material is likely to the small intestine. The small intestine is likely. The\n",
      "Ep 3 (Step 002300): Train loss 3.343, Val loss 3.636\n",
      "Ep 3 (Step 002310): Train loss 3.562, Val loss 3.787\n",
      "Ep 3 (Step 002320): Train loss 3.409, Val loss 3.744\n",
      "Ep 3 (Step 002330): Train loss 3.533, Val loss 3.739\n",
      "Ep 3 (Step 002340): Train loss 3.715, Val loss 3.840\n",
      "Ep 3 (Step 002350): Train loss 3.458, Val loss 3.712\n",
      "Ep 3 (Step 002360): Train loss 3.438, Val loss 3.621\n",
      "Ep 3 (Step 002370): Train loss 3.379, Val loss 3.533\n",
      "Ep 3 (Step 002380): Train loss 3.420, Val loss 3.556\n",
      "Ep 3 (Step 002390): Train loss 3.335, Val loss 3.602\n",
      "Ep 3 (Step 002400): Train loss 3.217, Val loss 3.570\n",
      "Ep 3 (Step 002410): Train loss 3.490, Val loss 3.633\n",
      "Ep 3 (Step 002420): Train loss 3.361, Val loss 3.590\n",
      "Ep 3 (Step 002430): Train loss 3.202, Val loss 3.585\n",
      "Ep 3 (Step 002440): Train loss 3.450, Val loss 3.635\n",
      "Ep 3 (Step 002450): Train loss 3.560, Val loss 3.815\n",
      "Ep 3 (Step 002460): Train loss 3.617, Val loss 3.784\n",
      "Ep 3 (Step 002470): Train loss 3.641, Val loss 3.679\n",
      "Ep 3 (Step 002480): Train loss 3.513, Val loss 3.624\n",
      "Ep 3 (Step 002490): Train loss 3.457, Val loss 3.618\n",
      "Ep 3 (Step 002500): Train loss 3.435, Val loss 3.626\n",
      "Ep 3 (Step 002510): Train loss 3.315, Val loss 3.738\n",
      "Ep 3 (Step 002520): Train loss 3.565, Val loss 3.845\n",
      "Ep 3 (Step 002530): Train loss 3.252, Val loss 3.777\n",
      "Ep 3 (Step 002540): Train loss 3.676, Val loss 3.800\n",
      "Ep 3 (Step 002550): Train loss 3.615, Val loss 3.848\n",
      "Ep 3 (Step 002560): Train loss 3.847, Val loss 4.015\n",
      "Ep 3 (Step 002570): Train loss 3.621, Val loss 3.784\n",
      "Ep 3 (Step 002580): Train loss 3.713, Val loss 3.659\n",
      "Ep 3 (Step 002590): Train loss 3.473, Val loss 3.596\n",
      "Ep 3 (Step 002600): Train loss 3.089, Val loss 3.537\n",
      "Ep 3 (Step 002610): Train loss 3.155, Val loss 3.604\n",
      "Ep 3 (Step 002620): Train loss 3.384, Val loss 3.643\n",
      "Ep 3 (Step 002630): Train loss 3.252, Val loss 3.702\n",
      "Ep 3 (Step 002640): Train loss 3.397, Val loss 3.744\n",
      "Ep 3 (Step 002650): Train loss 3.271, Val loss 3.699\n",
      "Ep 3 (Step 002660): Train loss 3.414, Val loss 3.714\n",
      "Ep 3 (Step 002670): Train loss 3.132, Val loss 3.629\n",
      "Ep 3 (Step 002680): Train loss 3.360, Val loss 3.557\n",
      "Ep 3 (Step 002690): Train loss 3.364, Val loss 3.522\n",
      "Ep 3 (Step 002700): Train loss 3.359, Val loss 3.456\n",
      "Ep 3 (Step 002710): Train loss 3.287, Val loss 3.444\n",
      "Ep 3 (Step 002720): Train loss 3.131, Val loss 3.465\n",
      "Ep 3 (Step 002730): Train loss 3.375, Val loss 3.475\n",
      "Ep 3 (Step 002740): Train loss 3.345, Val loss 3.440\n",
      "Ep 3 (Step 002750): Train loss 3.201, Val loss 3.431\n",
      "Ep 3 (Step 002760): Train loss 3.331, Val loss 3.480\n",
      "Ep 3 (Step 002770): Train loss 3.081, Val loss 3.419\n",
      "Ep 3 (Step 002780): Train loss 3.286, Val loss 3.368\n",
      "Ep 3 (Step 002790): Train loss 3.243, Val loss 3.451\n",
      "Ep 3 (Step 002800): Train loss 3.165, Val loss 3.421\n",
      "Ep 3 (Step 002810): Train loss 3.059, Val loss 3.424\n",
      "Ep 3 (Step 002820): Train loss 3.061, Val loss 3.433\n",
      "Ep 3 (Step 002830): Train loss 3.188, Val loss 3.477\n",
      "Ep 3 (Step 002840): Train loss 3.513, Val loss 3.417\n",
      "Ep 3 (Step 002850): Train loss 3.225, Val loss 3.429\n",
      "Ep 3 (Step 002860): Train loss 3.384, Val loss 3.498\n",
      "Ep 3 (Step 002870): Train loss 3.310, Val loss 3.455\n",
      "Ep 3 (Step 002880): Train loss 3.376, Val loss 3.447\n",
      "Ep 3 (Step 002890): Train loss 3.147, Val loss 3.456\n",
      "Ep 3 (Step 002900): Train loss 3.441, Val loss 3.497\n",
      "Ep 3 (Step 002910): Train loss 3.243, Val loss 3.516\n",
      "Ep 3 (Step 002920): Train loss 3.567, Val loss 3.447\n",
      "Ep 3 (Step 002930): Train loss 3.191, Val loss 3.477\n",
      "Ep 3 (Step 002940): Train loss 3.306, Val loss 3.475\n",
      "Ep 3 (Step 002950): Train loss 3.353, Val loss 3.478\n",
      "Ep 3 (Step 002960): Train loss 3.212, Val loss 3.472\n",
      "Ep 3 (Step 002970): Train loss 3.319, Val loss 3.551\n",
      "Ep 3 (Step 002980): Train loss 3.201, Val loss 3.461\n",
      "Ep 3 (Step 002990): Train loss 3.172, Val loss 3.474\n",
      "Ep 3 (Step 003000): Train loss 3.281, Val loss 3.491\n",
      "Ep 3 (Step 003010): Train loss 3.318, Val loss 3.557\n",
      "Ep 3 (Step 003020): Train loss 3.125, Val loss 3.497\n",
      "Ep 3 (Step 003030): Train loss 3.150, Val loss 3.542\n",
      "Ep 3 (Step 003040): Train loss 3.262, Val loss 3.449\n",
      "Ep 3 (Step 003050): Train loss 3.097, Val loss 3.456\n",
      "Ep 3 (Step 003060): Train loss 3.159, Val loss 3.524\n",
      "Ep 3 (Step 003070): Train loss 3.345, Val loss 3.556\n",
      "Ep 3 (Step 003080): Train loss 3.266, Val loss 3.545\n",
      "Ep 3 (Step 003090): Train loss 3.292, Val loss 3.647\n",
      "Ep 3 (Step 003100): Train loss 3.461, Val loss 3.629\n",
      "Ep 3 (Step 003110): Train loss 3.247, Val loss 3.576\n",
      "Ep 3 (Step 003120): Train loss 3.136, Val loss 3.530\n",
      "Ep 3 (Step 003130): Train loss 3.220, Val loss 3.481\n",
      "Ep 3 (Step 003140): Train loss 3.201, Val loss 3.500\n",
      "Ep 3 (Step 003150): Train loss 3.182, Val loss 3.541\n",
      "Ep 3 (Step 003160): Train loss 3.307, Val loss 3.467\n",
      "Ep 3 (Step 003170): Train loss 3.179, Val loss 3.488\n",
      "Ep 3 (Step 003180): Train loss 3.419, Val loss 3.621\n",
      "Ep 3 (Step 003190): Train loss 3.139, Val loss 3.537\n",
      "Ep 3 (Step 003200): Train loss 3.428, Val loss 3.509\n",
      "Ep 3 (Step 003210): Train loss 2.981, Val loss 3.454\n",
      "Ep 3 (Step 003220): Train loss 2.883, Val loss 3.376\n",
      "Ep 3 (Step 003230): Train loss 3.124, Val loss 3.411\n",
      "Ep 3 (Step 003240): Train loss 3.124, Val loss 3.404\n",
      "Ep 3 (Step 003250): Train loss 3.067, Val loss 3.358\n",
      "Ep 3 (Step 003260): Train loss 3.118, Val loss 3.428\n",
      "Ep 3 (Step 003270): Train loss 3.291, Val loss 3.435\n",
      "Ep 3 (Step 003280): Train loss 2.929, Val loss 3.525\n",
      "Ep 3 (Step 003290): Train loss 3.250, Val loss 3.493\n",
      "Ep 3 (Step 003300): Train loss 3.088, Val loss 3.456\n",
      "Ep 3 (Step 003310): Train loss 3.421, Val loss 3.462\n",
      "Ep 3 (Step 003320): Train loss 3.207, Val loss 3.537\n",
      "Ep 3 (Step 003330): Train loss 3.246, Val loss 3.478\n",
      "Ep 3 (Step 003340): Train loss 3.106, Val loss 3.566\n",
      "Ep 3 (Step 003350): Train loss 3.218, Val loss 3.527\n",
      "Ep 3 (Step 003360): Train loss 3.309, Val loss 3.462\n",
      "Ep 3 (Step 003370): Train loss 3.262, Val loss 3.435\n",
      "Ep 3 (Step 003380): Train loss 2.881, Val loss 3.443\n",
      "Ep 3 (Step 003390): Train loss 3.101, Val loss 3.388\n",
      "Ep 3 (Step 003400): Train loss 3.090, Val loss 3.414\n",
      "Ep 3 (Step 003410): Train loss 2.999, Val loss 3.420\n",
      "Ep 3 (Step 003420): Train loss 3.058, Val loss 3.319\n",
      "Ep 3 (Step 003430): Train loss 2.958, Val loss 3.343\n",
      "Ep 3 (Step 003440): Train loss 2.947, Val loss 3.340\n",
      "The included lumbar and the images dated 06/2024 are provided. The lungs are well-filled with rounded of the of the right of the of the right lateral image. The cardiac silhouette is normal. The trachea is normal. The trachea is normal\n",
      "Ep 4 (Step 003450): Train loss 3.095, Val loss 3.290\n",
      "Ep 4 (Step 003460): Train loss 2.969, Val loss 3.251\n",
      "Ep 4 (Step 003470): Train loss 3.037, Val loss 3.272\n",
      "Ep 4 (Step 003480): Train loss 2.816, Val loss 3.277\n",
      "Ep 4 (Step 003490): Train loss 3.230, Val loss 3.286\n",
      "Ep 4 (Step 003500): Train loss 2.714, Val loss 3.263\n",
      "Ep 4 (Step 003510): Train loss 3.119, Val loss 3.292\n",
      "Ep 4 (Step 003520): Train loss 3.124, Val loss 3.230\n",
      "Ep 4 (Step 003530): Train loss 2.992, Val loss 3.259\n",
      "Ep 4 (Step 003540): Train loss 3.114, Val loss 3.285\n",
      "Ep 4 (Step 003550): Train loss 3.032, Val loss 3.216\n",
      "Ep 4 (Step 003560): Train loss 3.084, Val loss 3.249\n",
      "Ep 4 (Step 003570): Train loss 2.907, Val loss 3.292\n",
      "Ep 4 (Step 003580): Train loss 3.021, Val loss 3.248\n",
      "Ep 4 (Step 003590): Train loss 2.819, Val loss 3.259\n",
      "Ep 4 (Step 003600): Train loss 3.184, Val loss 3.323\n",
      "Ep 4 (Step 003610): Train loss 2.872, Val loss 3.225\n",
      "Ep 4 (Step 003620): Train loss 3.053, Val loss 3.246\n",
      "Ep 4 (Step 003630): Train loss 2.917, Val loss 3.190\n",
      "Ep 4 (Step 003640): Train loss 3.092, Val loss 3.214\n",
      "Ep 4 (Step 003650): Train loss 3.017, Val loss 3.291\n",
      "Ep 4 (Step 003660): Train loss 2.718, Val loss 3.323\n",
      "Ep 4 (Step 003670): Train loss 3.081, Val loss 3.268\n",
      "Ep 4 (Step 003680): Train loss 2.903, Val loss 3.233\n",
      "Ep 4 (Step 003690): Train loss 2.952, Val loss 3.192\n",
      "Ep 4 (Step 003700): Train loss 3.083, Val loss 3.211\n",
      "Ep 4 (Step 003710): Train loss 2.819, Val loss 3.198\n",
      "Ep 4 (Step 003720): Train loss 3.053, Val loss 3.195\n",
      "Ep 4 (Step 003730): Train loss 2.944, Val loss 3.182\n",
      "Ep 4 (Step 003740): Train loss 3.131, Val loss 3.191\n",
      "Ep 4 (Step 003750): Train loss 2.850, Val loss 3.209\n",
      "Ep 4 (Step 003760): Train loss 3.240, Val loss 3.187\n",
      "Ep 4 (Step 003770): Train loss 3.120, Val loss 3.163\n",
      "Ep 4 (Step 003780): Train loss 2.839, Val loss 3.196\n",
      "Ep 4 (Step 003790): Train loss 2.898, Val loss 3.187\n",
      "Ep 4 (Step 003800): Train loss 2.836, Val loss 3.173\n",
      "Ep 4 (Step 003810): Train loss 2.991, Val loss 3.179\n",
      "Ep 4 (Step 003820): Train loss 2.967, Val loss 3.204\n",
      "Ep 4 (Step 003830): Train loss 2.990, Val loss 3.160\n",
      "Ep 4 (Step 003840): Train loss 3.019, Val loss 3.154\n",
      "Ep 4 (Step 003850): Train loss 2.958, Val loss 3.128\n",
      "Ep 4 (Step 003860): Train loss 2.860, Val loss 3.127\n",
      "Ep 4 (Step 003870): Train loss 2.799, Val loss 3.095\n",
      "Ep 4 (Step 003880): Train loss 2.780, Val loss 3.105\n",
      "Ep 4 (Step 003890): Train loss 2.627, Val loss 3.086\n",
      "Ep 4 (Step 003900): Train loss 2.792, Val loss 3.061\n",
      "Ep 4 (Step 003910): Train loss 2.778, Val loss 3.073\n",
      "Ep 4 (Step 003920): Train loss 2.981, Val loss 3.077\n",
      "Ep 4 (Step 003930): Train loss 2.787, Val loss 3.049\n",
      "Ep 4 (Step 003940): Train loss 2.990, Val loss 3.059\n",
      "Ep 4 (Step 003950): Train loss 2.802, Val loss 3.050\n",
      "Ep 4 (Step 003960): Train loss 2.870, Val loss 3.043\n",
      "Ep 4 (Step 003970): Train loss 2.795, Val loss 3.032\n",
      "Ep 4 (Step 003980): Train loss 2.736, Val loss 3.057\n",
      "Ep 4 (Step 003990): Train loss 2.572, Val loss 3.066\n",
      "Ep 4 (Step 004000): Train loss 2.774, Val loss 3.000\n",
      "Ep 4 (Step 004010): Train loss 2.865, Val loss 3.034\n",
      "Ep 4 (Step 004020): Train loss 2.862, Val loss 3.017\n",
      "Ep 4 (Step 004030): Train loss 2.931, Val loss 3.048\n",
      "Ep 4 (Step 004040): Train loss 2.767, Val loss 3.037\n",
      "Ep 4 (Step 004050): Train loss 2.655, Val loss 3.092\n",
      "Ep 4 (Step 004060): Train loss 2.697, Val loss 3.074\n",
      "Ep 4 (Step 004070): Train loss 2.637, Val loss 3.059\n",
      "Ep 4 (Step 004080): Train loss 2.702, Val loss 3.068\n",
      "Ep 4 (Step 004090): Train loss 2.763, Val loss 3.106\n",
      "Ep 4 (Step 004100): Train loss 2.890, Val loss 3.092\n",
      "Ep 4 (Step 004110): Train loss 2.821, Val loss 3.309\n",
      "Ep 4 (Step 004120): Train loss 2.762, Val loss 3.197\n",
      "Ep 4 (Step 004130): Train loss 2.780, Val loss 3.149\n",
      "Ep 4 (Step 004140): Train loss 2.695, Val loss 3.114\n",
      "Ep 4 (Step 004150): Train loss 2.837, Val loss 3.128\n",
      "Ep 4 (Step 004160): Train loss 2.792, Val loss 3.097\n",
      "Ep 4 (Step 004170): Train loss 2.875, Val loss 3.066\n",
      "Ep 4 (Step 004180): Train loss 2.941, Val loss 3.115\n",
      "Ep 4 (Step 004190): Train loss 2.805, Val loss 3.063\n",
      "Ep 4 (Step 004200): Train loss 2.811, Val loss 3.039\n",
      "Ep 4 (Step 004210): Train loss 2.623, Val loss 3.072\n",
      "Ep 4 (Step 004220): Train loss 2.791, Val loss 3.116\n",
      "Ep 4 (Step 004230): Train loss 2.827, Val loss 3.114\n",
      "Ep 4 (Step 004240): Train loss 2.911, Val loss 3.167\n",
      "Ep 4 (Step 004250): Train loss 2.730, Val loss 3.129\n",
      "Ep 4 (Step 004260): Train loss 2.957, Val loss 3.129\n",
      "Ep 4 (Step 004270): Train loss 2.772, Val loss 3.110\n",
      "Ep 4 (Step 004280): Train loss 2.815, Val loss 3.130\n",
      "Ep 4 (Step 004290): Train loss 3.034, Val loss 3.120\n",
      "Ep 4 (Step 004300): Train loss 2.711, Val loss 3.104\n",
      "Ep 4 (Step 004310): Train loss 2.768, Val loss 3.108\n",
      "Ep 4 (Step 004320): Train loss 2.886, Val loss 3.119\n",
      "Ep 4 (Step 004330): Train loss 2.787, Val loss 3.114\n",
      "Ep 4 (Step 004340): Train loss 2.896, Val loss 3.114\n",
      "Ep 4 (Step 004350): Train loss 2.642, Val loss 3.109\n",
      "Ep 4 (Step 004360): Train loss 2.915, Val loss 3.086\n",
      "Ep 4 (Step 004370): Train loss 2.656, Val loss 3.145\n",
      "Ep 4 (Step 004380): Train loss 2.758, Val loss 3.143\n",
      "Ep 4 (Step 004390): Train loss 2.757, Val loss 3.144\n",
      "Ep 4 (Step 004400): Train loss 3.085, Val loss 3.123\n",
      "Ep 4 (Step 004410): Train loss 2.738, Val loss 3.148\n",
      "Ep 4 (Step 004420): Train loss 2.940, Val loss 3.147\n",
      "Ep 4 (Step 004430): Train loss 2.853, Val loss 3.103\n",
      "Ep 4 (Step 004440): Train loss 2.842, Val loss 3.157\n",
      "Ep 4 (Step 004450): Train loss 2.820, Val loss 3.075\n",
      "Ep 4 (Step 004460): Train loss 2.949, Val loss 3.091\n",
      "Ep 4 (Step 004470): Train loss 2.644, Val loss 3.080\n",
      "Ep 4 (Step 004480): Train loss 2.916, Val loss 3.062\n",
      "Ep 4 (Step 004490): Train loss 2.900, Val loss 3.049\n",
      "Ep 4 (Step 004500): Train loss 2.726, Val loss 3.038\n",
      "Ep 4 (Step 004510): Train loss 2.649, Val loss 3.044\n",
      "Ep 4 (Step 004520): Train loss 2.732, Val loss 3.028\n",
      "Ep 4 (Step 004530): Train loss 2.724, Val loss 3.060\n",
      "Ep 4 (Step 004540): Train loss 2.853, Val loss 3.049\n",
      "Ep 4 (Step 004550): Train loss 2.925, Val loss 3.164\n",
      "Ep 4 (Step 004560): Train loss 2.759, Val loss 3.104\n",
      "Ep 4 (Step 004570): Train loss 2.785, Val loss 3.077\n",
      "Ep 4 (Step 004580): Train loss 2.952, Val loss 3.069\n",
      "The included lumbar and the left lung imagesclusions and recommendations the right lateral. The cardiac silhouette margins of the right lateral margin of the right lateral to the right lateral to the right lateral to the right lateral to the right lateral images. The right lateralofgeal lymph node\n",
      "Ep 5 (Step 004590): Train loss 2.959, Val loss 3.046\n",
      "Ep 5 (Step 004600): Train loss 2.836, Val loss 3.063\n",
      "Ep 5 (Step 004610): Train loss 2.719, Val loss 3.073\n",
      "Ep 5 (Step 004620): Train loss 2.690, Val loss 3.073\n",
      "Ep 5 (Step 004630): Train loss 2.645, Val loss 3.057\n",
      "Ep 5 (Step 004640): Train loss 3.012, Val loss 3.069\n",
      "Ep 5 (Step 004650): Train loss 2.540, Val loss 3.090\n",
      "Ep 5 (Step 004660): Train loss 2.939, Val loss 3.131\n",
      "Ep 5 (Step 004670): Train loss 2.749, Val loss 3.083\n",
      "Ep 5 (Step 004680): Train loss 2.710, Val loss 3.064\n",
      "Ep 5 (Step 004690): Train loss 2.746, Val loss 3.086\n",
      "Ep 5 (Step 004700): Train loss 2.815, Val loss 3.150\n",
      "Ep 5 (Step 004710): Train loss 2.803, Val loss 3.132\n",
      "Ep 5 (Step 004720): Train loss 2.742, Val loss 3.095\n",
      "Ep 5 (Step 004730): Train loss 2.784, Val loss 3.072\n",
      "Ep 5 (Step 004740): Train loss 2.756, Val loss 3.087\n",
      "Ep 5 (Step 004750): Train loss 2.945, Val loss 3.021\n",
      "Ep 5 (Step 004760): Train loss 2.866, Val loss 3.019\n",
      "Ep 5 (Step 004770): Train loss 2.943, Val loss 3.093\n",
      "Ep 5 (Step 004780): Train loss 3.256, Val loss 3.115\n",
      "Ep 5 (Step 004790): Train loss 2.906, Val loss 3.217\n",
      "Ep 5 (Step 004800): Train loss 3.060, Val loss 3.373\n",
      "Ep 5 (Step 004810): Train loss 2.784, Val loss 3.301\n",
      "Ep 5 (Step 004820): Train loss 3.091, Val loss 3.259\n",
      "Ep 5 (Step 004830): Train loss 3.239, Val loss 3.341\n",
      "Ep 5 (Step 004840): Train loss 3.075, Val loss 3.294\n",
      "Ep 5 (Step 004850): Train loss 3.078, Val loss 3.345\n",
      "Ep 5 (Step 004860): Train loss 3.185, Val loss 3.308\n",
      "Ep 5 (Step 004870): Train loss 2.932, Val loss 3.345\n",
      "Ep 5 (Step 004880): Train loss 2.900, Val loss 3.241\n",
      "Ep 5 (Step 004890): Train loss 2.948, Val loss 3.225\n",
      "Ep 5 (Step 004900): Train loss 3.067, Val loss 3.226\n",
      "Ep 5 (Step 004910): Train loss 3.081, Val loss 3.169\n",
      "Ep 5 (Step 004920): Train loss 3.045, Val loss 3.161\n",
      "Ep 5 (Step 004930): Train loss 2.848, Val loss 3.284\n",
      "Ep 5 (Step 004940): Train loss 3.095, Val loss 3.250\n",
      "Ep 5 (Step 004950): Train loss 2.992, Val loss 3.274\n",
      "Ep 5 (Step 004960): Train loss 2.989, Val loss 3.223\n",
      "Ep 5 (Step 004970): Train loss 2.979, Val loss 3.163\n",
      "Ep 5 (Step 004980): Train loss 3.015, Val loss 3.168\n",
      "Ep 5 (Step 004990): Train loss 2.675, Val loss 3.183\n",
      "Ep 5 (Step 005000): Train loss 2.825, Val loss 3.165\n",
      "Ep 5 (Step 005010): Train loss 3.006, Val loss 3.195\n",
      "Ep 5 (Step 005020): Train loss 2.733, Val loss 3.160\n",
      "Ep 5 (Step 005030): Train loss 2.563, Val loss 3.171\n",
      "Ep 5 (Step 005040): Train loss 3.020, Val loss 3.197\n",
      "Ep 5 (Step 005050): Train loss 3.012, Val loss 3.231\n",
      "Ep 5 (Step 005060): Train loss 3.047, Val loss 3.196\n",
      "Ep 5 (Step 005070): Train loss 2.911, Val loss 3.205\n",
      "Ep 5 (Step 005080): Train loss 2.877, Val loss 3.170\n",
      "Ep 5 (Step 005090): Train loss 2.966, Val loss 3.122\n",
      "Ep 5 (Step 005100): Train loss 2.839, Val loss 3.163\n",
      "Ep 5 (Step 005110): Train loss 2.798, Val loss 3.124\n",
      "Ep 5 (Step 005120): Train loss 2.904, Val loss 3.129\n",
      "Ep 5 (Step 005130): Train loss 2.932, Val loss 3.170\n",
      "Ep 5 (Step 005140): Train loss 2.797, Val loss 3.147\n",
      "Ep 5 (Step 005150): Train loss 3.045, Val loss 3.122\n",
      "Ep 5 (Step 005160): Train loss 2.849, Val loss 3.166\n",
      "Ep 5 (Step 005170): Train loss 2.807, Val loss 3.164\n",
      "Ep 5 (Step 005180): Train loss 2.655, Val loss 3.198\n",
      "Ep 5 (Step 005190): Train loss 3.033, Val loss 3.255\n",
      "Ep 5 (Step 005200): Train loss 3.219, Val loss 3.248\n",
      "Ep 5 (Step 005210): Train loss 3.091, Val loss 3.227\n",
      "Ep 5 (Step 005220): Train loss 2.903, Val loss 3.362\n",
      "Ep 5 (Step 005230): Train loss 3.108, Val loss 3.372\n",
      "Ep 5 (Step 005240): Train loss 2.944, Val loss 3.382\n",
      "Ep 5 (Step 005250): Train loss 2.765, Val loss 3.329\n",
      "Ep 5 (Step 005260): Train loss 2.880, Val loss 3.345\n",
      "Ep 5 (Step 005270): Train loss 3.071, Val loss 3.346\n",
      "Ep 5 (Step 005280): Train loss 2.753, Val loss 3.244\n",
      "Ep 5 (Step 005290): Train loss 3.127, Val loss 3.186\n",
      "Ep 5 (Step 005300): Train loss 2.989, Val loss 3.182\n",
      "Ep 5 (Step 005310): Train loss 2.957, Val loss 3.213\n",
      "Ep 5 (Step 005320): Train loss 2.835, Val loss 3.246\n",
      "Ep 5 (Step 005330): Train loss 3.335, Val loss 3.318\n",
      "Ep 5 (Step 005340): Train loss 3.009, Val loss 3.287\n",
      "Ep 5 (Step 005350): Train loss 3.048, Val loss 3.207\n",
      "Ep 5 (Step 005360): Train loss 3.000, Val loss 3.275\n",
      "Ep 5 (Step 005370): Train loss 2.778, Val loss 3.313\n",
      "Ep 5 (Step 005380): Train loss 2.812, Val loss 3.206\n",
      "Ep 5 (Step 005390): Train loss 2.777, Val loss 3.238\n",
      "Ep 5 (Step 005400): Train loss 3.049, Val loss 3.250\n",
      "Ep 5 (Step 005410): Train loss 3.027, Val loss 3.226\n",
      "Ep 5 (Step 005420): Train loss 2.931, Val loss 3.197\n",
      "Ep 5 (Step 005430): Train loss 2.863, Val loss 3.217\n",
      "Ep 5 (Step 005440): Train loss 3.178, Val loss 3.273\n",
      "Ep 5 (Step 005450): Train loss 3.142, Val loss 3.205\n",
      "Ep 5 (Step 005460): Train loss 2.555, Val loss 3.209\n",
      "Ep 5 (Step 005470): Train loss 2.783, Val loss 3.174\n",
      "Ep 5 (Step 005480): Train loss 2.895, Val loss 3.143\n",
      "Ep 5 (Step 005490): Train loss 2.947, Val loss 3.134\n",
      "Ep 5 (Step 005500): Train loss 2.783, Val loss 3.097\n",
      "Ep 5 (Step 005510): Train loss 2.713, Val loss 3.076\n",
      "Ep 5 (Step 005520): Train loss 3.022, Val loss 3.099\n",
      "Ep 5 (Step 005530): Train loss 2.867, Val loss 3.083\n",
      "Ep 5 (Step 005540): Train loss 2.654, Val loss 3.072\n",
      "Ep 5 (Step 005550): Train loss 2.773, Val loss 3.076\n",
      "Ep 5 (Step 005560): Train loss 2.718, Val loss 3.103\n",
      "Ep 5 (Step 005570): Train loss 2.621, Val loss 3.119\n",
      "Ep 5 (Step 005580): Train loss 2.852, Val loss 3.069\n",
      "Ep 5 (Step 005590): Train loss 2.867, Val loss 3.055\n",
      "Ep 5 (Step 005600): Train loss 2.665, Val loss 3.041\n",
      "Ep 5 (Step 005610): Train loss 2.621, Val loss 3.024\n",
      "Ep 5 (Step 005620): Train loss 2.797, Val loss 3.152\n",
      "Ep 5 (Step 005630): Train loss 2.986, Val loss 3.191\n",
      "Ep 5 (Step 005640): Train loss 2.918, Val loss 3.198\n",
      "Ep 5 (Step 005650): Train loss 2.795, Val loss 3.279\n",
      "Ep 5 (Step 005660): Train loss 3.055, Val loss 3.362\n",
      "Ep 5 (Step 005670): Train loss 3.137, Val loss 3.414\n",
      "Ep 5 (Step 005680): Train loss 3.052, Val loss 3.302\n",
      "Ep 5 (Step 005690): Train loss 2.977, Val loss 3.279\n",
      "Ep 5 (Step 005700): Train loss 2.869, Val loss 3.302\n",
      "Ep 5 (Step 005710): Train loss 3.078, Val loss 3.188\n",
      "Ep 5 (Step 005720): Train loss 2.873, Val loss 3.181\n",
      "Ep 5 (Step 005730): Train loss 2.708, Val loss 3.115\n",
      "The included lumbar and normal. The remainder of the right and normal. The remainder is normal. The remainder is normal. The remainder of the right caud musculature is normal. The remainder of the right cervical spine is normal. The remainder of the right caud\n",
      "Ep 6 (Step 005740): Train loss 2.734, Val loss 3.101\n",
      "Ep 6 (Step 005750): Train loss 2.740, Val loss 3.071\n",
      "Ep 6 (Step 005760): Train loss 2.827, Val loss 3.089\n",
      "Ep 6 (Step 005770): Train loss 3.004, Val loss 3.082\n",
      "Ep 6 (Step 005780): Train loss 2.927, Val loss 3.070\n",
      "Ep 6 (Step 005790): Train loss 2.864, Val loss 3.082\n",
      "Ep 6 (Step 005800): Train loss 2.931, Val loss 3.140\n",
      "Ep 6 (Step 005810): Train loss 3.186, Val loss 3.198\n",
      "Ep 6 (Step 005820): Train loss 3.491, Val loss 3.601\n",
      "Ep 6 (Step 005830): Train loss 3.198, Val loss 3.457\n",
      "Ep 6 (Step 005840): Train loss 3.063, Val loss 3.323\n",
      "Ep 6 (Step 005850): Train loss 2.920, Val loss 3.305\n",
      "Ep 6 (Step 005860): Train loss 3.117, Val loss 3.273\n",
      "Ep 6 (Step 005870): Train loss 3.014, Val loss 3.254\n",
      "Ep 6 (Step 005880): Train loss 2.933, Val loss 3.233\n",
      "Ep 6 (Step 005890): Train loss 3.075, Val loss 3.181\n",
      "Ep 6 (Step 005900): Train loss 3.082, Val loss 3.195\n",
      "Ep 6 (Step 005910): Train loss 3.066, Val loss 3.156\n",
      "Ep 6 (Step 005920): Train loss 3.015, Val loss 3.237\n",
      "Ep 6 (Step 005930): Train loss 3.153, Val loss 3.338\n",
      "Ep 6 (Step 005940): Train loss 2.901, Val loss 3.287\n",
      "Ep 6 (Step 005950): Train loss 2.745, Val loss 3.232\n",
      "Ep 6 (Step 005960): Train loss 2.956, Val loss 3.234\n",
      "Ep 6 (Step 005970): Train loss 2.941, Val loss 3.176\n",
      "Ep 6 (Step 005980): Train loss 2.851, Val loss 3.161\n",
      "Ep 6 (Step 005990): Train loss 2.993, Val loss 3.168\n",
      "Ep 6 (Step 006000): Train loss 2.960, Val loss 3.130\n",
      "Ep 6 (Step 006010): Train loss 2.985, Val loss 3.225\n",
      "Ep 6 (Step 006020): Train loss 3.008, Val loss 3.223\n",
      "Ep 6 (Step 006030): Train loss 3.035, Val loss 3.197\n",
      "Ep 6 (Step 006040): Train loss 3.003, Val loss 3.299\n",
      "Ep 6 (Step 006050): Train loss 3.184, Val loss 3.378\n",
      "Ep 6 (Step 006060): Train loss 3.216, Val loss 3.342\n",
      "Ep 6 (Step 006070): Train loss 3.063, Val loss 3.245\n",
      "Ep 6 (Step 006080): Train loss 2.910, Val loss 3.182\n",
      "Ep 6 (Step 006090): Train loss 3.159, Val loss 3.248\n",
      "Ep 6 (Step 006100): Train loss 3.083, Val loss 3.171\n",
      "Ep 6 (Step 006110): Train loss 2.915, Val loss 3.198\n",
      "Ep 6 (Step 006120): Train loss 2.895, Val loss 3.209\n",
      "Ep 6 (Step 006130): Train loss 2.880, Val loss 3.152\n",
      "Ep 6 (Step 006140): Train loss 2.846, Val loss 3.123\n",
      "Ep 6 (Step 006150): Train loss 2.975, Val loss 3.190\n",
      "Ep 6 (Step 006160): Train loss 3.104, Val loss 3.206\n",
      "Ep 6 (Step 006170): Train loss 2.880, Val loss 3.171\n",
      "Ep 6 (Step 006180): Train loss 2.648, Val loss 3.207\n",
      "Ep 6 (Step 006190): Train loss 2.953, Val loss 3.178\n",
      "Ep 6 (Step 006200): Train loss 2.896, Val loss 3.208\n",
      "Ep 6 (Step 006210): Train loss 2.882, Val loss 3.244\n",
      "Ep 6 (Step 006220): Train loss 2.804, Val loss 3.259\n",
      "Ep 6 (Step 006230): Train loss 3.137, Val loss 3.226\n",
      "Ep 6 (Step 006240): Train loss 3.086, Val loss 3.231\n",
      "Ep 6 (Step 006250): Train loss 2.844, Val loss 3.281\n",
      "Ep 6 (Step 006260): Train loss 2.851, Val loss 3.219\n",
      "Ep 6 (Step 006270): Train loss 2.822, Val loss 3.215\n",
      "Ep 6 (Step 006280): Train loss 3.081, Val loss 3.135\n",
      "Ep 6 (Step 006290): Train loss 2.952, Val loss 3.171\n",
      "Ep 6 (Step 006300): Train loss 2.948, Val loss 3.128\n",
      "Ep 6 (Step 006310): Train loss 2.997, Val loss 3.161\n",
      "Ep 6 (Step 006320): Train loss 2.899, Val loss 3.193\n",
      "Ep 6 (Step 006330): Train loss 2.926, Val loss 3.169\n",
      "Ep 6 (Step 006340): Train loss 2.935, Val loss 3.129\n",
      "Ep 6 (Step 006350): Train loss 2.613, Val loss 3.156\n",
      "Ep 6 (Step 006360): Train loss 2.910, Val loss 3.073\n",
      "Ep 6 (Step 006370): Train loss 3.002, Val loss 3.083\n",
      "Ep 6 (Step 006380): Train loss 2.947, Val loss 3.132\n",
      "Ep 6 (Step 006390): Train loss 2.787, Val loss 3.133\n",
      "Ep 6 (Step 006400): Train loss 2.991, Val loss 3.091\n",
      "Ep 6 (Step 006410): Train loss 2.682, Val loss 3.113\n",
      "Ep 6 (Step 006420): Train loss 2.716, Val loss 3.096\n",
      "Ep 6 (Step 006430): Train loss 2.913, Val loss 3.061\n",
      "Ep 6 (Step 006440): Train loss 2.737, Val loss 3.051\n",
      "Ep 6 (Step 006450): Train loss 2.685, Val loss 3.023\n",
      "Ep 6 (Step 006460): Train loss 2.641, Val loss 3.068\n",
      "Ep 6 (Step 006470): Train loss 2.826, Val loss 3.059\n",
      "Ep 6 (Step 006480): Train loss 2.727, Val loss 3.030\n",
      "Ep 6 (Step 006490): Train loss 2.682, Val loss 3.058\n",
      "Ep 6 (Step 006500): Train loss 2.830, Val loss 3.029\n",
      "Ep 6 (Step 006510): Train loss 2.745, Val loss 3.051\n",
      "Ep 6 (Step 006520): Train loss 3.012, Val loss 3.062\n",
      "Ep 6 (Step 006530): Train loss 2.626, Val loss 3.038\n",
      "Ep 6 (Step 006540): Train loss 2.758, Val loss 3.030\n",
      "Ep 6 (Step 006550): Train loss 2.828, Val loss 3.038\n",
      "Ep 6 (Step 006560): Train loss 2.866, Val loss 3.049\n",
      "Ep 6 (Step 006570): Train loss 2.922, Val loss 3.073\n",
      "Ep 6 (Step 006580): Train loss 2.797, Val loss 2.996\n",
      "Ep 6 (Step 006590): Train loss 2.622, Val loss 2.984\n",
      "Ep 6 (Step 006600): Train loss 2.542, Val loss 2.988\n",
      "Ep 6 (Step 006610): Train loss 2.915, Val loss 3.023\n",
      "Ep 6 (Step 006620): Train loss 2.680, Val loss 3.029\n",
      "Ep 6 (Step 006630): Train loss 2.790, Val loss 2.998\n",
      "Ep 6 (Step 006640): Train loss 2.906, Val loss 2.999\n",
      "Ep 6 (Step 006650): Train loss 2.749, Val loss 2.990\n",
      "Ep 6 (Step 006660): Train loss 2.845, Val loss 3.001\n",
      "Ep 6 (Step 006670): Train loss 2.906, Val loss 2.978\n",
      "Ep 6 (Step 006680): Train loss 2.838, Val loss 2.949\n",
      "Ep 6 (Step 006690): Train loss 2.749, Val loss 2.987\n",
      "Ep 6 (Step 006700): Train loss 2.674, Val loss 3.022\n",
      "Ep 6 (Step 006710): Train loss 2.716, Val loss 3.047\n",
      "Ep 6 (Step 006720): Train loss 2.667, Val loss 3.003\n",
      "Ep 6 (Step 006730): Train loss 2.732, Val loss 3.016\n",
      "Ep 6 (Step 006740): Train loss 2.533, Val loss 3.037\n",
      "Ep 6 (Step 006750): Train loss 2.741, Val loss 3.038\n",
      "Ep 6 (Step 006760): Train loss 2.650, Val loss 3.010\n",
      "Ep 6 (Step 006770): Train loss 2.778, Val loss 3.050\n",
      "Ep 6 (Step 006780): Train loss 2.940, Val loss 3.046\n",
      "Ep 6 (Step 006790): Train loss 2.754, Val loss 3.066\n",
      "Ep 6 (Step 006800): Train loss 2.790, Val loss 3.032\n",
      "Ep 6 (Step 006810): Train loss 2.592, Val loss 3.051\n",
      "Ep 6 (Step 006820): Train loss 2.609, Val loss 3.033\n",
      "Ep 6 (Step 006830): Train loss 2.821, Val loss 3.030\n",
      "Ep 6 (Step 006840): Train loss 2.691, Val loss 3.039\n",
      "Ep 6 (Step 006850): Train loss 2.626, Val loss 3.074\n",
      "Ep 6 (Step 006860): Train loss 2.707, Val loss 3.099\n",
      "Ep 6 (Step 006870): Train loss 2.818, Val loss 3.138\n",
      "Ep 6 (Step 006880): Train loss 2.922, Val loss 3.174\n",
      "The included lumbar and is present in the caudal lung spine. The remainder of the caudal thorax areacheal and the caudal spine is normal. The tracheal abdomen is normal in position. The tracheal abdomen and principal bronchi\n",
      "Ep 7 (Step 006890): Train loss 2.778, Val loss 3.148\n",
      "Ep 7 (Step 006900): Train loss 2.748, Val loss 3.109\n",
      "Ep 7 (Step 006910): Train loss 2.714, Val loss 3.141\n",
      "Ep 7 (Step 006920): Train loss 2.904, Val loss 3.124\n",
      "Ep 7 (Step 006930): Train loss 2.678, Val loss 3.083\n",
      "Ep 7 (Step 006940): Train loss 2.740, Val loss 3.091\n",
      "Ep 7 (Step 006950): Train loss 2.828, Val loss 3.026\n",
      "Ep 7 (Step 006960): Train loss 2.757, Val loss 3.031\n",
      "Ep 7 (Step 006970): Train loss 2.633, Val loss 3.022\n",
      "Ep 7 (Step 006980): Train loss 2.511, Val loss 3.068\n",
      "Ep 7 (Step 006990): Train loss 2.917, Val loss 3.010\n",
      "Ep 7 (Step 007000): Train loss 2.937, Val loss 2.960\n",
      "Ep 7 (Step 007010): Train loss 2.960, Val loss 3.202\n",
      "Ep 7 (Step 007020): Train loss 2.799, Val loss 3.204\n",
      "Ep 7 (Step 007030): Train loss 3.006, Val loss 3.157\n",
      "Ep 7 (Step 007040): Train loss 2.949, Val loss 3.114\n",
      "Ep 7 (Step 007050): Train loss 2.700, Val loss 3.071\n",
      "Ep 7 (Step 007060): Train loss 2.589, Val loss 3.045\n",
      "Ep 7 (Step 007070): Train loss 2.654, Val loss 3.005\n",
      "Ep 7 (Step 007080): Train loss 2.660, Val loss 3.020\n",
      "Ep 7 (Step 007090): Train loss 2.784, Val loss 3.054\n",
      "Ep 7 (Step 007100): Train loss 2.671, Val loss 3.008\n",
      "Ep 7 (Step 007110): Train loss 2.659, Val loss 2.998\n",
      "Ep 7 (Step 007120): Train loss 2.751, Val loss 2.935\n",
      "Ep 7 (Step 007130): Train loss 2.698, Val loss 2.948\n",
      "Ep 7 (Step 007140): Train loss 2.759, Val loss 2.957\n",
      "Ep 7 (Step 007150): Train loss 2.731, Val loss 2.951\n",
      "Ep 7 (Step 007160): Train loss 2.771, Val loss 2.959\n",
      "Ep 7 (Step 007170): Train loss 2.578, Val loss 2.921\n",
      "Ep 7 (Step 007180): Train loss 2.675, Val loss 2.928\n",
      "Ep 7 (Step 007190): Train loss 2.697, Val loss 2.952\n",
      "Ep 7 (Step 007200): Train loss 2.564, Val loss 2.983\n",
      "Ep 7 (Step 007210): Train loss 2.601, Val loss 2.952\n",
      "Ep 7 (Step 007220): Train loss 2.503, Val loss 2.978\n",
      "Ep 7 (Step 007230): Train loss 2.726, Val loss 2.932\n",
      "Ep 7 (Step 007240): Train loss 2.681, Val loss 2.937\n",
      "Ep 7 (Step 007250): Train loss 2.660, Val loss 2.908\n",
      "Ep 7 (Step 007260): Train loss 2.611, Val loss 2.867\n",
      "Ep 7 (Step 007270): Train loss 2.614, Val loss 2.878\n",
      "Ep 7 (Step 007280): Train loss 2.674, Val loss 2.929\n",
      "Ep 7 (Step 007290): Train loss 2.612, Val loss 2.949\n",
      "Ep 7 (Step 007300): Train loss 2.740, Val loss 3.036\n",
      "Ep 7 (Step 007310): Train loss 2.748, Val loss 3.044\n",
      "Ep 7 (Step 007320): Train loss 2.696, Val loss 3.015\n",
      "Ep 7 (Step 007330): Train loss 2.787, Val loss 3.006\n",
      "Ep 7 (Step 007340): Train loss 2.644, Val loss 3.019\n",
      "Ep 7 (Step 007350): Train loss 2.597, Val loss 2.999\n",
      "Ep 7 (Step 007360): Train loss 2.635, Val loss 2.988\n",
      "Ep 7 (Step 007370): Train loss 2.562, Val loss 2.994\n",
      "Ep 7 (Step 007380): Train loss 2.515, Val loss 3.036\n",
      "Ep 7 (Step 007390): Train loss 2.567, Val loss 3.042\n",
      "Ep 7 (Step 007400): Train loss 2.529, Val loss 3.060\n",
      "Ep 7 (Step 007410): Train loss 2.704, Val loss 2.972\n",
      "Ep 7 (Step 007420): Train loss 2.425, Val loss 2.953\n",
      "Ep 7 (Step 007430): Train loss 2.646, Val loss 2.932\n",
      "Ep 7 (Step 007440): Train loss 2.479, Val loss 2.930\n",
      "Ep 7 (Step 007450): Train loss 2.530, Val loss 2.957\n",
      "Ep 7 (Step 007460): Train loss 2.700, Val loss 2.952\n",
      "Ep 7 (Step 007470): Train loss 2.559, Val loss 2.947\n",
      "Ep 7 (Step 007480): Train loss 2.753, Val loss 2.963\n",
      "Ep 7 (Step 007490): Train loss 2.713, Val loss 2.893\n",
      "Ep 7 (Step 007500): Train loss 2.518, Val loss 2.880\n",
      "Ep 7 (Step 007510): Train loss 2.516, Val loss 2.890\n",
      "Ep 7 (Step 007520): Train loss 2.654, Val loss 2.881\n",
      "Ep 7 (Step 007530): Train loss 2.578, Val loss 2.888\n",
      "Ep 7 (Step 007540): Train loss 2.601, Val loss 2.852\n",
      "Ep 7 (Step 007550): Train loss 2.637, Val loss 2.823\n",
      "Ep 7 (Step 007560): Train loss 2.589, Val loss 2.808\n",
      "Ep 7 (Step 007570): Train loss 2.507, Val loss 2.779\n",
      "Ep 7 (Step 007580): Train loss 2.644, Val loss 2.785\n",
      "Ep 7 (Step 007590): Train loss 2.433, Val loss 2.832\n",
      "Ep 7 (Step 007600): Train loss 2.607, Val loss 2.850\n",
      "Ep 7 (Step 007610): Train loss 2.428, Val loss 2.864\n",
      "Ep 7 (Step 007620): Train loss 2.619, Val loss 2.857\n",
      "Ep 7 (Step 007630): Train loss 2.607, Val loss 2.869\n",
      "Ep 7 (Step 007640): Train loss 2.678, Val loss 2.881\n",
      "Ep 7 (Step 007650): Train loss 2.599, Val loss 2.834\n",
      "Ep 7 (Step 007660): Train loss 2.675, Val loss 2.815\n",
      "Ep 7 (Step 007670): Train loss 2.412, Val loss 2.852\n",
      "Ep 7 (Step 007680): Train loss 2.723, Val loss 2.832\n",
      "Ep 7 (Step 007690): Train loss 2.606, Val loss 2.839\n",
      "Ep 7 (Step 007700): Train loss 2.412, Val loss 2.855\n",
      "Ep 7 (Step 007710): Train loss 2.473, Val loss 2.813\n",
      "Ep 7 (Step 007720): Train loss 2.554, Val loss 2.839\n",
      "Ep 7 (Step 007730): Train loss 2.496, Val loss 2.851\n",
      "Ep 7 (Step 007740): Train loss 2.776, Val loss 2.867\n",
      "Ep 7 (Step 007750): Train loss 2.672, Val loss 2.941\n",
      "Ep 7 (Step 007760): Train loss 2.664, Val loss 2.896\n",
      "Ep 7 (Step 007770): Train loss 2.829, Val loss 2.899\n",
      "Ep 7 (Step 007780): Train loss 2.521, Val loss 2.979\n",
      "Ep 7 (Step 007790): Train loss 2.658, Val loss 2.880\n",
      "Ep 7 (Step 007800): Train loss 2.440, Val loss 2.894\n",
      "Ep 7 (Step 007810): Train loss 2.563, Val loss 2.861\n",
      "Ep 7 (Step 007820): Train loss 2.611, Val loss 2.874\n",
      "Ep 7 (Step 007830): Train loss 2.448, Val loss 2.876\n",
      "Ep 7 (Step 007840): Train loss 2.662, Val loss 2.864\n",
      "Ep 7 (Step 007850): Train loss 2.580, Val loss 2.852\n",
      "Ep 7 (Step 007860): Train loss 2.470, Val loss 2.827\n",
      "Ep 7 (Step 007870): Train loss 2.546, Val loss 2.808\n",
      "Ep 7 (Step 007880): Train loss 2.690, Val loss 2.849\n",
      "Ep 7 (Step 007890): Train loss 2.453, Val loss 2.805\n",
      "Ep 7 (Step 007900): Train loss 2.512, Val loss 2.775\n",
      "Ep 7 (Step 007910): Train loss 2.666, Val loss 2.743\n",
      "Ep 7 (Step 007920): Train loss 2.464, Val loss 2.783\n",
      "Ep 7 (Step 007930): Train loss 2.364, Val loss 2.785\n",
      "Ep 7 (Step 007940): Train loss 2.634, Val loss 2.838\n",
      "Ep 7 (Step 007950): Train loss 2.518, Val loss 2.858\n",
      "Ep 7 (Step 007960): Train loss 2.774, Val loss 2.868\n",
      "Ep 7 (Step 007970): Train loss 2.599, Val loss 2.796\n",
      "Ep 7 (Step 007980): Train loss 2.592, Val loss 2.779\n",
      "Ep 7 (Step 007990): Train loss 2.377, Val loss 2.759\n",
      "Ep 7 (Step 008000): Train loss 2.576, Val loss 2.779\n",
      "Ep 7 (Step 008010): Train loss 2.598, Val loss 2.769\n",
      "Ep 7 (Step 008020): Train loss 2.388, Val loss 2.812\n",
      "The included lumbar and ifusion. The right coxofemoral joints are normal. The right coxofemoral is normal. The right coxofemoral joints are normal. The right coxofemoral joints are normal. The right cox\n",
      "Ep 8 (Step 008030): Train loss 2.362, Val loss 2.771\n",
      "Ep 8 (Step 008040): Train loss 2.402, Val loss 2.749\n",
      "Ep 8 (Step 008050): Train loss 2.609, Val loss 2.735\n",
      "Ep 8 (Step 008060): Train loss 2.507, Val loss 2.742\n",
      "Ep 8 (Step 008070): Train loss 2.634, Val loss 2.800\n",
      "Ep 8 (Step 008080): Train loss 2.644, Val loss 2.846\n",
      "Ep 8 (Step 008090): Train loss 2.581, Val loss 2.888\n",
      "Ep 8 (Step 008100): Train loss 2.487, Val loss 2.796\n",
      "Ep 8 (Step 008110): Train loss 2.321, Val loss 2.776\n",
      "Ep 8 (Step 008120): Train loss 2.497, Val loss 2.788\n",
      "Ep 8 (Step 008130): Train loss 2.358, Val loss 2.792\n",
      "Ep 8 (Step 008140): Train loss 2.598, Val loss 2.808\n",
      "Ep 8 (Step 008150): Train loss 2.844, Val loss 2.971\n",
      "Ep 8 (Step 008160): Train loss 2.680, Val loss 3.003\n",
      "Ep 8 (Step 008170): Train loss 2.668, Val loss 3.035\n",
      "Ep 8 (Step 008180): Train loss 2.688, Val loss 2.971\n",
      "Ep 8 (Step 008190): Train loss 2.660, Val loss 2.992\n",
      "Ep 8 (Step 008200): Train loss 2.730, Val loss 2.972\n",
      "Ep 8 (Step 008210): Train loss 2.677, Val loss 2.957\n",
      "Ep 8 (Step 008220): Train loss 2.718, Val loss 2.931\n",
      "Ep 8 (Step 008230): Train loss 2.745, Val loss 2.949\n",
      "Ep 8 (Step 008240): Train loss 2.563, Val loss 2.945\n",
      "Ep 8 (Step 008250): Train loss 2.496, Val loss 2.937\n",
      "Ep 8 (Step 008260): Train loss 2.674, Val loss 2.991\n",
      "Ep 8 (Step 008270): Train loss 2.961, Val loss 2.994\n",
      "Ep 8 (Step 008280): Train loss 2.679, Val loss 2.957\n",
      "Ep 8 (Step 008290): Train loss 2.633, Val loss 2.932\n",
      "Ep 8 (Step 008300): Train loss 2.731, Val loss 2.892\n",
      "Ep 8 (Step 008310): Train loss 2.477, Val loss 2.895\n",
      "Ep 8 (Step 008320): Train loss 2.460, Val loss 2.878\n",
      "Ep 8 (Step 008330): Train loss 2.444, Val loss 2.836\n",
      "Ep 8 (Step 008340): Train loss 2.563, Val loss 2.871\n",
      "Ep 8 (Step 008350): Train loss 2.609, Val loss 2.808\n",
      "Ep 8 (Step 008360): Train loss 2.515, Val loss 2.827\n",
      "Ep 8 (Step 008370): Train loss 2.593, Val loss 2.799\n",
      "Ep 8 (Step 008380): Train loss 2.846, Val loss 2.850\n",
      "Ep 8 (Step 008390): Train loss 2.676, Val loss 2.908\n",
      "Ep 8 (Step 008400): Train loss 2.543, Val loss 2.862\n",
      "Ep 8 (Step 008410): Train loss 2.688, Val loss 2.859\n",
      "Ep 8 (Step 008420): Train loss 2.694, Val loss 2.838\n",
      "Ep 8 (Step 008430): Train loss 2.368, Val loss 2.854\n",
      "Ep 8 (Step 008440): Train loss 2.305, Val loss 2.858\n",
      "Ep 8 (Step 008450): Train loss 2.429, Val loss 2.841\n",
      "Ep 8 (Step 008460): Train loss 2.483, Val loss 2.866\n",
      "Ep 8 (Step 008470): Train loss 2.594, Val loss 2.879\n",
      "Ep 8 (Step 008480): Train loss 2.650, Val loss 2.826\n",
      "Ep 8 (Step 008490): Train loss 2.680, Val loss 2.788\n",
      "Ep 8 (Step 008500): Train loss 2.536, Val loss 2.875\n",
      "Ep 8 (Step 008510): Train loss 2.647, Val loss 2.850\n",
      "Ep 8 (Step 008520): Train loss 2.709, Val loss 2.846\n",
      "Ep 8 (Step 008530): Train loss 2.608, Val loss 2.863\n",
      "Ep 8 (Step 008540): Train loss 2.537, Val loss 2.840\n",
      "Ep 8 (Step 008550): Train loss 2.511, Val loss 2.835\n",
      "Ep 8 (Step 008560): Train loss 2.569, Val loss 2.818\n",
      "Ep 8 (Step 008570): Train loss 2.473, Val loss 2.829\n",
      "Ep 8 (Step 008580): Train loss 2.444, Val loss 2.785\n",
      "Ep 8 (Step 008590): Train loss 2.364, Val loss 2.799\n",
      "Ep 8 (Step 008600): Train loss 2.592, Val loss 2.791\n",
      "Ep 8 (Step 008610): Train loss 2.397, Val loss 2.815\n",
      "Ep 8 (Step 008620): Train loss 2.451, Val loss 2.808\n",
      "Ep 8 (Step 008630): Train loss 2.400, Val loss 2.756\n",
      "Ep 8 (Step 008640): Train loss 2.673, Val loss 2.790\n",
      "Ep 8 (Step 008650): Train loss 2.463, Val loss 2.769\n",
      "Ep 8 (Step 008660): Train loss 2.460, Val loss 2.737\n",
      "Ep 8 (Step 008670): Train loss 2.393, Val loss 2.769\n",
      "Ep 8 (Step 008680): Train loss 2.304, Val loss 2.750\n",
      "Ep 8 (Step 008690): Train loss 2.346, Val loss 2.753\n",
      "Ep 8 (Step 008700): Train loss 2.516, Val loss 2.767\n",
      "Ep 8 (Step 008710): Train loss 2.402, Val loss 2.748\n",
      "Ep 8 (Step 008720): Train loss 2.269, Val loss 2.755\n",
      "Ep 8 (Step 008730): Train loss 2.459, Val loss 2.719\n",
      "Ep 8 (Step 008740): Train loss 2.351, Val loss 2.722\n",
      "Ep 8 (Step 008750): Train loss 2.403, Val loss 2.683\n",
      "Ep 8 (Step 008760): Train loss 2.361, Val loss 2.681\n",
      "Ep 8 (Step 008770): Train loss 2.271, Val loss 2.685\n",
      "Ep 8 (Step 008780): Train loss 2.345, Val loss 2.681\n",
      "Ep 8 (Step 008790): Train loss 2.202, Val loss 2.674\n",
      "Ep 8 (Step 008800): Train loss 2.450, Val loss 2.618\n",
      "Ep 8 (Step 008810): Train loss 2.268, Val loss 2.631\n",
      "Ep 8 (Step 008820): Train loss 2.257, Val loss 2.628\n",
      "Ep 8 (Step 008830): Train loss 2.386, Val loss 2.638\n",
      "Ep 8 (Step 008840): Train loss 2.312, Val loss 2.659\n",
      "Ep 8 (Step 008850): Train loss 2.340, Val loss 2.674\n",
      "Ep 8 (Step 008860): Train loss 2.301, Val loss 2.700\n",
      "Ep 8 (Step 008870): Train loss 2.443, Val loss 2.728\n",
      "Ep 8 (Step 008880): Train loss 2.351, Val loss 2.686\n",
      "Ep 8 (Step 008890): Train loss 2.392, Val loss 2.690\n",
      "Ep 8 (Step 008900): Train loss 2.437, Val loss 2.693\n",
      "Ep 8 (Step 008910): Train loss 2.524, Val loss 2.727\n",
      "Ep 8 (Step 008920): Train loss 2.385, Val loss 2.696\n",
      "Ep 8 (Step 008930): Train loss 2.452, Val loss 2.697\n",
      "Ep 8 (Step 008940): Train loss 2.350, Val loss 2.683\n",
      "Ep 8 (Step 008950): Train loss 2.506, Val loss 2.729\n",
      "Ep 8 (Step 008960): Train loss 2.327, Val loss 2.701\n",
      "Ep 8 (Step 008970): Train loss 2.390, Val loss 2.683\n",
      "Ep 8 (Step 008980): Train loss 2.496, Val loss 2.688\n",
      "Ep 8 (Step 008990): Train loss 2.416, Val loss 2.782\n",
      "Ep 8 (Step 009000): Train loss 2.340, Val loss 2.771\n",
      "Ep 8 (Step 009010): Train loss 2.504, Val loss 2.776\n",
      "Ep 8 (Step 009020): Train loss 2.462, Val loss 2.762\n",
      "Ep 8 (Step 009030): Train loss 2.440, Val loss 2.783\n",
      "Ep 8 (Step 009040): Train loss 2.504, Val loss 2.796\n",
      "Ep 8 (Step 009050): Train loss 2.487, Val loss 2.765\n",
      "Ep 8 (Step 009060): Train loss 2.418, Val loss 2.785\n",
      "Ep 8 (Step 009070): Train loss 2.417, Val loss 2.780\n",
      "Ep 8 (Step 009080): Train loss 2.530, Val loss 2.756\n",
      "Ep 8 (Step 009090): Train loss 2.663, Val loss 2.716\n",
      "Ep 8 (Step 009100): Train loss 2.220, Val loss 2.737\n",
      "Ep 8 (Step 009110): Train loss 2.188, Val loss 2.784\n",
      "Ep 8 (Step 009120): Train loss 2.408, Val loss 2.810\n",
      "Ep 8 (Step 009130): Train loss 2.572, Val loss 2.777\n",
      "Ep 8 (Step 009140): Train loss 2.319, Val loss 2.753\n",
      "Ep 8 (Step 009150): Train loss 2.418, Val loss 2.725\n",
      "Ep 8 (Step 009160): Train loss 2.374, Val loss 2.715\n",
      "Ep 8 (Step 009170): Train loss 2.264, Val loss 2.698\n",
      "The included lumbar and pel spine. The right stifle space is normal. The popliteal articular margins are normal. The popliteal lymph node and rounded and rounded and rounded and rounded and rounded. The long bones are normal. The tibial tuberosity\n",
      "Ep 9 (Step 009180): Train loss 2.211, Val loss 2.699\n",
      "Ep 9 (Step 009190): Train loss 2.497, Val loss 2.720\n",
      "Ep 9 (Step 009200): Train loss 2.490, Val loss 2.690\n",
      "Ep 9 (Step 009210): Train loss 2.596, Val loss 2.716\n",
      "Ep 9 (Step 009220): Train loss 2.341, Val loss 2.743\n",
      "Ep 9 (Step 009230): Train loss 2.437, Val loss 2.728\n",
      "Ep 9 (Step 009240): Train loss 2.388, Val loss 2.695\n",
      "Ep 9 (Step 009250): Train loss 2.028, Val loss 2.692\n",
      "Ep 9 (Step 009260): Train loss 2.396, Val loss 2.668\n",
      "Ep 9 (Step 009270): Train loss 2.287, Val loss 2.644\n",
      "Ep 9 (Step 009280): Train loss 2.370, Val loss 2.623\n",
      "Ep 9 (Step 009290): Train loss 2.477, Val loss 2.629\n",
      "Ep 9 (Step 009300): Train loss 2.247, Val loss 2.654\n",
      "Ep 9 (Step 009310): Train loss 2.609, Val loss 2.660\n",
      "Ep 9 (Step 009320): Train loss 2.423, Val loss 2.642\n",
      "Ep 9 (Step 009330): Train loss 2.176, Val loss 2.666\n",
      "Ep 9 (Step 009340): Train loss 2.338, Val loss 2.766\n",
      "Ep 9 (Step 009350): Train loss 2.434, Val loss 2.790\n",
      "Ep 9 (Step 009360): Train loss 2.534, Val loss 2.775\n",
      "Ep 9 (Step 009370): Train loss 2.732, Val loss 2.788\n",
      "Ep 9 (Step 009380): Train loss 2.479, Val loss 2.763\n",
      "Ep 9 (Step 009390): Train loss 2.393, Val loss 2.804\n",
      "Ep 9 (Step 009400): Train loss 2.353, Val loss 2.797\n",
      "Ep 9 (Step 009410): Train loss 2.177, Val loss 2.732\n",
      "Ep 9 (Step 009420): Train loss 2.379, Val loss 2.710\n",
      "Ep 9 (Step 009430): Train loss 2.374, Val loss 2.748\n",
      "Ep 9 (Step 009440): Train loss 2.447, Val loss 2.756\n",
      "Ep 9 (Step 009450): Train loss 2.410, Val loss 2.710\n",
      "Ep 9 (Step 009460): Train loss 2.480, Val loss 2.740\n",
      "Ep 9 (Step 009470): Train loss 2.421, Val loss 2.696\n",
      "Ep 9 (Step 009480): Train loss 2.393, Val loss 2.672\n",
      "Ep 9 (Step 009490): Train loss 2.204, Val loss 2.651\n",
      "Ep 9 (Step 009500): Train loss 2.216, Val loss 2.659\n",
      "Ep 9 (Step 009510): Train loss 2.324, Val loss 2.626\n",
      "Ep 9 (Step 009520): Train loss 2.425, Val loss 2.591\n",
      "Ep 9 (Step 009530): Train loss 2.437, Val loss 2.635\n",
      "Ep 9 (Step 009540): Train loss 2.488, Val loss 2.641\n",
      "Ep 9 (Step 009550): Train loss 2.169, Val loss 2.651\n",
      "Ep 9 (Step 009560): Train loss 2.456, Val loss 2.651\n",
      "Ep 9 (Step 009570): Train loss 2.260, Val loss 2.702\n",
      "Ep 9 (Step 009580): Train loss 2.342, Val loss 2.666\n",
      "Ep 9 (Step 009590): Train loss 2.557, Val loss 2.706\n",
      "Ep 9 (Step 009600): Train loss 2.482, Val loss 2.687\n",
      "Ep 9 (Step 009610): Train loss 2.494, Val loss 2.694\n",
      "Ep 9 (Step 009620): Train loss 2.182, Val loss 2.707\n",
      "Ep 9 (Step 009630): Train loss 2.378, Val loss 2.673\n",
      "Ep 9 (Step 009640): Train loss 2.369, Val loss 2.658\n",
      "Ep 9 (Step 009650): Train loss 2.475, Val loss 2.640\n",
      "Ep 9 (Step 009660): Train loss 2.460, Val loss 2.684\n",
      "Ep 9 (Step 009670): Train loss 2.454, Val loss 2.720\n",
      "Ep 9 (Step 009680): Train loss 2.414, Val loss 2.710\n",
      "Ep 9 (Step 009690): Train loss 2.489, Val loss 2.723\n",
      "Ep 9 (Step 009700): Train loss 2.350, Val loss 2.707\n",
      "Ep 9 (Step 009710): Train loss 2.438, Val loss 2.683\n",
      "Ep 9 (Step 009720): Train loss 2.533, Val loss 2.700\n",
      "Ep 9 (Step 009730): Train loss 2.248, Val loss 2.675\n",
      "Ep 9 (Step 009740): Train loss 2.417, Val loss 2.641\n",
      "Ep 9 (Step 009750): Train loss 2.283, Val loss 2.626\n",
      "Ep 9 (Step 009760): Train loss 2.321, Val loss 2.656\n",
      "Ep 9 (Step 009770): Train loss 2.345, Val loss 2.641\n",
      "Ep 9 (Step 009780): Train loss 2.463, Val loss 2.666\n",
      "Ep 9 (Step 009790): Train loss 2.393, Val loss 2.694\n",
      "Ep 9 (Step 009800): Train loss 2.213, Val loss 2.646\n",
      "Ep 9 (Step 009810): Train loss 2.307, Val loss 2.622\n",
      "Ep 9 (Step 009820): Train loss 2.478, Val loss 2.590\n",
      "Ep 9 (Step 009830): Train loss 2.179, Val loss 2.598\n",
      "Ep 9 (Step 009840): Train loss 2.233, Val loss 2.575\n",
      "Ep 9 (Step 009850): Train loss 2.302, Val loss 2.602\n",
      "Ep 9 (Step 009860): Train loss 2.244, Val loss 2.587\n",
      "Ep 9 (Step 009870): Train loss 2.070, Val loss 2.627\n",
      "Ep 9 (Step 009880): Train loss 2.304, Val loss 2.644\n",
      "Ep 9 (Step 009890): Train loss 2.188, Val loss 2.648\n",
      "Ep 9 (Step 009900): Train loss 2.545, Val loss 2.719\n",
      "Ep 9 (Step 009910): Train loss 2.173, Val loss 2.658\n",
      "Ep 9 (Step 009920): Train loss 2.374, Val loss 2.627\n",
      "Ep 9 (Step 009930): Train loss 2.409, Val loss 2.599\n",
      "Ep 9 (Step 009940): Train loss 2.482, Val loss 2.562\n",
      "Ep 9 (Step 009950): Train loss 2.308, Val loss 2.561\n",
      "Ep 9 (Step 009960): Train loss 2.426, Val loss 2.577\n",
      "Ep 9 (Step 009970): Train loss 2.454, Val loss 2.593\n",
      "Ep 9 (Step 009980): Train loss 2.297, Val loss 2.559\n",
      "Ep 9 (Step 009990): Train loss 2.308, Val loss 2.571\n",
      "Ep 9 (Step 010000): Train loss 2.322, Val loss 2.591\n",
      "Ep 9 (Step 010010): Train loss 2.418, Val loss 2.600\n",
      "Ep 9 (Step 010020): Train loss 2.097, Val loss 2.592\n",
      "Ep 9 (Step 010030): Train loss 2.376, Val loss 2.603\n",
      "Ep 9 (Step 010040): Train loss 2.388, Val loss 2.577\n",
      "Ep 9 (Step 010050): Train loss 2.217, Val loss 2.574\n",
      "Ep 9 (Step 010060): Train loss 2.123, Val loss 2.558\n",
      "Ep 9 (Step 010070): Train loss 2.288, Val loss 2.564\n",
      "Ep 9 (Step 010080): Train loss 2.331, Val loss 2.582\n",
      "Ep 9 (Step 010090): Train loss 2.379, Val loss 2.577\n",
      "Ep 9 (Step 010100): Train loss 2.181, Val loss 2.587\n",
      "Ep 9 (Step 010110): Train loss 2.513, Val loss 2.611\n",
      "Ep 9 (Step 010120): Train loss 2.392, Val loss 2.616\n",
      "Ep 9 (Step 010130): Train loss 2.337, Val loss 2.582\n",
      "Ep 9 (Step 010140): Train loss 2.428, Val loss 2.594\n",
      "Ep 9 (Step 010150): Train loss 2.299, Val loss 2.578\n",
      "Ep 9 (Step 010160): Train loss 2.347, Val loss 2.580\n",
      "Ep 9 (Step 010170): Train loss 2.300, Val loss 2.594\n",
      "Ep 9 (Step 010180): Train loss 2.234, Val loss 2.574\n",
      "Ep 9 (Step 010190): Train loss 2.242, Val loss 2.614\n",
      "Ep 9 (Step 010200): Train loss 2.177, Val loss 2.626\n",
      "Ep 9 (Step 010210): Train loss 2.347, Val loss 2.622\n",
      "Ep 9 (Step 010220): Train loss 2.326, Val loss 2.604\n",
      "Ep 9 (Step 010230): Train loss 2.326, Val loss 2.632\n",
      "Ep 9 (Step 010240): Train loss 2.471, Val loss 2.613\n",
      "Ep 9 (Step 010250): Train loss 2.389, Val loss 2.626\n",
      "Ep 9 (Step 010260): Train loss 2.356, Val loss 2.604\n",
      "Ep 9 (Step 010270): Train loss 2.271, Val loss 2.588\n",
      "Ep 9 (Step 010280): Train loss 2.385, Val loss 2.585\n",
      "Ep 9 (Step 010290): Train loss 2.263, Val loss 2.580\n",
      "Ep 9 (Step 010300): Train loss 2.132, Val loss 2.603\n",
      "Ep 9 (Step 010310): Train loss 2.393, Val loss 2.578\n",
      "Ep 9 (Step 010320): Train loss 2.241, Val loss 2.594\n",
      "The included lumbar and the spine. The remainder of the abdomen is normal. The included abdomen is normal. The included small intestine is normal in diameter. The colon contains a moderate volume of the colon contains a moderate volume of fecal material. The peritoneal serosal\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(CONFIG)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1810fcfc-920a-4126-aba7-2de68b05fd86",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m----> 2\u001b[0m train_losses, val_losses, tokens_seen \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe included lumbar and\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[108], line 13\u001b[0m, in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_batch, target_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Reset loss gradients from previous batch iteration\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# Calculate loss gradients\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Update model weights using loss gradients\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[106], line 3\u001b[0m, in \u001b[0;36mcalc_loss_batch\u001b[0;34m(input_batch, target_batch, model, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_loss_batch\u001b[39m(input_batch, target_batch, model, device):\n\u001b[1;32m      2\u001b[0m     input_batch, target_batch \u001b[38;5;241m=\u001b[39m input_batch\u001b[38;5;241m.\u001b[39mto(device), target_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), target_batch\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m, in \u001b[0;36mGPTModel.forward\u001b[0;34m(self, in_idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_idx):\n\u001b[1;32m     15\u001b[0m     batch_size, seq_len \u001b[38;5;241m=\u001b[39m in_idx\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 16\u001b[0m     tok_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtok_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     pos_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb(torch\u001b[38;5;241m.\u001b[39marange(seq_len, device\u001b[38;5;241m=\u001b[39min_idx\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m tok_embeds \u001b[38;5;241m+\u001b[39m pos_embeds  \u001b[38;5;66;03m# Shape [batch_size, num_tokens, emb_size]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=10, eval_iter=10,\n",
    "    start_context=\"The included lumbar and\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "58abed9c-21fc-4da4-a958-74f17b3d8793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc53b6-d9d9-41b1-aed7-f021f61dccf4",
   "metadata": {},
   "source": [
    "# Controlling randomness in the text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5ceab935-0231-4870-8f05-57d99b64562e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " The included lumbar and lumbar spine. The remainder of the osseous structures are normal. Conclusions and recommendations: The appearance of the\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"The included lumbar and\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5043d3-cba3-4f16-bbab-106da11c4024",
   "metadata": {},
   "source": [
    "- Even if we execute the `generate_text_simple` function above multiple times, the LLM will always generate the same outputs\n",
    "- We now introduce two concepts, so-called decoding strategies, to modify the `generate_text_simple`: *temperature scaling* and *top-k* sampling\n",
    "- These will allow the model to control the randomness and diversity of the generated text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4e833-79f5-4eb8-a5e9-d6d2e2a4c863",
   "metadata": {},
   "source": [
    "## Temperature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60efaba-b972-4113-8cef-f6d335fdb977",
   "metadata": {},
   "source": [
    "- Previously, we always sampled the token with the highest probability as the next token using `torch.argmax`\n",
    "- To add variety, we can sample the next token using The `torch.multinomial(probs, num_samples=1)`, sampling from a probability distribution\n",
    "- Here, each index's chance of being picked corresponds to its probability in the input tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4374da-38b5-4e60-9dcc-e7d3587f7527",
   "metadata": {},
   "source": [
    "- Here's a little recap of generating the next token, assuming a very small vocabulary for illustration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3043c278-6240-4b7a-99a8-9617ebb51e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a786d62d-cffa-4d3f-8612-0639133de230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "64bc8c8d-7a20-4ee0-a291-3bf262131b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f173f-122f-4e2b-b136-bd1595a818eb",
   "metadata": {},
   "source": [
    "- We can control the distribution and selection process via a concept called temperature scaling\n",
    "- \"Temperature scaling\" is just a fancy word for dividing the logits by a number greater than 0\n",
    "- Temperatures greater than 1 will result in more uniformly distributed token probabilities after applying the softmax\n",
    "- Temperatures smaller than 1 will result in more confident (sharper or more peaky) distributions after applying the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "540a62b1-64f9-43c8-b6b7-11c5fc8d4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "33dbfccd-ff42-437f-8699-7e341878c587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c9be97fb-8db1-4e6c-b43f-53854888062b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "992 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "8 x toward\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc4a61-8f82-44d5-a3d6-013d97f6e64c",
   "metadata": {},
   "source": [
    "The rescaled probabilities via temperature 5 are more uniformly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b17fea1a-c9ae-4f24-9397-ccbd77fee9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 x closer\n",
      "68 x every\n",
      "55 x effort\n",
      "223 x forward\n",
      "102 x inches\n",
      "50 x moves\n",
      "43 x pizza\n",
      "218 x toward\n",
      "88 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3c9fd-866a-41fc-aa16-875b1bec695a",
   "metadata": {},
   "source": [
    "## Top k sampling\n",
    "\n",
    "To be able to use higher temperatures to increase output diversity and to reduce the probability of nonsensical sentences, we can restrict the sampled tokens to the top-k most likely tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "335251bc-3926-4207-91d6-68e43e09bee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c90fa381-2360-49ed-92d2-146eb6eb578a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float('-inf')), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "48f648c9-edac-4e80-af11-2e39fdc4a34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9720a9-e163-40b8-8240-c168770d9683",
   "metadata": {},
   "source": [
    "## Modifying the text generation function\n",
    "\n",
    "- The previous two subsections introduced temperature sampling and top-k sampling\n",
    "- Let's use these two concepts to modify the `generate_simple` function we used to generate text via the LLM earlier, creating a new `generate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6f587008-1bb7-46c3-9435-1042e595d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9c3250ce-b65a-4b58-a6d6-2d93fc906473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " The included lumbar and thoracic spine for evaluation would be considered. Left lateral thoracic\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"The included lumbar and\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=CONFIG[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ea726-bd1c-4f51-9960-5d5ea0d24211",
   "metadata": {},
   "source": [
    "## Loading and saving model weights in PyTorch\n",
    "\n",
    "The recommended way in PyTorch is to save the model weights, the so-called `state_dict` via by applying the `torch.save` function to the `.state_dict()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fdb8f544-65b8-4586-af18-85e5d98bccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"../../models/model_{MODEL_SIZE}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42211c64-eb7e-4daf-86b3-f3a596a9cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(CONFIG)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(f\"../../models/model_{MODEL_SIZE}.pth\", map_location=device, weights_only=True))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd4623-325c-4479-91df-5430847500ba",
   "metadata": {},
   "source": [
    "- It's common to train LLMs with adaptive optimizers like Adam or AdamW instead of regular SGD\n",
    "- These adaptive optimizers store additional parameters for each model weight, so it makes sense to save them as well in case we plan to continue the pretraining later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c608bf-fe1c-45a2-8e92-c22d41921ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"../models/model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0dfd7-6a24-4ba2-9a15-c6e606b64f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../models/model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(CONFIG)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
