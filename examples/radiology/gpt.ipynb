{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c83d75-8e21-4c49-ae23-004e8f14cc32",
   "metadata": {},
   "source": [
    "Note code is coming from Sebastian Raschka's [repo](https://github.com/rasbt/LLMs-from-scratch) covering LLMs from scratch with slight modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23037aab-769a-467c-b4c6-a689d3ab51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import json_to_dataframe, json_to_string_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38145ed6-c229-424f-a20b-b224be98667a",
   "metadata": {},
   "source": [
    "## Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e4fa00-662c-44e3-9da1-62fc80ddae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_decode_example(list_of_strings):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Get the token ID for <|endoftext|>\n",
    "    endoftext_token = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "    all_tokens = []\n",
    "    for text in list_of_strings:\n",
    "        # Encode the text\n",
    "        encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        all_tokens.extend(encoded + [endoftext_token])\n",
    "\n",
    "    # Decode the tokens\n",
    "    decoded = tokenizer.decode(all_tokens)\n",
    "\n",
    "    return all_tokens, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a53cb81-7240-4cfd-9d45-dc3aa0fbe53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, articles, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Get the token ID for <|endoftext|>\n",
    "        endoftext_token = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "        # Tokenize all articles with end-of-text token\n",
    "        all_tokens = []\n",
    "        for article in articles:\n",
    "            article_tokens = tokenizer.encode(article, allowed_special={\"<|endoftext|>\"})\n",
    "            all_tokens.extend(article_tokens + [endoftext_token])\n",
    "\n",
    "        # Use a sliding window to chunk the tokens into overlapping sequences of max_length\n",
    "        for i in range(0, len(all_tokens) - max_length, stride):\n",
    "            input_chunk = all_tokens[i:i + max_length]\n",
    "            target_chunk = all_tokens[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3796d3da-d0c2-48e9-b141-da4277d40833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(articles, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(articles, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e2d9a-fbbc-47b7-a8ee-efc5117dcfca",
   "metadata": {},
   "source": [
    "## Load radiology reports dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b10af37b-6b85-4e45-9601-34b612fb6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/vector_veterinary_imaging.json'\n",
    "\n",
    "df = json_to_dataframe(filepath) \n",
    "rad_strings = json_to_string_list(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28ebf5d3-b68f-4864-b00e-6aaa73ac29c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_identifier</th>\n",
       "      <th>findings</th>\n",
       "      <th>conclusions_and_recommendations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181153</td>\n",
       "      <td>Orthogonal pelvis and orthogonal right shoulde...</td>\n",
       "      <td>1. Medial right mildly comminuted acetabular f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181413</td>\n",
       "      <td>Three view whole body images dated April 14, 2...</td>\n",
       "      <td>The material within the stomach and small inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181821</td>\n",
       "      <td>Three view thoracic radiographs (total of 5 th...</td>\n",
       "      <td>No aggressive osseous changes are noted. The b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181886</td>\n",
       "      <td>Orthogonal images of the right pelvic limb are...</td>\n",
       "      <td>1. Chronic right calcaneal tendonopathy, with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>181911</td>\n",
       "      <td>Lateral abdomen and pelvis images are provided...</td>\n",
       "      <td>1. Numerous small urinary cystoliths, non-obst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>215115</td>\n",
       "      <td>Ventrodorsal pelvis and and stifles and latera...</td>\n",
       "      <td>Marked bilateral stifle synovitis/synovial eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>215123</td>\n",
       "      <td>Three view thorax images dated 12/1/2023 are p...</td>\n",
       "      <td>Moderate diffuse lower airway thickening, most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>215136</td>\n",
       "      <td>Three view whole body images dated 12/1/2023 a...</td>\n",
       "      <td>Hepatosplenomegaly. Separately, these may repr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>215177</td>\n",
       "      <td>Three view thoracic images dated 12/2/2023 are...</td>\n",
       "      <td>Marked left and right-sided cardiomegaly. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>215379</td>\n",
       "      <td>Three view thorax images dated 12/4/2023 are p...</td>\n",
       "      <td>Moderate diffuse bronchocentric lower airway t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>603 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     case_identifier                                           findings  \\\n",
       "0             181153  Orthogonal pelvis and orthogonal right shoulde...   \n",
       "1             181413  Three view whole body images dated April 14, 2...   \n",
       "2             181821  Three view thoracic radiographs (total of 5 th...   \n",
       "3             181886  Orthogonal images of the right pelvic limb are...   \n",
       "4             181911  Lateral abdomen and pelvis images are provided...   \n",
       "..               ...                                                ...   \n",
       "598           215115  Ventrodorsal pelvis and and stifles and latera...   \n",
       "599           215123  Three view thorax images dated 12/1/2023 are p...   \n",
       "600           215136  Three view whole body images dated 12/1/2023 a...   \n",
       "601           215177  Three view thoracic images dated 12/2/2023 are...   \n",
       "602           215379  Three view thorax images dated 12/4/2023 are p...   \n",
       "\n",
       "                       conclusions_and_recommendations  \n",
       "0    1. Medial right mildly comminuted acetabular f...  \n",
       "1    The material within the stomach and small inte...  \n",
       "2    No aggressive osseous changes are noted. The b...  \n",
       "3    1. Chronic right calcaneal tendonopathy, with ...  \n",
       "4    1. Numerous small urinary cystoliths, non-obst...  \n",
       "..                                                 ...  \n",
       "598  Marked bilateral stifle synovitis/synovial eff...  \n",
       "599  Moderate diffuse lower airway thickening, most...  \n",
       "600  Hepatosplenomegaly. Separately, these may repr...  \n",
       "601  Marked left and right-sided cardiomegaly. The ...  \n",
       "602  Moderate diffuse bronchocentric lower airway t...  \n",
       "\n",
       "[603 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0680ab3c-5529-47bf-ab21-a343548b6722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rad_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c662dd88-fcb6-4729-8678-499012d60e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Findings: Orthogonal pelvis and orthogonal right shoulder and lateral left shoulder images dated April 17, 2023 are provided for review (total of 5 images). Shoulders: A sagittal plane fracture is present through the right scapular body, where the spine meets the body, extending cranially through the cranial margin of the acromion. this fracture does not articulate with the glenoid rims or the scapulohumeral joint. This fracture is visualized on the craniocaudal image, not visualized on the lateral image, thought due to superimposition. The fracture is non-displaced. Small fissures are suspected extending into the scapular spine. The right first rib is fractured in the body. A non-displaced fracture is also suspected in the body of the right second rib. The visible scapula, scapulohumeral margins, and humerus of the left shoulder are normal. The included cervical and thoracic spine is normal. Pelvis: A mildly comminuted segment fracture is present through the medial and cranial third of the right acetabulum. This fracture is mildly medially displaced and overriding, causing widening of the coxofemoral joint space. A transverse fracture is present through the right pubis. The medial aspect of the right acetabulum is mildly heterogenous and poorly defined though no distinct fracture is identified. The left ischial apophysis is fractured and caudally displaced. The remainder of the pelvis is normal. The femoral heads remain smooth and rounded. The sacroiliac joints are normal. The included lumbar and caudal spine is normal. Conclusions and recommendations: 1. Medial right mildly comminuted acetabular fracture, affecting the weight-bearing surface of the acetabulum. Consultation with an orthopedist is warranted regarding surgical fixation. 2. Longitudinal right scapular fracture, affecting the spine where it inserts onto the body. Surgical fixation may also be warranted of this fracture. 3. Left ischial apophysis avulsion fracture, likely involving avulsion of the left quadriceps origin. 4. Right pubis fracture. 5. Possible right non-displaced ischial fracture. 6. Right first and probable second rib fractures.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rad_strings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28363559-3f43-4eda-9105-d41e1fdc8acb",
   "metadata": {},
   "source": [
    "## Create data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738db129-e6c7-4682-9a14-6be51c9076c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "max_len = 1024\n",
    "context_length = max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38e6bb14-7b1e-429d-b728-0e6115451c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "token_embedding_layer = nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbc49c66-779f-4194-a21c-a9c838652d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(rad_strings, batch_size=8, max_length=max_length, stride=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39d0c24a-88b4-4e52-b989-478c666685d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_batch(x, y, n_samples=2):\n",
    "    for i in range(min(n_samples, len(x))):\n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        \n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        \n",
    "        # Decode and print the input sequence\n",
    "        input_text = tokenizer.decode(x[i].tolist())\n",
    "        print(f\"Input text: {input_text}\")\n",
    "        print(f\"Input encoding: {x[i].tolist()}\")\n",
    "        \n",
    "        # Decode and print the target sequence\n",
    "        target_text = tokenizer.decode(y[i].tolist())\n",
    "        print(f\"Target text: {target_text}\")\n",
    "        print(f\"Target encoding: {y[i].tolist()}\")\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65e2634e-79b0-4bab-b53b-3d6aa9c584d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSPECT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a84c4a1-cfbd-4b73-a397-2a7f2929e8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "Input text:  of 4 images).\n",
      "Input encoding: [286, 604, 4263, 737]\n",
      "Target text:  4 images). The\n",
      "Target encoding: [604, 4263, 737, 383]\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 2:\n",
      "Input text: ax in the region\n",
      "Input encoding: [897, 287, 262, 3814]\n",
      "Target text:  in the region of\n",
      "Target encoding: [287, 262, 3814, 286]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "\n",
    "    if INSPECT:\n",
    "        # Visual inspection\n",
    "        inspect_batch(x, y)\n",
    "\n",
    "    token_embeddings = token_embedding_layer(x)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c6d1d95-18b8-4c2f-85b9-4617dfc2c260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c2383-dc43-4f77-be49-560b3d7c5dcb",
   "metadata": {},
   "source": [
    "# Define GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2069516d-cbea-4019-9d5f-b207b5d5f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89bebd56-46e4-4c35-ab21-56587819f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9726f58c-6849-484e-a481-4ed97bb68bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb076600-0858-4476-b2a7-81aa3a474225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bceff59-276c-4501-bd01-c38d6a435579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0dca6-ba38-41f1-aa4c-e7029ac313e7",
   "metadata": {},
   "source": [
    "# Initialize GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "80641b2c-e2f0-4d8a-8a6f-14fa93cf1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "GPT2_CONFIG_355M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 1024,\n",
    "    \"n_heads\": 16,\n",
    "    \"n_layers\": 24,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "GPT2_CONFIG_774M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 1280,\n",
    "    \"n_heads\": 20,\n",
    "    \"n_layers\": 36,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "GPT2_CONFIG_1558M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 1600,\n",
    "    \"n_heads\": 25,\n",
    "    \"n_layers\": 48,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488eb2d-e5c7-406e-a2dc-828e5a32b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = GPT_CONFIG_124M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aca8ce-4745-4150-8f91-000e13b9a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(CONFIG)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "07d5d5d1-7391-4207-95de-9d99768ef67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "892aa126-3dc3-4602-8469-d9d540a1b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"The included lumbar and\"\n",
    "# start_context = \"extending cranially through\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c8a7d217-ce44-48de-908e-25f27cdcb888",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=CONFIG[\"context_length\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc5099-3068-4410-ab8c-87c7fbd0eefc",
   "metadata": {},
   "source": [
    "The model will generate random text prior to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "69ee08cc-6aa7-42b0-9c9b-4230018e39fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " The included lumbar and Publication,' Somebody ampbase Dying counsellingilibrium Influ haul\n"
     ]
    }
   ],
   "source": [
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed82de-54ce-4462-9084-f10a960dd175",
   "metadata": {},
   "source": [
    "## Explore dataset\n",
    "\n",
    "Look at distribution of token lengths of reports, also look at token length of the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9d596963-2d66-4ede-8289-891bf578928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c0cf8230-b5fd-4b0c-b368-478978a33db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the entire dataset: 220912\n"
     ]
    }
   ],
   "source": [
    "# Calculate token length of entire dataset\n",
    "entire_text = \" \".join(rad_strings)\n",
    "total_tokens = len(tokenizer.encode(entire_text))\n",
    "\n",
    "print(f\"Total tokens in the entire dataset: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5ca61c9f-97db-4d41-8c11-8d85ec39b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token length statistics for individual strings:\n",
      "Mean: 366.35\n",
      "Median: 336.00\n",
      "Minimum: 130\n",
      "Maximum: 1554\n",
      "Standard Deviation: 166.36\n",
      "\n",
      "Percentiles:\n",
      "25th: 250.50\n",
      "50th: 336.00\n",
      "75th: 440.50\n",
      "90th: 556.00\n",
      "95th: 652.50\n",
      "99th: 903.92\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate token lengths for individual strings\n",
    "token_lengths = [len(tokenizer.encode(s)) for s in rad_strings]\n",
    "\n",
    "# Calculate statistics\n",
    "mean_length = np.mean(token_lengths)\n",
    "median_length = np.median(token_lengths)\n",
    "min_length = np.min(token_lengths)\n",
    "max_length = np.max(token_lengths)\n",
    "std_dev = np.std(token_lengths)\n",
    "\n",
    "# Calculate percentiles\n",
    "percentiles = np.percentile(token_lengths, [25, 50, 75, 90, 95, 99])\n",
    "\n",
    "print(\"\\nToken length statistics for individual strings:\")\n",
    "print(f\"Mean: {mean_length:.2f}\")\n",
    "print(f\"Median: {median_length:.2f}\")\n",
    "print(f\"Minimum: {min_length}\")\n",
    "print(f\"Maximum: {max_length}\")\n",
    "print(f\"Standard Deviation: {std_dev:.2f}\")\n",
    "print(\"\\nPercentiles:\")\n",
    "print(f\"25th: {percentiles[0]:.2f}\")\n",
    "print(f\"50th: {percentiles[1]:.2f}\")\n",
    "print(f\"75th: {percentiles[2]:.2f}\")\n",
    "print(f\"90th: {percentiles[3]:.2f}\")\n",
    "print(f\"95th: {percentiles[4]:.2f}\")\n",
    "print(f\"99th: {percentiles[5]:.2f}\")\n",
    "\n",
    "# Optionally, you can create a histogram of token lengths\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(token_lengths, bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Token Lengths')\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80830e-3399-4600-b85d-a86858a50ece",
   "metadata": {},
   "source": [
    "# Calculating metrics\n",
    "\n",
    "We will use cross-entropy and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "268da083-34fa-4318-a918-2e400457b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A couple examples of inputs and targets\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a01657c-dd17-4538-b753-331d3cca3625",
   "metadata": {},
   "source": [
    "Feeding the `inputs` to the model, we obtain the logits vector for the 2 input examples that consist of 3 tokens each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d96a0021-024a-4992-b12c-65b084b1fc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55bd0bf-4472-4662-83ca-95a09d789e85",
   "metadata": {},
   "source": [
    "Since we have 2 input batches with 3 tokens each, we obtain 2 by 3 predicted token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ffef7fa9-f62c-4516-a07b-c70e9c582525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ca6c7-a4e9-4882-976b-13a166839c0f",
   "metadata": {},
   "source": [
    "If we decode these tokens, we find that these are quite different from the tokens we want the model to predict, namely the target tokens. This reflects that the model hasn't been trained yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e2c3c6ad-2b54-4f6b-95c3-8925821a965c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6229df5c-86b6-4cab-8570-6567c1f3e50b",
   "metadata": {},
   "source": [
    "The token probabilities corresponding to the target indices are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4f83d452-5626-4432-ab07-507a7dde9133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb007f-d921-41ca-a1bf-728ec222fa00",
   "metadata": {},
   "source": [
    "We want to maximize all these values, bringing them close to a probability of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bda3aad9-2f61-4ea4-a8d4-de09c82e0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2f08825b-af8d-49ca-a4e8-2f7406b8db0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3f5a0f2f-314d-4c13-937c-10160ca33785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "be339ccd-44a2-4e62-b9f5-4e5d8eda4870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db571088-59d0-4a87-8605-f20ebbbff971",
   "metadata": {},
   "source": [
    "For the `cross_entropy` function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ae19354f-8c40-4362-8c3c-cfcbbe6b0420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8688f85d-bebc-48fa-9759-f6904e9b7da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed6b10-6f2f-4a09-808c-3661fce69e9f",
   "metadata": {},
   "source": [
    "- A concept related to the cross-entropy loss is the perplexity of an LLM\n",
    "- The perplexity is simply the exponential of the cross-entropy loss\n",
    "- The perplexity is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that'd be 48,725 words or tokens)\n",
    "- In other words, perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset\n",
    "- Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "96059422-6ba1-410e-9375-aab98da1ff2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78162dc4-c5e9-472b-8ef2-1a56831179a4",
   "metadata": {},
   "source": [
    "## Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5af2868c-07f7-4855-964f-e312a3d4c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, text_data = encode_and_decode_example(rad_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d88ce3b2-cdba-472a-a184-54dce5522d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1031639"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "315ceaf5-00b4-4862-b2c0-0da67487cf5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221515"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c0df3695-86bf-4629-8fd0-57170c27a8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Findings: Orthogonal pelvis and orthogonal right shoulder and lateral left shoulder images dated Apr\n"
     ]
    }
   ],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c559b5f1-f97e-4dc3-9486-0c769012dc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olar lavage and viral testing prior to empiric therapy for inflammatory airway disease.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Last 100 characters\n",
    "print(text_data[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fb773da0-b1f8-473b-b15e-40aaaa99e86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 1031639\n",
      "Tokens: 221515\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokens)\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0feabc-52f2-43bd-b7a3-39c06b7d0c68",
   "metadata": {},
   "source": [
    "Create the training and validation data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9bd266ec-bac6-46f1-9cb0-64e83ed1b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(rad_strings))\n",
    "train_data = rad_strings[:split_idx]\n",
    "val_data = rad_strings[split_idx:]\n",
    "\n",
    "# split_idx = int(train_ratio * len(text_data))\n",
    "# train_data = text_data[:split_idx]\n",
    "# val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "96eaa226-ebd5-4f4a-b6db-754750ffe22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=8,\n",
    "    max_length=CONFIG[\"context_length\"],\n",
    "    stride=CONFIG[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=8,\n",
    "    max_length=CONFIG[\"context_length\"],\n",
    "    stride=CONFIG[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4a1245aa-1027-4e11-9ce9-c046856272e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < CONFIG[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `CONFIG['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < CONFIG[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `CONFIG['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748726e-b95c-4d06-bb6b-c0c32b13ea79",
   "metadata": {},
   "source": [
    "An optional check that the data was loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fdb8b93a-8694-493c-9b79-8d74ce031cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Train loader:\")\n",
    "# for x, y in train_loader:\n",
    "#     print(x.shape, y.shape)\n",
    "\n",
    "# print(\"\\nValidation loader:\")\n",
    "# for x, y in val_loader:\n",
    "#     print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce26fe-b73f-4681-95d3-e75139fd5fcd",
   "metadata": {},
   "source": [
    "Another optional check that the token sizes are in the expected ballpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1ac57645-f908-4b28-930a-53d403df378c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 198656\n",
      "Validation tokens: 22272\n",
      "All tokens: 220928\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95558a-21a1-438e-962c-40c277b1237d",
   "metadata": {},
   "source": [
    "- Next, we implement a utility function to calculate the cross-entropy loss of a given batch\n",
    "- In addition, we implement a second utility function to compute the loss for a user-specified number of batches in a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f8c1d344-4278-486f-b8d4-dfee81606dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c8b872e1-f3d5-42ba-9ab4-dd3cf91a2307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.994663071386594\n",
      "Validation loss: 11.00070875341242\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6866283-dc19-42a2-8d36-da8ce0370677",
   "metadata": {},
   "source": [
    "# LLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "04f58dbb-c3a3-41c1-a354-aedf456e07c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223f4aa-5337-4a91-8711-2aadf17e6006",
   "metadata": {},
   "source": [
    "With the current dataset and settings there are ~350 steps per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a33ed61c-9f61-4cf9-b8e8-b02cd6aaf383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.776, Val loss 9.754\n",
      "Ep 1 (Step 000005): Train loss 7.872, Val loss 7.795\n",
      "Ep 1 (Step 000010): Train loss 6.443, Val loss 6.352\n",
      "Ep 1 (Step 000015): Train loss 5.610, Val loss 5.566\n",
      "Ep 1 (Step 000020): Train loss 5.161, Val loss 5.020\n",
      "Ep 1 (Step 000025): Train loss 4.645, Val loss 4.618\n",
      "Ep 1 (Step 000030): Train loss 4.407, Val loss 4.353\n",
      "Ep 1 (Step 000035): Train loss 4.279, Val loss 4.124\n",
      "Ep 1 (Step 000040): Train loss 3.890, Val loss 3.969\n",
      "Ep 1 (Step 000045): Train loss 4.005, Val loss 3.814\n",
      "Ep 1 (Step 000050): Train loss 3.719, Val loss 3.704\n",
      "Ep 1 (Step 000055): Train loss 3.577, Val loss 3.606\n",
      "Ep 1 (Step 000060): Train loss 3.470, Val loss 3.530\n",
      "Ep 1 (Step 000065): Train loss 3.468, Val loss 3.466\n",
      "Ep 1 (Step 000070): Train loss 3.330, Val loss 3.412\n",
      "Ep 1 (Step 000075): Train loss 3.236, Val loss 3.346\n",
      "Ep 1 (Step 000080): Train loss 3.199, Val loss 3.297\n",
      "Ep 1 (Step 000085): Train loss 3.226, Val loss 3.261\n",
      "Ep 1 (Step 000090): Train loss 3.091, Val loss 3.225\n",
      "Ep 1 (Step 000095): Train loss 3.068, Val loss 3.196\n",
      "The included lumbar and recommendations: Orthogonal left stifle joint. The included caudal serosal detail is normal. The included caudal serosal detail is normal. The included caudal serosal detail is normal. The included caudal serosal\n",
      "Ep 2 (Step 000100): Train loss 3.031, Val loss 3.174\n",
      "Ep 2 (Step 000105): Train loss 2.957, Val loss 3.149\n",
      "Ep 2 (Step 000110): Train loss 3.010, Val loss 3.130\n",
      "Ep 2 (Step 000115): Train loss 2.936, Val loss 3.108\n",
      "Ep 2 (Step 000120): Train loss 2.950, Val loss 3.084\n",
      "Ep 2 (Step 000125): Train loss 2.996, Val loss 3.054\n",
      "Ep 2 (Step 000130): Train loss 2.782, Val loss 3.012\n",
      "Ep 2 (Step 000135): Train loss 2.677, Val loss 2.988\n",
      "Ep 2 (Step 000140): Train loss 2.708, Val loss 2.962\n",
      "Ep 2 (Step 000145): Train loss 2.718, Val loss 2.919\n",
      "Ep 2 (Step 000150): Train loss 2.686, Val loss 2.903\n",
      "Ep 2 (Step 000155): Train loss 2.613, Val loss 2.882\n",
      "Ep 2 (Step 000160): Train loss 2.583, Val loss 2.855\n",
      "Ep 2 (Step 000165): Train loss 2.577, Val loss 2.846\n",
      "Ep 2 (Step 000170): Train loss 2.595, Val loss 2.835\n",
      "Ep 2 (Step 000175): Train loss 2.573, Val loss 2.814\n",
      "Ep 2 (Step 000180): Train loss 2.539, Val loss 2.812\n",
      "Ep 2 (Step 000185): Train loss 2.561, Val loss 2.777\n",
      "Ep 2 (Step 000190): Train loss 2.433, Val loss 2.763\n",
      "The included lumbar and lysis is present. The included lysis is normal. The included abdomen is normal. The included lysis is normal. The included abdomen is normal. The included thoracic lymph nodes are normal. The trachea and mediastinum\n",
      "Ep 3 (Step 000195): Train loss 2.393, Val loss 2.743\n",
      "Ep 3 (Step 000200): Train loss 2.515, Val loss 2.742\n",
      "Ep 3 (Step 000205): Train loss 2.377, Val loss 2.725\n",
      "Ep 3 (Step 000210): Train loss 2.423, Val loss 2.707\n",
      "Ep 3 (Step 000215): Train loss 2.325, Val loss 2.684\n",
      "Ep 3 (Step 000220): Train loss 2.261, Val loss 2.666\n",
      "Ep 3 (Step 000225): Train loss 2.188, Val loss 2.662\n",
      "Ep 3 (Step 000230): Train loss 2.159, Val loss 2.639\n",
      "Ep 3 (Step 000235): Train loss 2.194, Val loss 2.629\n",
      "Ep 3 (Step 000240): Train loss 2.078, Val loss 2.627\n",
      "Ep 3 (Step 000245): Train loss 2.192, Val loss 2.608\n",
      "Ep 3 (Step 000250): Train loss 2.141, Val loss 2.601\n",
      "Ep 3 (Step 000255): Train loss 2.091, Val loss 2.596\n",
      "Ep 3 (Step 000260): Train loss 2.100, Val loss 2.558\n",
      "Ep 3 (Step 000265): Train loss 2.014, Val loss 2.534\n",
      "Ep 3 (Step 000270): Train loss 2.029, Val loss 2.529\n",
      "Ep 3 (Step 000275): Train loss 1.992, Val loss 2.508\n",
      "Ep 3 (Step 000280): Train loss 1.948, Val loss 2.497\n",
      "Ep 3 (Step 000285): Train loss 1.858, Val loss 2.484\n",
      "Ep 3 (Step 000290): Train loss 1.788, Val loss 2.469\n",
      "The included lumbar and normal in diameter. The included abdomen is normal in diameter. The included abdomen is normal in position. The included abdomen are normal. The included abdomen is normal in diameter. The included abdomen is normal. The included abdomen is normal in position. The included\n",
      "Ep 4 (Step 000295): Train loss 2.006, Val loss 2.469\n",
      "Ep 4 (Step 000300): Train loss 1.894, Val loss 2.463\n",
      "Ep 4 (Step 000305): Train loss 1.775, Val loss 2.448\n",
      "Ep 4 (Step 000310): Train loss 1.779, Val loss 2.456\n",
      "Ep 4 (Step 000315): Train loss 1.728, Val loss 2.449\n",
      "Ep 4 (Step 000320): Train loss 1.766, Val loss 2.449\n",
      "Ep 4 (Step 000325): Train loss 1.675, Val loss 2.435\n",
      "Ep 4 (Step 000330): Train loss 1.542, Val loss 2.442\n",
      "Ep 4 (Step 000335): Train loss 1.707, Val loss 2.420\n",
      "Ep 4 (Step 000340): Train loss 1.645, Val loss 2.407\n",
      "Ep 4 (Step 000345): Train loss 1.610, Val loss 2.395\n",
      "Ep 4 (Step 000350): Train loss 1.644, Val loss 2.410\n",
      "Ep 4 (Step 000355): Train loss 1.583, Val loss 2.405\n",
      "Ep 4 (Step 000360): Train loss 1.570, Val loss 2.383\n",
      "Ep 4 (Step 000365): Train loss 1.581, Val loss 2.374\n",
      "Ep 4 (Step 000370): Train loss 1.516, Val loss 2.358\n",
      "Ep 4 (Step 000375): Train loss 1.449, Val loss 2.351\n",
      "Ep 4 (Step 000380): Train loss 1.450, Val loss 2.366\n",
      "Ep 4 (Step 000385): Train loss 1.439, Val loss 2.346\n",
      "The included lumbar and lumbosacral spine is normal. The included lumbosacral spine are normal. Conclusions and recommendations: Mild left sacral spine, likely incidental finding and lumbosacral spine, with mild bilateral coxofemoral\n",
      "Ep 5 (Step 000390): Train loss 1.344, Val loss 2.340\n",
      "Ep 5 (Step 000395): Train loss 1.400, Val loss 2.335\n",
      "Ep 5 (Step 000400): Train loss 1.279, Val loss 2.337\n",
      "Ep 5 (Step 000405): Train loss 1.345, Val loss 2.337\n",
      "Ep 5 (Step 000410): Train loss 1.378, Val loss 2.345\n",
      "Ep 5 (Step 000415): Train loss 1.233, Val loss 2.335\n",
      "Ep 5 (Step 000420): Train loss 1.247, Val loss 2.355\n",
      "Ep 5 (Step 000425): Train loss 1.235, Val loss 2.332\n",
      "Ep 5 (Step 000430): Train loss 1.264, Val loss 2.354\n",
      "Ep 5 (Step 000435): Train loss 1.247, Val loss 2.346\n",
      "Ep 5 (Step 000440): Train loss 1.132, Val loss 2.344\n",
      "Ep 5 (Step 000445): Train loss 1.192, Val loss 2.347\n",
      "Ep 5 (Step 000450): Train loss 1.161, Val loss 2.336\n",
      "Ep 5 (Step 000455): Train loss 1.135, Val loss 2.332\n",
      "Ep 5 (Step 000460): Train loss 1.162, Val loss 2.323\n",
      "Ep 5 (Step 000465): Train loss 1.080, Val loss 2.310\n",
      "Ep 5 (Step 000470): Train loss 1.033, Val loss 2.313\n",
      "Ep 5 (Step 000475): Train loss 1.000, Val loss 2.333\n",
      "Ep 5 (Step 000480): Train loss 0.922, Val loss 2.337\n",
      "The included lumbar and lumbosacral degenerative change.<|endoftext|>Findings: Three view thorax images dated June 6, 2023 are provided for review (total of 3 images). A mild diffuse bronchial pattern is present. The lungs are not hyperin\n",
      "Ep 6 (Step 000485): Train loss 0.998, Val loss 2.342\n",
      "Ep 6 (Step 000490): Train loss 0.972, Val loss 2.340\n",
      "Ep 6 (Step 000495): Train loss 0.963, Val loss 2.324\n",
      "Ep 6 (Step 000500): Train loss 0.863, Val loss 2.356\n",
      "Ep 6 (Step 000505): Train loss 0.919, Val loss 2.354\n",
      "Ep 6 (Step 000510): Train loss 0.792, Val loss 2.382\n",
      "Ep 6 (Step 000515): Train loss 0.849, Val loss 2.374\n",
      "Ep 6 (Step 000520): Train loss 0.793, Val loss 2.377\n",
      "Ep 6 (Step 000525): Train loss 0.815, Val loss 2.409\n",
      "Ep 6 (Step 000530): Train loss 0.793, Val loss 2.401\n",
      "Ep 6 (Step 000535): Train loss 0.751, Val loss 2.413\n",
      "Ep 6 (Step 000540): Train loss 0.715, Val loss 2.424\n",
      "Ep 6 (Step 000545): Train loss 0.739, Val loss 2.411\n",
      "Ep 6 (Step 000550): Train loss 0.681, Val loss 2.395\n",
      "Ep 6 (Step 000555): Train loss 0.678, Val loss 2.429\n",
      "Ep 6 (Step 000560): Train loss 0.747, Val loss 2.404\n",
      "Ep 6 (Step 000565): Train loss 0.700, Val loss 2.423\n",
      "Ep 6 (Step 000570): Train loss 0.688, Val loss 2.425\n",
      "Ep 6 (Step 000575): Train loss 0.661, Val loss 2.423\n",
      "Ep 6 (Step 000580): Train loss 0.630, Val loss 2.419\n",
      "The included lumbar and lumbosacral spine. The femoral heads are smooth and rounded and well-seated in their respective acetabulum. No femoral heads are noted. No bony lumbosacral junction are noted. No bony l\n",
      "Ep 7 (Step 000585): Train loss 0.597, Val loss 2.422\n",
      "Ep 7 (Step 000590): Train loss 0.589, Val loss 2.435\n",
      "Ep 7 (Step 000595): Train loss 0.550, Val loss 2.468\n",
      "Ep 7 (Step 000600): Train loss 0.551, Val loss 2.474\n",
      "Ep 7 (Step 000605): Train loss 0.475, Val loss 2.478\n",
      "Ep 7 (Step 000610): Train loss 0.476, Val loss 2.498\n",
      "Ep 7 (Step 000615): Train loss 0.498, Val loss 2.490\n",
      "Ep 7 (Step 000620): Train loss 0.419, Val loss 2.487\n",
      "Ep 7 (Step 000625): Train loss 0.463, Val loss 2.537\n",
      "Ep 7 (Step 000630): Train loss 0.420, Val loss 2.497\n",
      "Ep 7 (Step 000635): Train loss 0.442, Val loss 2.491\n",
      "Ep 7 (Step 000640): Train loss 0.407, Val loss 2.519\n",
      "Ep 7 (Step 000645): Train loss 0.496, Val loss 2.515\n",
      "Ep 7 (Step 000650): Train loss 0.420, Val loss 2.520\n",
      "Ep 7 (Step 000655): Train loss 0.448, Val loss 2.536\n",
      "Ep 7 (Step 000660): Train loss 0.440, Val loss 2.534\n",
      "Ep 7 (Step 000665): Train loss 0.400, Val loss 2.528\n",
      "Ep 7 (Step 000670): Train loss 0.354, Val loss 2.507\n",
      "Ep 7 (Step 000675): Train loss 0.362, Val loss 2.515\n",
      "The included lumbar and/2023 are provided for review, with coronal and sagittal plane reconstructions created for review. A broad-based roughly discoid homogenously enhancing soft tissue mass with irregular margins is present in the dorsal subcutaneous tissues to the left\n",
      "Ep 8 (Step 000680): Train loss 0.321, Val loss 2.544\n",
      "Ep 8 (Step 000685): Train loss 0.317, Val loss 2.564\n",
      "Ep 8 (Step 000690): Train loss 0.323, Val loss 2.581\n",
      "Ep 8 (Step 000695): Train loss 0.309, Val loss 2.578\n",
      "Ep 8 (Step 000700): Train loss 0.298, Val loss 2.597\n",
      "Ep 8 (Step 000705): Train loss 0.282, Val loss 2.595\n",
      "Ep 8 (Step 000710): Train loss 0.256, Val loss 2.627\n",
      "Ep 8 (Step 000715): Train loss 0.261, Val loss 2.622\n",
      "Ep 8 (Step 000720): Train loss 0.257, Val loss 2.639\n",
      "Ep 8 (Step 000725): Train loss 0.210, Val loss 2.637\n",
      "Ep 8 (Step 000730): Train loss 0.219, Val loss 2.646\n",
      "Ep 8 (Step 000735): Train loss 0.222, Val loss 2.656\n",
      "Ep 8 (Step 000740): Train loss 0.246, Val loss 2.674\n",
      "Ep 8 (Step 000745): Train loss 0.206, Val loss 2.669\n",
      "Ep 8 (Step 000750): Train loss 0.185, Val loss 2.716\n",
      "Ep 8 (Step 000755): Train loss 0.229, Val loss 2.676\n",
      "Ep 8 (Step 000760): Train loss 0.189, Val loss 2.689\n",
      "Ep 8 (Step 000765): Train loss 0.181, Val loss 2.657\n",
      "Ep 8 (Step 000770): Train loss 0.186, Val loss 2.691\n",
      "Ep 8 (Step 000775): Train loss 0.170, Val loss 2.692\n",
      "The included lumbar and lumbosacral degenerative change of the femoral heads. The femoral heads are smooth and rounded and well-seated in their respective acetabulum. The mild irregularity is normal. The coxofemoral joints are normal\n",
      "Ep 9 (Step 000780): Train loss 0.171, Val loss 2.737\n",
      "Ep 9 (Step 000785): Train loss 0.153, Val loss 2.750\n",
      "Ep 9 (Step 000790): Train loss 0.159, Val loss 2.749\n",
      "Ep 9 (Step 000795): Train loss 0.145, Val loss 2.769\n",
      "Ep 9 (Step 000800): Train loss 0.143, Val loss 2.742\n",
      "Ep 9 (Step 000805): Train loss 0.155, Val loss 2.747\n",
      "Ep 9 (Step 000810): Train loss 0.124, Val loss 2.775\n",
      "Ep 9 (Step 000815): Train loss 0.122, Val loss 2.796\n",
      "Ep 9 (Step 000820): Train loss 0.137, Val loss 2.753\n",
      "Ep 9 (Step 000825): Train loss 0.118, Val loss 2.813\n",
      "Ep 9 (Step 000830): Train loss 0.114, Val loss 2.777\n",
      "Ep 9 (Step 000835): Train loss 0.098, Val loss 2.798\n",
      "Ep 9 (Step 000840): Train loss 0.096, Val loss 2.786\n",
      "Ep 9 (Step 000845): Train loss 0.138, Val loss 2.813\n",
      "Ep 9 (Step 000850): Train loss 0.105, Val loss 2.798\n",
      "Ep 9 (Step 000855): Train loss 0.118, Val loss 2.804\n",
      "Ep 9 (Step 000860): Train loss 0.099, Val loss 2.795\n",
      "Ep 9 (Step 000865): Train loss 0.094, Val loss 2.804\n",
      "Ep 9 (Step 000870): Train loss 0.088, Val loss 2.829\n",
      "The included lumbar and is normal. The femoral heads are smooth and rounded. The femoral heads are smooth and well-seated in their respective acetabulum. The pelvis is normal. A large increase in opacity is present in the cranial and caud\n",
      "Ep 10 (Step 000875): Train loss 0.100, Val loss 2.836\n",
      "Ep 10 (Step 000880): Train loss 0.091, Val loss 2.847\n",
      "Ep 10 (Step 000885): Train loss 0.103, Val loss 2.833\n",
      "Ep 10 (Step 000890): Train loss 0.083, Val loss 2.854\n",
      "Ep 10 (Step 000895): Train loss 0.088, Val loss 2.870\n",
      "Ep 10 (Step 000900): Train loss 0.069, Val loss 2.848\n",
      "Ep 10 (Step 000905): Train loss 0.086, Val loss 2.855\n",
      "Ep 10 (Step 000910): Train loss 0.065, Val loss 2.857\n",
      "Ep 10 (Step 000915): Train loss 0.090, Val loss 2.864\n",
      "Ep 10 (Step 000920): Train loss 0.070, Val loss 2.891\n",
      "Ep 10 (Step 000925): Train loss 0.066, Val loss 2.874\n",
      "Ep 10 (Step 000930): Train loss 0.076, Val loss 2.890\n",
      "Ep 10 (Step 000935): Train loss 0.077, Val loss 2.902\n",
      "Ep 10 (Step 000940): Train loss 0.065, Val loss 2.915\n",
      "Ep 10 (Step 000945): Train loss 0.067, Val loss 2.884\n",
      "Ep 10 (Step 000950): Train loss 0.064, Val loss 2.909\n",
      "Ep 10 (Step 000955): Train loss 0.064, Val loss 2.909\n",
      "Ep 10 (Step 000960): Train loss 0.063, Val loss 2.933\n",
      "Ep 10 (Step 000965): Train loss 0.057, Val loss 2.906\n",
      "The included lumbar and is normal. The femoral condyles are normal. Conclusions and recommendations: Bilateral radial diaphyseal medullary sclerosis. This is most likely to represent bilateral panosteitis and is likely the source of clinical signs, less\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(CONFIG)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"The included lumbar and\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "58abed9c-21fc-4da4-a958-74f17b3d8793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc53b6-d9d9-41b1-aed7-f021f61dccf4",
   "metadata": {},
   "source": [
    "# Controlling randomness in the text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceab935-0231-4870-8f05-57d99b64562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"The included lumbar and\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5043d3-cba3-4f16-bbab-106da11c4024",
   "metadata": {},
   "source": [
    "- Even if we execute the `generate_text_simple` function above multiple times, the LLM will always generate the same outputs\n",
    "- We now introduce two concepts, so-called decoding strategies, to modify the `generate_text_simple`: *temperature scaling* and *top-k* sampling\n",
    "- These will allow the model to control the randomness and diversity of the generated text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4e833-79f5-4eb8-a5e9-d6d2e2a4c863",
   "metadata": {},
   "source": [
    "## Temperature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60efaba-b972-4113-8cef-f6d335fdb977",
   "metadata": {},
   "source": [
    "- Previously, we always sampled the token with the highest probability as the next token using `torch.argmax`\n",
    "- To add variety, we can sample the next token using The `torch.multinomial(probs, num_samples=1)`, sampling from a probability distribution\n",
    "- Here, each index's chance of being picked corresponds to its probability in the input tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4374da-38b5-4e60-9dcc-e7d3587f7527",
   "metadata": {},
   "source": [
    "- Here's a little recap of generating the next token, assuming a very small vocabulary for illustration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3043c278-6240-4b7a-99a8-9617ebb51e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a786d62d-cffa-4d3f-8612-0639133de230",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc8c8d-7a20-4ee0-a291-3bf262131b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f173f-122f-4e2b-b136-bd1595a818eb",
   "metadata": {},
   "source": [
    "- We can control the distribution and selection process via a concept called temperature scaling\n",
    "- \"Temperature scaling\" is just a fancy word for dividing the logits by a number greater than 0\n",
    "- Temperatures greater than 1 will result in more uniformly distributed token probabilities after applying the softmax\n",
    "- Temperatures smaller than 1 will result in more confident (sharper or more peaky) distributions after applying the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a62b1-64f9-43c8-b6b7-11c5fc8d4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dbfccd-ff42-437f-8699-7e341878c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be97fb-8db1-4e6c-b43f-53854888062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc4a61-8f82-44d5-a3d6-013d97f6e64c",
   "metadata": {},
   "source": [
    "The rescaled probabilities via temperature 5 are more uniformly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17fea1a-c9ae-4f24-9397-ccbd77fee9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3c9fd-866a-41fc-aa16-875b1bec695a",
   "metadata": {},
   "source": [
    "## Top k sampling\n",
    "\n",
    "To be able to use higher temperatures to increase output diversity and to reduce the probability of nonsensical sentences, we can restrict the sampled tokens to the top-k most likely tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335251bc-3926-4207-91d6-68e43e09bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fa381-2360-49ed-92d2-146eb6eb578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float('-inf')), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f648c9-edac-4e80-af11-2e39fdc4a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9720a9-e163-40b8-8240-c168770d9683",
   "metadata": {},
   "source": [
    "## Modifying the text generation function\n",
    "\n",
    "- The previous two subsections introduced temperature sampling and top-k sampling\n",
    "- Let's use these two concepts to modify the `generate_simple` function we used to generate text via the LLM earlier, creating a new `generate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f587008-1bb7-46c3-9435-1042e595d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3250ce-b65a-4b58-a6d6-2d93fc906473",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"The included lumbar and\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ea726-bd1c-4f51-9960-5d5ea0d24211",
   "metadata": {},
   "source": [
    "## Loading and saving model weights in PyTorch\n",
    "\n",
    "The recommended way in PyTorch is to save the model weights, the so-called `state_dict` via by applying the `torch.save` function to the `.state_dict()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8f544-65b8-4586-af18-85e5d98bccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../models/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42211c64-eb7e-4daf-86b3-f3a596a9cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"../models/model.pth\", map_location=device, weights_only=True))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd4623-325c-4479-91df-5430847500ba",
   "metadata": {},
   "source": [
    "- It's common to train LLMs with adaptive optimizers like Adam or AdamW instead of regular SGD\n",
    "- These adaptive optimizers store additional parameters for each model weight, so it makes sense to save them as well in case we plan to continue the pretraining later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c608bf-fe1c-45a2-8e92-c22d41921ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"../models/model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0dfd7-6a24-4ba2-9a15-c6e606b64f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../models/model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
