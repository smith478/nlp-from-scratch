{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a9e4bd",
   "metadata": {},
   "source": [
    "Prior to running this notebook run: `ollama serve &`. This will start the Ollama server and allow you to interact with it through this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13201b23-bfbf-448e-9598-f1fd42ef35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "from utils import json_to_dataframe, json_to_string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c3c5a-fdfa-400f-b11f-6c1c4fddcd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/vector_veterinary_imaging_2.json'\n",
    "\n",
    "df = json_to_dataframe(filepath) \n",
    "rad_strings = json_to_string_list(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b6f5e-9982-45e2-b7e3-b1363bd4c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0330b5ba-dd87-48ad-bc9e-a3cd531df25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "findings = list(df['findings'])\n",
    "conclusions = list(df['conclusions_and_recommendations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647ea8f-0fc8-459c-a97f-9808fb2c1fcc",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171c12d-74f2-4cc5-adf5-ad6627298d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformerRetriever(dspy.Retrieve):\n",
    "    def __init__(self, model: str, findings: List[str], conclusions: List[str], k: int):\n",
    "        self.model = model if isinstance(model, SentenceTransformer) else SentenceTransformer(model, trust_remote_code=True)\n",
    "        self.findings = findings\n",
    "        self.conclusions = conclusions\n",
    "        self.k = k\n",
    "        self.embeddings = None\n",
    "        self.init_embeddings()\n",
    "\n",
    "    def init_embeddings(self):\n",
    "        self.embeddings = self.model.encode(self.findings)\n",
    "\n",
    "    def forward(self, query: str, k: int) -> List[Dict[str, Union[str, float]]]:\n",
    "        query_embedding = self.model.encode([query])\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_k_indices:\n",
    "            results.append({\n",
    "                'finding': self.findings[idx],\n",
    "                'conclusion': self.conclusions[idx],\n",
    "                'score': float(similarities[idx])\n",
    "            })\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99c231-1c02-4a25-bd49-0ea166144e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# vectorizer = \"dunzhang/stella_en_400M_v5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d5901-0986-427e-b884-e7c906cd3f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_model = SentenceTransformerRetriever(model=vectorizer, findings=findings, conclusions=conclusions, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e99355-5d3f-420b-b401-679a33e41651",
   "metadata": {},
   "outputs": [],
   "source": [
    "findings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a33d5c-cca5-4211-93b0-55405fb01539",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_model.forward(query=findings[1], k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d0d51-5e70-4f9a-8ca7-674d48986df6",
   "metadata": {},
   "source": [
    "# Subtask breakdown\n",
    "\n",
    "The idea here is to break the task of generating the `conclusions and recommendations` from the `findings` section down into smaller more manageable tasks and then recombining. This will also allow us to investigate each component and work on addressing specific issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b36d8b-e530-4c03-873c-9da8e3901388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from typing import List, Dict\n",
    "import re\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe0656-98ba-4289-8ba7-4443d52e2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Finding:\n",
    "    \"\"\"Structure to hold parsed findings\"\"\"\n",
    "    anatomical_location: str\n",
    "    observation: str\n",
    "    attributes: Dict[str, str]\n",
    "\n",
    "class ExtractKeyFindings(dspy.Signature):\n",
    "    \"\"\"Extract and categorize key findings that require addressing in conclusions.\"\"\"\n",
    "    \n",
    "    finding = dspy.InputField(desc=\"Complete findings section of radiology report\")\n",
    "    similar_examples = dspy.InputField(desc=\"Similar example reports for reference\")\n",
    "    \n",
    "    abnormal_findings = dspy.OutputField(desc=\"List of abnormal findings that need to be addressed\")\n",
    "    normal_relevant = dspy.OutputField(desc=\"List of relevant normal findings that provide important context\")\n",
    "    \n",
    "class AssessClinicalSignificance(dspy.Signature):\n",
    "    \"\"\"Assess the clinical significance of the identified findings.\"\"\"\n",
    "    \n",
    "    abnormal_findings = dspy.InputField(desc=\"List of abnormal findings\")\n",
    "    normal_relevant = dspy.InputField(desc=\"List of relevant normal findings\")\n",
    "    similar_examples = dspy.InputField(desc=\"Similar example reports for reference\")\n",
    "    \n",
    "    significance = dspy.OutputField(desc=\"Clinical significance of findings\")\n",
    "    risk_assessment = dspy.OutputField(desc=\"Assessment of any risks or concerns\")\n",
    "\n",
    "class GenerateRecommendations(dspy.Signature):\n",
    "    \"\"\"Generate specific recommendations based on findings and their significance.\"\"\"\n",
    "    \n",
    "    abnormal_findings = dspy.InputField()\n",
    "    significance = dspy.InputField()\n",
    "    risk_assessment = dspy.InputField()\n",
    "    similar_examples = dspy.InputField()\n",
    "    \n",
    "    recommendations = dspy.OutputField(desc=\"Specific, actionable recommendations\")\n",
    "\n",
    "class ValidateAndFormatConclusions(dspy.Signature):\n",
    "    \"\"\"Validate and format the final conclusions section.\"\"\"\n",
    "    \n",
    "    original_finding = dspy.InputField()\n",
    "    abnormal_findings = dspy.InputField()\n",
    "    significance = dspy.InputField()\n",
    "    recommendations = dspy.InputField()\n",
    "    \n",
    "    final_conclusions = dspy.OutputField(desc=\"Complete, formatted conclusions section\")\n",
    "\n",
    "class RadiologyReportGenerator(dspy.Module):\n",
    "    \"\"\"Complete pipeline for generating radiology report conclusions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.extract = ExtractKeyFindings()\n",
    "        self.assess = AssessClinicalSignificance()\n",
    "        self.recommend = GenerateRecommendations()\n",
    "        self.validate = ValidateAndFormatConclusions()\n",
    "        \n",
    "        # Validation rules\n",
    "        self.validation_rules = [\n",
    "            self.validate_findings_addressed,\n",
    "            self.validate_recommendation_format,\n",
    "            self.validate_medical_terminology,\n",
    "            self.validate_length,\n",
    "            self.validate_structure\n",
    "        ]\n",
    "    \n",
    "    def forward(self, finding: str, similar_examples: List[dict]) -> str:\n",
    "        # Extract key findings\n",
    "        findings_output = self.extract(\n",
    "            finding=finding,\n",
    "            similar_examples=similar_examples\n",
    "        )\n",
    "        \n",
    "        # Assess clinical significance\n",
    "        significance_output = self.assess(\n",
    "            abnormal_findings=findings_output.abnormal_findings,\n",
    "            normal_relevant=findings_output.normal_relevant,\n",
    "            similar_examples=similar_examples\n",
    "        )\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = self.recommend(\n",
    "            abnormal_findings=findings_output.abnormal_findings,\n",
    "            significance=significance_output.significance,\n",
    "            risk_assessment=significance_output.risk_assessment,\n",
    "            similar_examples=similar_examples\n",
    "        )\n",
    "        \n",
    "        # Validate and format final conclusions\n",
    "        final_output = self.validate(\n",
    "            original_finding=finding,\n",
    "            abnormal_findings=findings_output.abnormal_findings,\n",
    "            significance=significance_output.significance,\n",
    "            recommendations=recommendations.recommendations\n",
    "        )\n",
    "        \n",
    "        # Run validation checks\n",
    "        self.run_validations(\n",
    "            original_finding=finding,\n",
    "            final_output=final_output.final_conclusions\n",
    "        )\n",
    "        \n",
    "        return final_output.final_conclusions\n",
    "    \n",
    "    def validate_findings_addressed(self, finding: str, conclusion: str) -> bool:\n",
    "        \"\"\"Validate that all abnormal findings are addressed in conclusions.\"\"\"\n",
    "        # Extract key medical terms from finding\n",
    "        finding_terms = self.extract_medical_terms(finding)\n",
    "        \n",
    "        # Check if key terms appear in conclusion\n",
    "        conclusion_terms = self.extract_medical_terms(conclusion)\n",
    "        \n",
    "        missing_terms = [term for term in finding_terms if term not in conclusion_terms]\n",
    "        if missing_terms:\n",
    "            raise ValidationError(f\"Findings not addressed: {missing_terms}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def validate_recommendation_format(self, conclusion: str) -> bool:\n",
    "        \"\"\"Validate recommendation format and actionability.\"\"\"\n",
    "        # Check for specific recommendation indicators\n",
    "        recommendation_patterns = [\n",
    "            r\"recommend[s]?\\s\",\n",
    "            r\"consider[ing]?\\s\",\n",
    "            r\"warrant[s]?\\s\",\n",
    "            r\"suggest[s]?\\s\"\n",
    "        ]\n",
    "        \n",
    "        has_recommendations = any(re.search(pattern, conclusion, re.IGNORECASE) \n",
    "                                for pattern in recommendation_patterns)\n",
    "        \n",
    "        if not has_recommendations:\n",
    "            raise ValidationError(\"No clear recommendations found\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def validate_medical_terminology(self, conclusion: str) -> bool:\n",
    "        \"\"\"Validate proper use of medical terminology.\"\"\"\n",
    "        # This would connect to a medical terminology database\n",
    "        # For now, we'll use a simple check\n",
    "        required_terms = [\"examination\", \"findings\", \"indicates\", \"suggests\"]\n",
    "        \n",
    "        missing_terms = [term for term in required_terms \n",
    "                        if term.lower() not in conclusion.lower()]\n",
    "        \n",
    "        if missing_terms:\n",
    "            raise ValidationError(f\"Missing professional medical terms: {missing_terms}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def validate_length(self, conclusion: str) -> bool:\n",
    "        \"\"\"Validate conclusion length is appropriate.\"\"\"\n",
    "        words = conclusion.split()\n",
    "        if len(words) < 50 or len(words) > 300:\n",
    "            raise ValidationError(\"Conclusion length outside acceptable range\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def validate_structure(self, conclusion: str) -> bool:\n",
    "        \"\"\"Validate conclusion structure and formatting.\"\"\"\n",
    "        required_sections = [\n",
    "            \"findings\",\n",
    "            \"significance\",\n",
    "            \"recommendations\"\n",
    "        ]\n",
    "        \n",
    "        missing_sections = [section for section in required_sections \n",
    "                          if not self._has_section(conclusion, section)]\n",
    "        \n",
    "        if missing_sections:\n",
    "            raise ValidationError(f\"Missing required sections: {missing_sections}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _has_section(self, text: str, section: str) -> bool:\n",
    "        \"\"\"Helper method to check if a section exists in the text.\"\"\"\n",
    "        section_patterns = [\n",
    "            f\"{section}:\",\n",
    "            f\"{section.capitalize()}:\",\n",
    "            f\"{section.upper()}:\"\n",
    "        ]\n",
    "        \n",
    "        return any(pattern in text for pattern in section_patterns)\n",
    "    \n",
    "    def extract_medical_terms(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract medical terms from text.\n",
    "        This is a simplified version - in practice, you'd want to use\n",
    "        a medical NLP library or terminology database.\"\"\"\n",
    "        # Example implementation\n",
    "        terms = []\n",
    "        # Add actual medical term extraction logic\n",
    "        return terms\n",
    "    \n",
    "    def run_validations(self, original_finding: str, final_output: str):\n",
    "        \"\"\"Run all validation checks on the output.\"\"\"\n",
    "        for validation_rule in self.validation_rules:\n",
    "            if validation_rule.__name__ == 'validate_findings_addressed':\n",
    "                validation_rule(original_finding, final_output)\n",
    "            else:\n",
    "                validation_rule(final_output)\n",
    "\n",
    "class ValidationError(Exception):\n",
    "    \"\"\"Custom exception for validation errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Example prompt templates for each component\n",
    "EXTRACT_PROMPT = \"\"\"\n",
    "Analyze the radiological findings and identify:\n",
    "1. All abnormal findings that require attention\n",
    "2. Normal findings that provide important context\n",
    "\n",
    "Format your response as two separate lists.\n",
    "Use precise medical terminology and maintain professional tone.\n",
    "\"\"\"\n",
    "\n",
    "ASSESS_PROMPT = \"\"\"\n",
    "Based on the identified findings, provide:\n",
    "1. Clinical significance of each abnormal finding\n",
    "2. Overall risk assessment considering the combination of findings\n",
    "\n",
    "Consider both immediate and potential long-term implications.\n",
    "\"\"\"\n",
    "\n",
    "RECOMMEND_PROMPT = \"\"\"\n",
    "Generate specific, actionable recommendations that:\n",
    "1. Address each significant finding\n",
    "2. Prioritize based on clinical urgency\n",
    "3. Include clear next steps for follow-up\n",
    "\n",
    "Recommendations should be concrete and specific.\n",
    "\"\"\"\n",
    "\n",
    "VALIDATE_PROMPT = \"\"\"\n",
    "Create a final conclusions section that:\n",
    "1. Integrates all components coherently\n",
    "2. Maintains professional medical tone\n",
    "3. Follows standard radiology report format\n",
    "4. Is concise and focused\n",
    "\n",
    "Ensure all significant findings are addressed and recommendations are clear.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13277c0a-95f5-4272-bfc4-46fcedf5f66f",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4157dbf-a735-4d12-aeed-dce0b6b1a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# language_model = \"qwen2.5\"\n",
    "language_model = \"gemma2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c62e8e-3df7-466e-9621-5bcbb0c9c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = dspy.OllamaLocal(\n",
    "    base_url='http://127.0.0.1:11434',\n",
    "    timeout_s=500,\n",
    "    model=language_model,\n",
    "    model_type='text',\n",
    "    max_tokens=1024,\n",
    "    num_ctx=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3dfda-52ca-4312-b5a6-8d014796c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DSPy to use Ollama\n",
    "dspy.settings.configure(lm=ollama_model, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c998555-0865-4c98-a00b-417a69a7b431",
   "metadata": {},
   "source": [
    "## Extract key findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812955b1-947f-4677-99a0-a5b887e43d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyFindingsModule(dspy.Module):\n",
    "    def __init__(self, retriever):\n",
    "        super().__init__()\n",
    "        self.extract_key_findings = dspy.Predict(ExtractKeyFindings)\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def forward(self, finding: str) -> dict:\n",
    "        # Retrieve similar examples\n",
    "        retrieved = self.retriever(finding, k=3)\n",
    "        \n",
    "        # Format examples for prompt\n",
    "        examples_text = \"\"\n",
    "        for i, ex in enumerate(retrieved, 1):\n",
    "            examples_text += f\"Example {i}:\\n\"\n",
    "            examples_text += f\"Finding: {ex['finding']}\\n\"\n",
    "            examples_text += f\"Conclusion: {ex['conclusion']}\\n\\n\"\n",
    "\n",
    "        # Extract key findings\n",
    "        prediction = self.extract_key_findings(\n",
    "            finding=finding,\n",
    "            similar_examples=examples_text\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'finding': finding,\n",
    "            'abnormal_findings': prediction.abnormal_findings,\n",
    "            'normal_relevant': prediction.normal_relevant,\n",
    "            'similar_examples': retrieved\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71037855-7adb-4c65-a6e8-40f21c868e66",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load data\n",
    "df = json_to_dataframe(filepath)\n",
    "findings = list(df['findings'])\n",
    "conclusions = list(df['conclusions_and_recommendations'])\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = SentenceTransformerRetriever(\n",
    "    model=vectorizer,\n",
    "    findings=findings,\n",
    "    conclusions=conclusions,\n",
    "    k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08218ee6-c579-41fb-9cc7-cb5b02bc4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_findings_extractor = KeyFindingsModule(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3232618f-5a43-4be6-8b66-213a575ba6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "findings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a9091e-991f-49d6-b69d-335ecc59ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_findings_example = key_findings_extractor(findings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c11b27-c238-4534-b584-2f9b65c9e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key_findings_example['abnormal_findings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be2178-9cfd-43db-a4cc-aec5275cecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key_findings_example['normal_relevant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3fefd-0745-4f7a-afaf-f80afc72b436",
   "metadata": {},
   "source": [
    "## Assess clinical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d6cd6b-aea7-49a0-ae78-9ae49d5c9ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssessClinicalSignificanceModule(dspy.Module):\n",
    "    def __init__(self, retriever):\n",
    "        super().__init__()\n",
    "        self.extract_key_findings = dspy.Predict(ExtractKeyFindings)\n",
    "        self.assess_clinical_findings = dspy.Predict(AssessClinicalSignificance)\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def forward(self, finding: str) -> dict:\n",
    "        # Retrieve similar examples\n",
    "        retrieved = self.retriever(finding, k=3)\n",
    "        \n",
    "        # Format examples for prompt\n",
    "        examples_text = \"\"\n",
    "        for i, ex in enumerate(retrieved, 1):\n",
    "            examples_text += f\"Example {i}:\\n\"\n",
    "            examples_text += f\"Finding: {ex['finding']}\\n\"\n",
    "            examples_text += f\"Conclusion: {ex['conclusion']}\\n\\n\"\n",
    "\n",
    "        # Extract key findings\n",
    "        key_findings = self.extract_key_findings(\n",
    "            finding=finding,\n",
    "            similar_examples=examples_text\n",
    "        )\n",
    "\n",
    "        # Assess clinical significance\n",
    "        clinical_significance = self.assess_clinical_findings(\n",
    "            abnormal_findings = key_findings.abnormal_findings,\n",
    "            normal_relevant = key_findings.normal_relevant,\n",
    "            similar_examples = examples_text\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'finding': finding,\n",
    "            'abnormal_findings': key_findings.abnormal_findings,\n",
    "            'normal_relevant': key_findings.normal_relevant,\n",
    "            'significance': clinical_significance.significance,\n",
    "            'risk_assessment': clinical_significance.risk_assessment,\n",
    "            'similar_examples': retrieved\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee057a20-d048-4c11-8a19-7dcb6a40af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_clinical_significance = AssessClinicalSignificanceModule(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d2850-8e73-4b4e-8128-4db39d840556",
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_significance_example = assess_clinical_significance(findings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e29c4e7-2e84-498b-ae66-779fcd36ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assess_significance_example['significance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0d2b99-898a-4b33-9e20-438fd5168a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assess_significance_example['risk_assessment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bddc10-98dc-427e-8a6a-cbfa005463dd",
   "metadata": {},
   "source": [
    "## Generate recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eeee65-2d7d-46bc-91df-745f1a852349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateRecommendationsModule(dspy.Module):\n",
    "    def __init__(self, retriever):\n",
    "        super().__init__()\n",
    "        self.extract_key_findings = dspy.Predict(ExtractKeyFindings)\n",
    "        self.assess_clinical_findings = dspy.Predict(AssessClinicalSignificance)\n",
    "        self.generate_recommendations = dspy.Predict(GenerateRecommendations)\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def forward(self, finding: str) -> dict:\n",
    "        # Retrieve similar examples\n",
    "        retrieved = self.retriever(finding, k=3)\n",
    "        \n",
    "        # Format examples for prompt\n",
    "        examples_text = \"\"\n",
    "        for i, ex in enumerate(retrieved, 1):\n",
    "            examples_text += f\"Example {i}:\\n\"\n",
    "            examples_text += f\"Finding: {ex['finding']}\\n\"\n",
    "            examples_text += f\"Conclusion: {ex['conclusion']}\\n\\n\"\n",
    "\n",
    "        # Extract key findings\n",
    "        key_findings = self.extract_key_findings(\n",
    "            finding=finding,\n",
    "            similar_examples=examples_text\n",
    "        )\n",
    "\n",
    "        # Assess clinical significance\n",
    "        clinical_significance = self.assess_clinical_findings(\n",
    "            abnormal_findings = key_findings.abnormal_findings,\n",
    "            normal_relevant = key_findings.normal_relevant,\n",
    "            similar_examples = examples_text\n",
    "        )\n",
    "\n",
    "        # Generate recommendations\n",
    "        generate_recommendations = self.generate_recommendations(\n",
    "            abnormal_findings = key_findings.abnormal_findings,\n",
    "            significance = clinical_significance.significance,\n",
    "            risk_assessment = clinical_significance.risk_assessment,\n",
    "            similar_examples = examples_text\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'finding': finding,\n",
    "            'abnormal_findings': key_findings.abnormal_findings,\n",
    "            'normal_relevant': key_findings.normal_relevant,\n",
    "            'significance': clinical_significance.significance,\n",
    "            'risk_assessment': clinical_significance.risk_assessment,\n",
    "            'recommendations': generate_recommendations.recommendations,\n",
    "            'similar_examples': retrieved\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9133e-45dd-4eb4-ad70-5dd1960ba2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_recommendations = GenerateRecommendationsModule(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effdd9f1-1c2c-48f3-9876-b0d5afd67f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_recommendations_example = generate_recommendations(findings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60385091-63d6-4a96-b375-f2bb5043f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_recommendations_example['recommendations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a7c13-8d54-416c-9ed8-6b8ebfa0e437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d77131-c71a-499b-83e8-66f5f793e144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d04c757-3915-4cff-918c-dd2e7338d177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499e299-de0c-4f6b-8dfe-c538ae6f079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the generator\n",
    "generator = RadiologyReportGenerator()\n",
    "\n",
    "# Generate conclusions\n",
    "try:\n",
    "    conclusions = generator(finding=finding_text, similar_examples=examples)\n",
    "    print(\"Successfully generated conclusions\")\n",
    "    print(conclusions)\n",
    "except ValidationError as e:\n",
    "    print(f\"Validation failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe6975-8a16-4c0e-bad2-2105d8caa815",
   "metadata": {},
   "outputs": [],
   "source": [
    "finding_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617dcefd-1a74-4bee-8359-55d40953a387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e06d54c-79b3-4030-8e9d-5ee0d4122c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c3266-3a6f-4ff8-8ff3-81aec5bc288d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5cb5c9-abca-4d45-8a1a-12856dfe35f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd80d22-4095-411b-9ac8-d22bc325ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateConclusions(dspy.Signature):\n",
    "    \"\"\"Given a radiology finding and similar examples, generate an appropriate conclusions and recommendations section.\n",
    "    The response should maintain a professional medical tone and follow the style of the examples.\"\"\"\n",
    "\n",
    "    finding = dspy.InputField(desc=\"Findings section of the radiology report.\")\n",
    "    similar_examples = dspy.InputField(desc=\"Similar examples of findings and corresponding conclusions and recommendations sections.\")\n",
    "    conclusions = dspy.OutputField(desc=\"The conclusions and recommendations section. Give the findings section above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92488ee-81ba-4e39-a514-9faa83ee234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadiologyModule(dspy.Module):\n",
    "    def __init__(self, retriever):\n",
    "        super().__init__()\n",
    "        self.generate_conclusion = dspy.Predict(GenerateConclusions)\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def forward(self, finding: str) -> dict:\n",
    "        # Retrieve similar examples\n",
    "        retrieved = self.retriever(finding, k=3)\n",
    "        \n",
    "        # Format examples for prompt\n",
    "        examples_text = \"\"\n",
    "        for i, ex in enumerate(retrieved, 1):\n",
    "            examples_text += f\"Example {i}:\\n\"\n",
    "            examples_text += f\"Finding: {ex['finding']}\\n\"\n",
    "            examples_text += f\"Conclusion: {ex['conclusion']}\\n\\n\"\n",
    "\n",
    "        # Generate new conclusion\n",
    "        prediction = self.generate_conclusion(\n",
    "            finding=finding,\n",
    "            similar_examples=examples_text\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'finding': finding,\n",
    "            'generated_conclusion': prediction.conclusions,\n",
    "            'similar_examples': retrieved\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab50c82-0b84-4ca6-bb49-beec9c71f26b",
   "metadata": {},
   "source": [
    "## Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8060bf-a509-4bba-932f-d8b1526bd1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rad_pipeline(filepath: str, vectorizer: str = \"sentence-transformers/all-MiniLM-L6-v2\", k: int = 3):\n",
    "    \"\"\"\n",
    "    Set up the complete radiology report generation pipeline\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = json_to_dataframe(filepath)\n",
    "    findings = list(df['findings'])\n",
    "    conclusions = list(df['conclusions_and_recommendations'])\n",
    "    \n",
    "    # Initialize retriever\n",
    "    retriever = SentenceTransformerRetriever(\n",
    "        model=vectorizer,\n",
    "        findings=findings,\n",
    "        conclusions=conclusions,\n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    # Create and return the radiology module\n",
    "    return RadiologyModule(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff638ac-cba8-48c5-9dcd-7b09ea02962d",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a094450-dd96-4a65-a258-2a18682e09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/vector_veterinary_imaging_2.json'\n",
    "rad_pipeline = setup_rad_pipeline(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a23e4e-2355-44dc-b1ee-4648e55f1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example finding\n",
    "test_finding = \"\"\"\n",
    "The thoracic cavity demonstrates normal cardiac silhouette size and shape. \n",
    "The pulmonary vasculature appears within normal limits. \n",
    "There is a mild interstitial pattern noted in the caudodorsal lung fields.\n",
    "No evidence of pleural effusion is noted.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4301a2ff-6e89-4fe9-9e31-2798d819c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rad_pipeline(test_finding)\n",
    "    \n",
    "print(\"Generated Conclusion:\")\n",
    "print(result['generated_conclusion'])\n",
    "print(\"\\nSimilar Examples Used:\")\n",
    "for i, example in enumerate(result['similar_examples'], 1):\n",
    "    print(f\"\\nExample {i} (Similarity Score: {example['score']:.3f}):\")\n",
    "    print(f\"Finding: {example['finding']}\")\n",
    "    print(f\"Conclusion: {example['conclusion']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da984fd5-2079-48fc-8991-fc91428a3c15",
   "metadata": {},
   "source": [
    "## Remove the example in question from retrieval\n",
    "\n",
    "As we go through a handful of examples, we want to make sure we don't include the example itself in the retrieval set. But it is fine to include all other examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7aa8fc-9d28-4351-9336-f4848b6b2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Dict, Union, Optional\n",
    "import pandas as pd\n",
    "\n",
    "class SentenceTransformerRetrieverWithExclusion(dspy.Retrieve):\n",
    "    def __init__(self, model: str, findings: List[str], conclusions: List[str], k: int):\n",
    "        self.model = model if isinstance(model, SentenceTransformer) else SentenceTransformer(model, trust_remote_code=True)\n",
    "        self.findings = findings\n",
    "        self.conclusions = conclusions\n",
    "        self.k = k\n",
    "        self.embeddings = None\n",
    "        self.excluded_indices = set()\n",
    "        self.init_embeddings()\n",
    "\n",
    "    def init_embeddings(self):\n",
    "        self.embeddings = self.model.encode(self.findings)\n",
    "        \n",
    "    def set_excluded_indices(self, indices: Optional[List[int]] = None):\n",
    "        \"\"\"Set indices to exclude from retrieval\"\"\"\n",
    "        self.excluded_indices = set(indices or [])\n",
    "        \n",
    "    def clear_excluded_indices(self):\n",
    "        \"\"\"Clear all excluded indices\"\"\"\n",
    "        self.excluded_indices = set()\n",
    "\n",
    "    def forward(self, query: str, k: int) -> List[Dict[str, Union[str, float]]]:\n",
    "        query_embedding = self.model.encode([query])\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Create mask for excluded indices\n",
    "        mask = np.ones_like(similarities, dtype=bool)\n",
    "        if self.excluded_indices:\n",
    "            mask[list(self.excluded_indices)] = False\n",
    "        \n",
    "        # Get top k indices excluding masked indices\n",
    "        masked_similarities = similarities.copy()\n",
    "        masked_similarities[~mask] = -np.inf\n",
    "        top_k_indices = np.argsort(masked_similarities)[-k:][::-1]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_k_indices:\n",
    "            results.append({\n",
    "                'finding': self.findings[idx],\n",
    "                'conclusion': self.conclusions[idx],\n",
    "                'score': float(similarities[idx])\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "def setup_rad_pipeline_with_exclusion(filepath: str, vectorizer: str = \"sentence-transformers/all-MiniLM-L6-v2\", k: int = 3):\n",
    "    \"\"\"\n",
    "    Set up the radiology pipeline with exclusion capability\n",
    "    \"\"\"\n",
    "    df = json_to_dataframe(filepath)\n",
    "    findings = list(df['findings'])\n",
    "    conclusions = list(df['conclusions_and_recommendations'])\n",
    "    \n",
    "    retriever = SentenceTransformerRetrieverWithExclusion(\n",
    "        model=vectorizer,\n",
    "        findings=findings,\n",
    "        conclusions=conclusions,\n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    return RadiologyModule(retriever), df\n",
    "\n",
    "def run_evaluation_with_exclusion(filepath: str, num_examples: int = 5, seed: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Run inference on a specified number of examples, excluding each example from its own retrieval set\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the JSON data file\n",
    "        num_examples: Number of examples to evaluate\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing the evaluation results\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        \n",
    "    # Setup pipeline with exclusion capability\n",
    "    rad_pipeline, df = setup_rad_pipeline_with_exclusion(filepath)\n",
    "    \n",
    "    # Randomly select examples\n",
    "    total_examples = len(df)\n",
    "    selected_indices = random.sample(range(total_examples), min(num_examples, total_examples))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx in selected_indices:\n",
    "        # Get the example\n",
    "        finding = df['findings'].iloc[idx]\n",
    "        actual_conclusion = df['conclusions_and_recommendations'].iloc[idx]\n",
    "        \n",
    "        # Set the current example to be excluded from retrieval\n",
    "        rad_pipeline.retriever.set_excluded_indices([idx])\n",
    "        \n",
    "        # Run inference\n",
    "        result = rad_pipeline(finding)\n",
    "        \n",
    "        # Clear exclusion for next iteration\n",
    "        rad_pipeline.retriever.clear_excluded_indices()\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'index': idx,\n",
    "            'finding': finding,\n",
    "            'actual_conclusion': actual_conclusion,\n",
    "            'generated_conclusion': result['generated_conclusion'],\n",
    "            'similar_examples': result['similar_examples']\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62efbf-ed5a-4bb7-95df-3f9d3a8f63d2",
   "metadata": {},
   "source": [
    "## TODO \n",
    "\n",
    "Pull out a handful of examples of prompts, actual conclusions, and predicted conclusions and have the model judge how close the actual and predicted responses are and also to come up with a new set of prompts that might work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6aa078-9177-44bf-bb81-00f4c716648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/vector_veterinary_imaging_2.json'\n",
    "\n",
    "# Run evaluation on 3 random examples\n",
    "results_df = run_evaluation_with_exclusion(filepath, num_examples=3, seed=42)\n",
    "\n",
    "# Print results\n",
    "for idx, row in results_df.iterrows():\n",
    "    print(f\"\\nExample {idx + 1}:\")\n",
    "    print(\"Finding:\")\n",
    "    print(row['finding'])\n",
    "    print(\"\\nActual Conclusion:\")\n",
    "    print(row['actual_conclusion'])\n",
    "    print(\"\\nGenerated Conclusion:\")\n",
    "    print(row['generated_conclusion'])\n",
    "    print(\"\\nSimilar Examples Used:\")\n",
    "    for i, example in enumerate(row['similar_examples'], 1):\n",
    "        print(f\"\\nReference {i} (Similarity Score: {example['score']:.3f}):\")\n",
    "        print(f\"Finding: {example['finding']}\")\n",
    "        print(f\"Conclusion: {example['conclusion']}\")\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e7ef3-ad07-44db-9a69-ccfd8ed941e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2092d672-a4c2-4a51-9ae4-5d84ca27634f",
   "metadata": {},
   "source": [
    "## TODO \n",
    "\n",
    "incorporate the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3506126-96d9-4c42-a7a6-54f1cb82e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation_results(results_df: pd.DataFrame, output_path: str, format: str = 'json'):\n",
    "    \"\"\"\n",
    "    Save evaluation results for LLM analysis\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame containing evaluation results\n",
    "        output_path: Path to save the results\n",
    "        format: Either 'json' or 'csv'\n",
    "    \"\"\"\n",
    "    # Prepare data for export\n",
    "    export_data = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        export_row = {\n",
    "            'finding': row['finding'],\n",
    "            'actual_conclusion': row['actual_conclusion'],\n",
    "            'generated_conclusion': row['generated_conclusion'],\n",
    "            'reference_examples': [\n",
    "                {\n",
    "                    'finding': ex['finding'],\n",
    "                    'conclusion': ex['conclusion'],\n",
    "                    'similarity_score': float(ex['score'])\n",
    "                } for ex in row['similar_examples']\n",
    "            ]\n",
    "        }\n",
    "        export_data.append(export_row)\n",
    "    \n",
    "    # Save in specified format\n",
    "    if format.lower() == 'json':\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(export_data, f, indent=2)\n",
    "    else:  # csv\n",
    "        # Flatten the reference examples\n",
    "        flat_data = []\n",
    "        for item in export_data:\n",
    "            flat_item = {\n",
    "                'finding': item['finding'],\n",
    "                'actual_conclusion': item['actual_conclusion'],\n",
    "                'generated_conclusion': item['generated_conclusion']\n",
    "            }\n",
    "            for i, ref in enumerate(item['reference_examples'], 1):\n",
    "                flat_item[f'ref_{i}_finding'] = ref['finding']\n",
    "                flat_item[f'ref_{i}_conclusion'] = ref['conclusion']\n",
    "                flat_item[f'ref_{i}_similarity'] = ref['similarity_score']\n",
    "            flat_data.append(flat_item)\n",
    "        pd.DataFrame(flat_data).to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e58bab1-eb66-45f2-b3c6-4bb72b19252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/vector_veterinary_imaging_2.json'\n",
    "    \n",
    "# Run evaluation and save results\n",
    "results_df = run_evaluation_with_exclusion(filepath, num_examples=20, seed=42)\n",
    "save_evaluation_results(results_df, 'evaluation_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120cbe26-82ab-43ec-9f38-dbdfb8ca9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56caaf3-f00f-46ea-94ca-beef5be20462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Union\n",
    "import pandas as pd\n",
    "import dspy\n",
    "from dspy.teleprompt import BootstrapFewShot, ValueEstimator\n",
    "import numpy as np\n",
    "\n",
    "class ConclusionScorer(dspy.Signature):\n",
    "    \"\"\"Rate the quality and accuracy of generated radiology conclusions compared to actual conclusions.\"\"\"\n",
    "    \n",
    "    finding = dspy.InputField()\n",
    "    generated_conclusion = dspy.InputField()\n",
    "    actual_conclusion = dspy.InputField()\n",
    "    \n",
    "    score = dspy.OutputField(desc=\"Score from 0-1 indicating similarity of content and style\")\n",
    "    reasoning = dspy.OutputField(desc=\"Explanation of the score and suggestions for improvement\")\n",
    "\n",
    "class RadiologyValueEstimator(ValueEstimator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scorer = dspy.Predict(ConclusionScorer)\n",
    "    \n",
    "    def forward(self, example, pred, trace=None):\n",
    "        score = self.scorer(\n",
    "            finding=example.finding,\n",
    "            generated_conclusion=pred.conclusions,\n",
    "            actual_conclusion=example.actual_conclusion\n",
    "        )\n",
    "        return float(score.score)\n",
    "\n",
    "def optimize_rad_prompt(filepath: str, num_bootstrap_examples: int = 10):\n",
    "    \"\"\"\n",
    "    Use DSPy's teleprompter to optimize the radiology prompt\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the dataset\n",
    "        num_bootstrap_examples: Number of examples to use for bootstrapping\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = json_to_dataframe(filepath)\n",
    "    \n",
    "    # Prepare training data\n",
    "    train_data = [\n",
    "        dspy.Example(\n",
    "            finding=row['findings'],\n",
    "            actual_conclusion=row['conclusions_and_recommendations']\n",
    "        ).with_inputs('finding')\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "    \n",
    "    # Initialize teleprompter components\n",
    "    bootstrapper = BootstrapFewShot(\n",
    "        demo_retriever='bm25',  # or 'sbert' for semantic search\n",
    "        k=num_bootstrap_examples\n",
    "    )\n",
    "    \n",
    "    value_estimator = RadiologyValueEstimator()\n",
    "    \n",
    "    # Create compiler configuration\n",
    "    config = dspy.TelepromptConfig(\n",
    "        metric=value_estimator,\n",
    "        max_bootstrapping_iterations=3,\n",
    "        max_rounds=5\n",
    "    )\n",
    "    \n",
    "    # Initialize and run teleprompter\n",
    "    teleprompter = dspy.Teleprompter(\n",
    "        GenerateConclusions,\n",
    "        bootstrapper=bootstrapper,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Optimize the prompt\n",
    "    optimized_program = teleprompter.compile(\n",
    "        train_data=train_data,\n",
    "        eval_data=train_data[:100]  # Use subset for evaluation\n",
    "    )\n",
    "    \n",
    "    return optimized_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ee13a-c48e-4030-bcb6-db11392e091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = '../../data/vector_veterinary_imaging_2.json'\n",
    "    \n",
    "    # Run evaluation and save results\n",
    "    results_df = run_evaluation_with_exclusion(filepath, num_examples=20, seed=42)\n",
    "    save_evaluation_results(results_df, 'evaluation_results.json')\n",
    "    \n",
    "    # Optimize prompt using teleprompter\n",
    "    optimized_program = optimize_rad_prompt(filepath)\n",
    "    \n",
    "    # Print optimized prompt\n",
    "    print(\"Optimized Prompt:\")\n",
    "    print(optimized_program.signature.instructions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
