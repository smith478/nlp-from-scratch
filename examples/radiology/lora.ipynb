{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf3a7a6e-5a10-4cab-aa36-befaccdc0f29",
   "metadata": {},
   "source": [
    "Resources:\n",
    "- [Medium blog post](https://medium.com/@tejpal.abhyuday/optimizing-language-model-fine-tuning-with-peft-qlora-integration-and-training-time-reduction-04df39dca72b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990bfe3e-a30f-426b-978a-d035d1a98b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import json_to_dataframe, json_to_string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98e73d-225f-4452-931f-1e83ce1adf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f5ae6-5e4d-4edb-bc7e-f409d068c77b",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113dc45-499b-4b4e-9b69-ed7478e6e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/vector_veterinary_imaging_2.json'\n",
    "\n",
    "df = json_to_dataframe(filepath) \n",
    "rad_strings = json_to_string_list(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4df04f-179b-44f4-8071-a2aa9be95bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, articles, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Get the token ID for <|endoftext|>\n",
    "        endoftext_token = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "        # Tokenize all articles with end-of-text token\n",
    "        all_tokens = []\n",
    "        for article in articles:\n",
    "            article_tokens = tokenizer.encode(article, allowed_special={\"<|endoftext|>\"})\n",
    "            all_tokens.extend(article_tokens + [endoftext_token])\n",
    "\n",
    "        # Use a sliding window to chunk the tokens into overlapping sequences of max_length\n",
    "        for i in range(0, len(all_tokens) - max_length, stride):\n",
    "            input_chunk = all_tokens[i:i + max_length]\n",
    "            target_chunk = all_tokens[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825772d-09dc-45e4-8c1a-3fdbbd8fa2d5",
   "metadata": {},
   "source": [
    "Configure quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d9071-746b-463d-8094-08aa3a3a4d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae98ab0-b292-4a14-b3df-003b4ba39f87",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab59d6-5f74-431d-acde-3ea764cc929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_map = {device_name: 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b74781-9821-45ac-8755-ffe9c45d1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='microsoft/phi-2'\n",
    "# model_name='microsoft/phi-1_5'\n",
    "# model_name = 'microsoft/Phi-3.5-mini-instruct'\n",
    "device_map = {\"\": 0}\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True) #,\n",
    "                                                      # use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e87a7f-77a7-498b-a738-40871fba25e3",
   "metadata": {},
   "source": [
    "Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00baca44-89b2-4cad-be05-8aefcb448056",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500c11f-f0dd-40d1-8de3-8221bd6fea58",
   "metadata": {},
   "source": [
    "Test zero-shot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b3832-c8e6-4b5c-a70e-1d5ddeba5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2b5f2-966b-46fa-bbab-6aa2fd83dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(articles, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(articles, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5479fe61-1e7d-40c0-9d0b-b968382aad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 10\n",
    "prompt = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n",
    "res = gen(original_model,formatted_prompt,100,)\n",
    "#print(res[0])\n",
    "output = res[0].split('Output:\\n')[1]\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e67755-8afc-4ba4-991a-d1972d7492e5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
