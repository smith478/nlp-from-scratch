{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf3a7a6e-5a10-4cab-aa36-befaccdc0f29",
   "metadata": {},
   "source": [
    "Resources:\n",
    "- [Medium blog post](https://medium.com/@tejpal.abhyuday/optimizing-language-model-fine-tuning-with-peft-qlora-integration-and-training-time-reduction-04df39dca72b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990bfe3e-a30f-426b-978a-d035d1a98b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "from utils import json_to_dataframe, json_to_string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98e73d-225f-4452-931f-1e83ce1adf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f5ae6-5e4d-4edb-bc7e-f409d068c77b",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113dc45-499b-4b4e-9b69-ed7478e6e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/vector_veterinary_imaging_2.json'\n",
    "\n",
    "df = json_to_dataframe(filepath) \n",
    "rad_strings = json_to_string_list(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4df04f-179b-44f4-8071-a2aa9be95bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GPTDatasetV1(Dataset):\n",
    "#     def __init__(self, articles, tokenizer, max_length, stride):\n",
    "#         self.input_ids = []\n",
    "#         self.target_ids = []\n",
    "\n",
    "#         # Get the token ID for <|endoftext|>\n",
    "#         # endoftext_token = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "#         endoftext_token = tokenizer.eos_token_id\n",
    "#         if endoftext_token is None:\n",
    "#             print('No end of text token included, creating one')\n",
    "#             # Handle the case where the model might not use \"<|endoftext|>\"\n",
    "#             endoftext_token = tokenizer.encode(\"<|endoftext|>\", add_special_tokens=False)[0]\n",
    "\n",
    "#         # Tokenize all articles with end-of-text token\n",
    "#         all_tokens = []\n",
    "#         for article in articles:\n",
    "#             article_tokens = tokenizer.encode(article, allowed_special={\"<|endoftext|>\"})\n",
    "#             all_tokens.extend(article_tokens + [endoftext_token])\n",
    "\n",
    "#         # Use a sliding window to chunk the tokens into overlapping sequences of max_length\n",
    "#         for i in range(0, len(all_tokens) - max_length, stride):\n",
    "#             input_chunk = all_tokens[i:i + max_length]\n",
    "#             target_chunk = all_tokens[i + 1: i + max_length + 1]\n",
    "#             self.input_ids.append(torch.tensor(input_chunk))\n",
    "#             self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.input_ids)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9aea9-a1bb-4d72-b584-1d0fde58c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, articles, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Get the token ID for <|endoftext|>\n",
    "        endoftext_token = tokenizer.eos_token_id\n",
    "        if endoftext_token is None:\n",
    "            print('No end of text token included, creating one')\n",
    "            endoftext_token = tokenizer.encode(\"<|endoftext|>\", add_special_tokens=False)[0]\n",
    "\n",
    "        # Tokenize all articles with end-of-text token\n",
    "        all_tokens = []\n",
    "        for article in articles:\n",
    "            # Remove 'allowed_special' since it's not recognized\n",
    "            article_tokens = tokenizer.encode(article, add_special_tokens=False)\n",
    "            all_tokens.extend(article_tokens + [endoftext_token])\n",
    "\n",
    "        # Use a sliding window to chunk the tokens into overlapping sequences of max_length\n",
    "        for i in range(0, len(all_tokens) - max_length, stride):\n",
    "            input_chunk = all_tokens[i:i + max_length]\n",
    "            target_chunk = all_tokens[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc1246b-31b2-4c18-9be9-fa0aaad0b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV2(Dataset):\n",
    "    def __init__(self, articles, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "\n",
    "        self.examples = []\n",
    "        self._prepare_examples(articles)\n",
    "\n",
    "        print(f\"Dataset initialized with {len(self.examples)} examples.\")\n",
    "\n",
    "    def _prepare_examples(self, articles):\n",
    "        print(f\"Preparing examples from {len(articles)} articles.\")\n",
    "        \n",
    "        # Concatenate all articles with end-of-text token\n",
    "        all_token_ids = []\n",
    "        for article in articles:\n",
    "            article_tokens = self.tokenizer.encode(article, add_special_tokens=False)\n",
    "            all_token_ids.extend(article_tokens + [self.tokenizer.eos_token_id])\n",
    "        \n",
    "        all_token_ids = torch.tensor(all_token_ids)\n",
    "        \n",
    "        # Create chunks of max_length with stride\n",
    "        for i in range(0, len(all_token_ids) - self.max_length + 1, self.stride):\n",
    "            chunk = all_token_ids[i:i + self.max_length]\n",
    "            self.examples.append(chunk)\n",
    "\n",
    "        print(f\"Created {len(self.examples)} examples.\")\n",
    "        print(f\"max_length: {self.max_length}, stride: {self.stride}\")\n",
    "        print(f\"Total concatenated length: {len(all_token_ids)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.examples):\n",
    "            raise IndexError(f\"Index {idx} out of range for dataset with {len(self.examples)} examples.\")\n",
    "        \n",
    "        input_ids = self.examples[idx]\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": input_ids.clone()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e5cbd-7546-4102-8433-ca285b2c30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_v2(articles, tokenizer, max_length, stride):\n",
    "    return GPTDatasetV2(articles, tokenizer, max_length, stride)\n",
    "\n",
    "def create_dataloader_v2(dataset, batch_size=4, shuffle=True, drop_last=False, num_workers=0):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0257aa-89dd-4e8c-a69d-edc1114cb12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below is to help debug any issues with the dataset or data loader\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# class GPTDatasetV4(Dataset):\n",
    "#     def __init__(self, articles, tokenizer, max_length, stride):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "#         self.stride = stride\n",
    "\n",
    "#         self.examples = []\n",
    "#         self._prepare_examples(articles)\n",
    "\n",
    "#     def _prepare_examples(self, articles):\n",
    "#         for article in articles:\n",
    "#             encodings = self.tokenizer(article, add_special_tokens=False, return_tensors=\"pt\")\n",
    "#             input_ids = encodings['input_ids'].squeeze()\n",
    "            \n",
    "#             # Use a sliding window to chunk the tokens into overlapping sequences\n",
    "#             for i in range(0, len(input_ids) - self.max_length + 1, self.stride):\n",
    "#                 chunk = input_ids[i:i + self.max_length]\n",
    "#                 self.examples.append(chunk)\n",
    "\n",
    "#         if not self.examples:\n",
    "#             raise ValueError(\"No valid examples were generated. Check your input data and parameters.\")\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.examples)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         input_ids = self.examples[idx]\n",
    "#         attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "#         return {\n",
    "#             \"input_ids\": input_ids,\n",
    "#             \"attention_mask\": attention_mask,\n",
    "#             \"labels\": input_ids.clone()\n",
    "#         }\n",
    "\n",
    "# def create_dataset_v4(articles, tokenizer, max_length, stride):\n",
    "#     try:\n",
    "#         return GPTDatasetV4(articles, tokenizer, max_length, stride)\n",
    "#     except ValueError as e:\n",
    "#         print(f\"Error creating dataset: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def create_dataloader_v4(dataset, batch_size=4, shuffle=True, drop_last=False, num_workers=0):\n",
    "#     if dataset is None or len(dataset) == 0:\n",
    "#         print(\"Dataset is empty or None. Cannot create DataLoader.\")\n",
    "#         return None\n",
    "\n",
    "#     return DataLoader(\n",
    "#         dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=shuffle,\n",
    "#         drop_last=drop_last,\n",
    "#         num_workers=num_workers,\n",
    "#         collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "#     )\n",
    "\n",
    "# # Usage:\n",
    "# train_dataset = create_dataset_v4(\n",
    "#     articles=train_data,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_length=max_length,\n",
    "#     stride=stride\n",
    "# )\n",
    "\n",
    "# val_dataset = create_dataset_v4(\n",
    "#     articles=val_data,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_length=max_length,\n",
    "#     stride=stride\n",
    "# )\n",
    "\n",
    "# if train_dataset:\n",
    "#     train_loader = create_dataloader_v4(\n",
    "#         dataset=train_dataset,\n",
    "#         batch_size=training_batch_size,\n",
    "#         shuffle=True,\n",
    "#         drop_last=True,\n",
    "#         num_workers=0\n",
    "#     )\n",
    "# else:\n",
    "#     print(\"Failed to create train_loader due to empty dataset.\")\n",
    "\n",
    "# if val_dataset:\n",
    "#     val_loader = create_dataloader_v4(\n",
    "#         dataset=val_dataset,\n",
    "#         batch_size=training_batch_size,\n",
    "#         shuffle=False,\n",
    "#         drop_last=False,\n",
    "#         num_workers=0\n",
    "#     )\n",
    "# else:\n",
    "#     print(\"Failed to create val_loader due to empty dataset.\")\n",
    "\n",
    "# # For use with PEFT trainer (only if datasets are not None):\n",
    "# if train_dataset and val_dataset:\n",
    "#     peft_trainer = transformers.Trainer(\n",
    "#         model=peft_model,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=val_dataset,\n",
    "#         args=peft_training_args,\n",
    "#         data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "#     )\n",
    "\n",
    "#     peft_trainer.train()\n",
    "# else:\n",
    "#     print(\"Cannot create PEFT trainer due to empty dataset(s).\")\n",
    "\n",
    "\n",
    "# print(f\"Number of training articles: {len(train_data)}\")\n",
    "# print(f\"Number of validation articles: {len(val_data)}\")\n",
    "# print(f\"Length of shortest training article: {min(len(article) for article in train_data)}\")\n",
    "# print(f\"Length of shortest validation article: {min(len(article) for article in val_data)}\")\n",
    "# print(f\"max_length: {max_length}\")\n",
    "# print(f\"stride: {stride}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825772d-09dc-45e4-8c1a-3fdbbd8fa2d5",
   "metadata": {},
   "source": [
    "## Configure quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d9071-746b-463d-8094-08aa3a3a4d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae98ab0-b292-4a14-b3df-003b4ba39f87",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab59d6-5f74-431d-acde-3ea764cc929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically detect and use GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set up the device map\n",
    "if torch.cuda.is_available():\n",
    "    device_map = \"auto\"  # This will automatically distribute the model across available GPUs\n",
    "else:\n",
    "    device_map = {\"\": device}  # Use the detected device (CPU in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd7ce0-dd75-48de-8700-60d2b394303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface login (if required)\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7dece-aeff-4b92-b586-6149b0b69f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'microsoft/phi-2'\n",
    "# model_name = 'microsoft/phi-1_5'\n",
    "# model_name = 'microsoft/Phi-3.5-mini-instruct'\n",
    "model_name = 'google/gemma-2-9b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b74781-9821-45ac-8755-ffe9c45d1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where the model will be saved locally\n",
    "local_model_path = os.path.join('../..', 'models', model_name.replace('/', '-'))\n",
    "\n",
    "# Check if the model exists locally\n",
    "if os.path.exists(local_model_path):\n",
    "    print(f\"Loading model from local path: {local_model_path}\")\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(\n",
    "        local_model_path,\n",
    "        device_map=device_map,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    print(f\"Downloading model from {model_name}\")\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    # Save the model locally\n",
    "    original_model.save_pretrained(local_model_path)\n",
    "    print(f\"Model saved to {local_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e87a7f-77a7-498b-a738-40871fba25e3",
   "metadata": {},
   "source": [
    "Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00baca44-89b2-4cad-be05-8aefcb448056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    "    use_fast=False\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500c11f-f0dd-40d1-8de3-8221bd6fea58",
   "metadata": {},
   "source": [
    "## Test zero-shot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b3832-c8e6-4b5c-a70e-1d5ddeba5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2b5f2-966b-46fa-bbab-6aa2fd83dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(articles, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(articles, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa96bb3f-4e37-4259-a2ff-419d2cc4d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d761d0-e9af-4030-8985-9f211a292ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_strings[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec8e85-7cc4-45ec-b201-b348d136967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, num_return_sequences=1, temperature=0.7, top_k=50, top_p=0.95):\n",
    "    # Encode the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    generated_texts = [tokenizer.decode(seq, skip_special_tokens=True) for seq in output]\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ff72e-042e-4305-9716-f2ee91d3171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your starting text\n",
    "prompt = \"Findings: Orthogonal pelvis and orthogonal right shoulder and lateral left shoulder images\"\n",
    "\n",
    "# Generate text\n",
    "generated_texts = generate_text(original_model, tokenizer, prompt, max_new_tokens=50)\n",
    "\n",
    "# Print the generated text\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated text {i+1}:\")\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6fa1b8-ec74-4514-8f16-8e95101658af",
   "metadata": {},
   "source": [
    "Snippet from actual text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca8804-460b-476e-a806-94bd481146bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_strings[index][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e67755-8afc-4ba4-991a-d1972d7492e5",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0421d909-429f-45dc-bc72-e0f4ef83d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = original_model.config.max_position_embeddings\n",
    "print(f\"Context length: {context_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87e7ca-6b3d-4898-8ec3-698815721cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(rad_strings))\n",
    "train_data = rad_strings[:split_idx]\n",
    "val_data = rad_strings[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc964ae-3399-4607-984a-e7da69590f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch_size = 8\n",
    "max_length = context_length\n",
    "stride = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93dbabe-84ae-4a2c-a7c1-8ea98b1429e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=training_batch_size,\n",
    "    max_length=max_length,\n",
    "    stride=stride,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=training_batch_size,\n",
    "    max_length=max_length,\n",
    "    stride=stride,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0a7a4-1a29-421f-81d1-b0318cadf7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c1f61-6858-420f-83cd-93747eef67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d7f20-a9e1-4dc2-a4e5-3879531cc92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset_v2(\n",
    "    articles=train_data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    stride=stride\n",
    ")\n",
    "\n",
    "val_dataset = create_dataset_v2(\n",
    "    articles=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    stride=stride\n",
    ")\n",
    "\n",
    "# train_loader = create_dataloader_v2(\n",
    "#     dataset=train_dataset,\n",
    "#     batch_size=training_batch_size,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     num_workers=0\n",
    "# )\n",
    "\n",
    "# val_loader = create_dataloader_v2(\n",
    "#     dataset=val_dataset,\n",
    "#     batch_size=training_batch_size,\n",
    "#     shuffle=False,\n",
    "#     drop_last=False,\n",
    "#     num_workers=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f4a24b-512a-42ac-94ed-1e55cbd9910f",
   "metadata": {},
   "source": [
    "## Preparing the model for QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0342d-0f23-4774-9781-b8c580518ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf18d84-aeba-4b05-a851-81ce6601b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "# Preparing the Model for QLoRA\n",
    "original_model = prepare_model_for_kbit_training(original_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10421cd1-4e5e-4f43-921f-10e3da731e85",
   "metadata": {},
   "source": [
    "### Setup PEFT for Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8532114c-9ea3-4e84-826f-fcd805a78fc9",
   "metadata": {},
   "source": [
    "# TODO clean up below and `peft_training_args`\n",
    "\n",
    "- So that once a model is selected, that determines a set of config values.\n",
    "- If gemma 2 9B still doesn't work with reduced parameters, try using 2B version instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0adc1-a589-43db-9b2c-08c7ac74556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For phi model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32, #Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "peft_model = get_peft_model(original_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00aec30-d5fd-4b6c-a929-11f9c0882d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For gemma 9b\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,  # Reduced rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "peft_model = get_peft_model(original_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb2b5c4-3045-461b-836a-c945ca7631bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f'trainable model parameters: {trainable_model_params}\\n \\\n",
    "            all model parameters: {all_model_params} \\n \\\n",
    "            percentage of trainable model parameters: {(trainable_model_params / all_model_params) * 100} %'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8733a19-bc4d-4ff0-a890-f74444ab4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed42d0a-38bc-476b-b01a-76ca48df27c9",
   "metadata": {},
   "source": [
    "## Train PEFT Adapter\n",
    "Define training arguments and create Trainer instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593cc8c-4721-4a26-8741-55eed0ae20e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft-radiology-training-{str(int(time.time()))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4cb1f4-8ff2-44aa-a6df-811d4288c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For phi model\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=25,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir = 'True',\n",
    "    group_by_length=True,\n",
    ")\n",
    "peft_model.config.use_cache = False\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1cdee-f54a-4f70-bc10-1c4e9cb252cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For gemma 9b\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=25,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir = 'True',\n",
    "    group_by_length=True,\n",
    ")\n",
    "peft_model.config.use_cache = False\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842fd45-6abc-4d7b-b97d-ec3a7884373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is an alternative set of training parameters\n",
    "\n",
    "# peft_training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     warmup_steps=100,  # Increased from 1\n",
    "#     per_device_train_batch_size=4,  # Increased from 1\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     max_steps=2000,  # Increased from 1000\n",
    "#     learning_rate=3e-4,  # Slightly increased\n",
    "#     optim=\"paged_adamw_8bit\",\n",
    "#     logging_steps=50,  # Adjusted\n",
    "#     logging_dir=\"./logs\",\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=50,  # Adjusted\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=50,  # Adjusted\n",
    "#     do_eval=True,\n",
    "#     gradient_checkpointing=True,\n",
    "#     report_to=\"none\",\n",
    "#     overwrite_output_dir='True',\n",
    "#     group_by_length=True,\n",
    "#     fp16=True,  # Added for mixed precision training\n",
    "#     weight_decay=0.01,  # Added for regularization\n",
    "#     lr_scheduler_type=\"cosine\",  # Added for better learning rate scheduling\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a09c7-8106-4ce2-aef2-16c1aa0eb308",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07286e9-1f0b-4567-b6a0-ef6937914cfc",
   "metadata": {},
   "source": [
    "## Load the PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c34ae-71e2-4a6c-800f-9de1fafc7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987251f5-e76d-4939-843f-bc9c5838ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(local_model_path, \n",
    "                                                  device_map=device_map,\n",
    "                                                  quantization_config=bnb_config,\n",
    "                                                  trust_remote_code=True\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762022f-db8f-4803-bbb2-b1c8849cdaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41824ef-1558-4186-a39f-efd1e40e01b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    add_bos_token=True,\n",
    "    trust_remote_code=True,\n",
    "    add_eos_token=True,\n",
    "    use_fast=False\n",
    ")\n",
    "eval_tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc08f8c-187e-49dd-a6ed-78c93ce28a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = PeftModel.from_pretrained(base_model, \"./peft-radiology-training-1725673449/checkpoint-225\",torch_dtype=torch.float16,is_trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5147f66-e939-496f-aee2-f3866ae99d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your starting text\n",
    "prompt = \"Findings: Orthogonal pelvis and orthogonal right shoulder and lateral left shoulder images\"\n",
    "\n",
    "# Generate text\n",
    "generated_texts = generate_text(ft_model, eval_tokenizer, prompt, max_new_tokens=50)\n",
    "\n",
    "# Print the generated text\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated text {i+1}:\")\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f141d1c-180f-4545-a8f8-9bd21c7f8831",
   "metadata": {},
   "source": [
    "Snippet from actual text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd46476-dd46-4125-bc8f-e2e945b373e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_strings[index][:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e26d8-0dde-4c0b-ba0b-3e42007af63c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
