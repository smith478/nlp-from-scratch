{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will explore the ideas from `EFFICIENTLY LEARNING AT TEST-TIME: ACTIVE FINE-TUNING OF LLMS` from Jonas Hubotter, Sascha Bongni, Ido Hakimi, and Andreas Krause. The paper is [here](https://arxiv.org/pdf/2410.08020) and repo is [here](https://github.com/jonhue/activeft). The idea is to fine tune at test time on a small sample of data. Of particular interest is the sample of data that is used. Rather than typical retrieval where nearest neighbors are selected from the embedding space, they introduce a more data efficient method of selecting data. This is useful not only for retraining, but also for RAG type applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import json_to_dataframe, json_to_string_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def encode_and_decode_example(list_of_strings):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Get the token ID for <|endoftext|>\n",
    "    endoftext_token = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "    all_tokens = []\n",
    "    for text in list_of_strings:\n",
    "        # Encode the text\n",
    "        encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        all_tokens.extend(encoded + [endoftext_token])\n",
    "\n",
    "    # Decode the tokens\n",
    "    decoded = tokenizer.decode(all_tokens)\n",
    "\n",
    "    return all_tokens, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, articles, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Get the token ID for <|endoftext|>\n",
    "        endoftext_token = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "        # Tokenize all articles with end-of-text token\n",
    "        all_tokens = []\n",
    "        for article in articles:\n",
    "            article_tokens = tokenizer.encode(article, allowed_special={\"<|endoftext|>\"})\n",
    "            all_tokens.extend(article_tokens + [endoftext_token])\n",
    "\n",
    "        # Use a sliding window to chunk the tokens into overlapping sequences of max_length\n",
    "        for i in range(0, len(all_tokens) - max_length, stride):\n",
    "            input_chunk = all_tokens[i:i + max_length]\n",
    "            target_chunk = all_tokens[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloader_v1(articles, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(articles, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load radiology reports dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/vector_veterinary_imaging_2.json'\n",
    "\n",
    "df = json_to_dataframe(filepath) \n",
    "rad_strings = json_to_string_list(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rad_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_strings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "max_len = 1024\n",
    "context_length = max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "token_embedding_layer = nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(rad_strings, batch_size=8, max_length=max_length, stride=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_batch(x, y, n_samples=2):\n",
    "    for i in range(min(n_samples, len(x))):\n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        \n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        \n",
    "        # Decode and print the input sequence\n",
    "        input_text = tokenizer.decode(x[i].tolist())\n",
    "        print(f\"Input text: {input_text}\")\n",
    "        print(f\"Input encoding: {x[i].tolist()}\")\n",
    "        \n",
    "        # Decode and print the target sequence\n",
    "        target_text = tokenizer.decode(y[i].tolist())\n",
    "        print(f\"Target text: {target_text}\")\n",
    "        print(f\"Target encoding: {y[i].tolist()}\")\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSPECT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "\n",
    "    if INSPECT:\n",
    "        # Visual inspection\n",
    "        inspect_batch(x, y)\n",
    "\n",
    "    token_embeddings = token_embedding_layer(x)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undirected data selection\n",
    "\n",
    "Explore using SIFT to select the most informative data without having a specific task"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
