{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a9e4bd",
   "metadata": {},
   "source": [
    "Prior to running this notebook run: `ollama serve &`. This will start the Ollama server and allow you to interact with it through this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13201b23-bfbf-448e-9598-f1fd42ef35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "from utils import json_to_dataframe, json_to_string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c3c5a-fdfa-400f-b11f-6c1c4fddcd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/vector_veterinary_imaging_2.json'\n",
    "\n",
    "df = json_to_dataframe(filepath) \n",
    "rad_strings = json_to_string_list(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b6f5e-9982-45e2-b7e3-b1363bd4c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0330b5ba-dd87-48ad-bc9e-a3cd531df25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "findings = list(df['findings'])\n",
    "conclusions = list(df['conclusions_and_recommendations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647ea8f-0fc8-459c-a97f-9808fb2c1fcc",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171c12d-74f2-4cc5-adf5-ad6627298d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformerRetriever(dspy.Retrieve):\n",
    "    def __init__(self, model: str, findings: List[str], conclusions: List[str], k: int):\n",
    "        self.model = model if isinstance(model, SentenceTransformer) else SentenceTransformer(model, trust_remote_code=True)\n",
    "        self.findings = findings\n",
    "        self.conclusions = conclusions\n",
    "        self.k = k\n",
    "        self.embeddings = None\n",
    "        self.init_embeddings()\n",
    "\n",
    "    def init_embeddings(self):\n",
    "        self.embeddings = self.model.encode(self.findings)\n",
    "\n",
    "    def forward(self, query: str, k: int) -> List[Dict[str, Union[str, float]]]:\n",
    "        query_embedding = self.model.encode([query])\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_k_indices:\n",
    "            results.append({\n",
    "                'finding': self.findings[idx],\n",
    "                'conclusion': self.conclusions[idx],\n",
    "                'score': float(similarities[idx])\n",
    "            })\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99c231-1c02-4a25-bd49-0ea166144e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# vectorizer = \"dunzhang/stella_en_400M_v5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d5901-0986-427e-b884-e7c906cd3f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_model = SentenceTransformerRetriever(model=vectorizer, findings=findings, conclusions=conclusions, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e99355-5d3f-420b-b401-679a33e41651",
   "metadata": {},
   "outputs": [],
   "source": [
    "findings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a33d5c-cca5-4211-93b0-55405fb01539",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_model.forward(query=findings[1], k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13277c0a-95f5-4272-bfc4-46fcedf5f66f",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4157dbf-a735-4d12-aeed-dce0b6b1a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = \"qwen2.5\"\n",
    "# language_model = \"gemma2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c62e8e-3df7-466e-9621-5bcbb0c9c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = dspy.OllamaLocal(\n",
    "    base_url='http://127.0.0.1:11434',\n",
    "    timeout_s=500,\n",
    "    model=language_model,\n",
    "    model_type='text',\n",
    "    max_tokens=1024,\n",
    "    num_ctx=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3dfda-52ca-4312-b5a6-8d014796c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DSPy to use Ollama\n",
    "dspy.settings.configure(lm=ollama_model, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd80d22-4095-411b-9ac8-d22bc325ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateConclusions(dspy.Signature):\n",
    "    \"\"\"Given a radiology finding and similar examples, generate an appropriate conclusions and recommendations section.\n",
    "    The response should maintain a professional medical tone and follow the style of the examples.\"\"\"\n",
    "\n",
    "    finding = dspy.InputField(desc=\"Findings section of the radiology report.\")\n",
    "    similar_examples = dspy.InputField(desc=\"Similar examples of findings and corresponding conclusions and recommendations sections.\")\n",
    "    conclusions = dspy.OutputField(desc=\"The conclusions and recommendations section. Give the findings section above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92488ee-81ba-4e39-a514-9faa83ee234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadiologyModule(dspy.Module):\n",
    "    def __init__(self, retriever):\n",
    "        super().__init__()\n",
    "        self.generate_conclusion = dspy.Predict(GenerateConclusions)\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def forward(self, finding: str) -> dict:\n",
    "        # Retrieve similar examples\n",
    "        retrieved = self.retriever(finding, k=3)\n",
    "        \n",
    "        # Format examples for prompt\n",
    "        examples_text = \"\"\n",
    "        for i, ex in enumerate(retrieved, 1):\n",
    "            examples_text += f\"Example {i}:\\n\"\n",
    "            examples_text += f\"Finding: {ex['finding']}\\n\"\n",
    "            examples_text += f\"Conclusion: {ex['conclusion']}\\n\\n\"\n",
    "\n",
    "        # Generate new conclusion\n",
    "        prediction = self.generate_conclusion(\n",
    "            finding=finding,\n",
    "            similar_examples=examples_text\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'finding': finding,\n",
    "            'generated_conclusion': prediction.conclusions,\n",
    "            'similar_examples': retrieved\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab50c82-0b84-4ca6-bb49-beec9c71f26b",
   "metadata": {},
   "source": [
    "## Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8060bf-a509-4bba-932f-d8b1526bd1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rad_pipeline(filepath: str, vectorizer: str = \"sentence-transformers/all-MiniLM-L6-v2\", k: int = 3):\n",
    "    \"\"\"\n",
    "    Set up the complete radiology report generation pipeline\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = json_to_dataframe(filepath)\n",
    "    findings = list(df['findings'])\n",
    "    conclusions = list(df['conclusions_and_recommendations'])\n",
    "    \n",
    "    # Initialize retriever\n",
    "    retriever = SentenceTransformerRetriever(\n",
    "        model=vectorizer,\n",
    "        findings=findings,\n",
    "        conclusions=conclusions,\n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    # Create and return the radiology module\n",
    "    return RadiologyModule(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff638ac-cba8-48c5-9dcd-7b09ea02962d",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a094450-dd96-4a65-a258-2a18682e09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/vector_veterinary_imaging_2.json'\n",
    "rad_pipeline = setup_rad_pipeline(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a23e4e-2355-44dc-b1ee-4648e55f1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example finding\n",
    "test_finding = \"\"\"\n",
    "The thoracic cavity demonstrates normal cardiac silhouette size and shape. \n",
    "The pulmonary vasculature appears within normal limits. \n",
    "There is a mild interstitial pattern noted in the caudodorsal lung fields.\n",
    "No evidence of pleural effusion is noted.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4301a2ff-6e89-4fe9-9e31-2798d819c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rad_pipeline(test_finding)\n",
    "    \n",
    "print(\"Generated Conclusion:\")\n",
    "print(result['generated_conclusion'])\n",
    "print(\"\\nSimilar Examples Used:\")\n",
    "for i, example in enumerate(result['similar_examples'], 1):\n",
    "    print(f\"\\nExample {i} (Similarity Score: {example['score']:.3f}):\")\n",
    "    print(f\"Finding: {example['finding']}\")\n",
    "    print(f\"Conclusion: {example['conclusion']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da984fd5-2079-48fc-8991-fc91428a3c15",
   "metadata": {},
   "source": [
    "## TODO \n",
    "\n",
    "As we go through a handful of examples, we want to make sure we don't include the example itself in the retrieval set. But it is fine to include all other examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7aa8fc-9d28-4351-9336-f4848b6b2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Dict, Union, Optional\n",
    "import pandas as pd\n",
    "\n",
    "class SentenceTransformerRetrieverWithExclusion(dspy.Retrieve):\n",
    "    def __init__(self, model: str, findings: List[str], conclusions: List[str], k: int):\n",
    "        self.model = model if isinstance(model, SentenceTransformer) else SentenceTransformer(model, trust_remote_code=True)\n",
    "        self.findings = findings\n",
    "        self.conclusions = conclusions\n",
    "        self.k = k\n",
    "        self.embeddings = None\n",
    "        self.excluded_indices = set()\n",
    "        self.init_embeddings()\n",
    "\n",
    "    def init_embeddings(self):\n",
    "        self.embeddings = self.model.encode(self.findings)\n",
    "        \n",
    "    def set_excluded_indices(self, indices: Optional[List[int]] = None):\n",
    "        \"\"\"Set indices to exclude from retrieval\"\"\"\n",
    "        self.excluded_indices = set(indices or [])\n",
    "        \n",
    "    def clear_excluded_indices(self):\n",
    "        \"\"\"Clear all excluded indices\"\"\"\n",
    "        self.excluded_indices = set()\n",
    "\n",
    "    def forward(self, query: str, k: int) -> List[Dict[str, Union[str, float]]]:\n",
    "        query_embedding = self.model.encode([query])\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Create mask for excluded indices\n",
    "        mask = np.ones_like(similarities, dtype=bool)\n",
    "        if self.excluded_indices:\n",
    "            mask[list(self.excluded_indices)] = False\n",
    "        \n",
    "        # Get top k indices excluding masked indices\n",
    "        masked_similarities = similarities.copy()\n",
    "        masked_similarities[~mask] = -np.inf\n",
    "        top_k_indices = np.argsort(masked_similarities)[-k:][::-1]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_k_indices:\n",
    "            results.append({\n",
    "                'finding': self.findings[idx],\n",
    "                'conclusion': self.conclusions[idx],\n",
    "                'score': float(similarities[idx])\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "def setup_rad_pipeline_with_exclusion(filepath: str, vectorizer: str = \"sentence-transformers/all-MiniLM-L6-v2\", k: int = 3):\n",
    "    \"\"\"\n",
    "    Set up the radiology pipeline with exclusion capability\n",
    "    \"\"\"\n",
    "    df = json_to_dataframe(filepath)\n",
    "    findings = list(df['findings'])\n",
    "    conclusions = list(df['conclusions_and_recommendations'])\n",
    "    \n",
    "    retriever = SentenceTransformerRetrieverWithExclusion(\n",
    "        model=vectorizer,\n",
    "        findings=findings,\n",
    "        conclusions=conclusions,\n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    return RadiologyModule(retriever), df\n",
    "\n",
    "def run_evaluation_with_exclusion(filepath: str, num_examples: int = 5, seed: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Run inference on a specified number of examples, excluding each example from its own retrieval set\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the JSON data file\n",
    "        num_examples: Number of examples to evaluate\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing the evaluation results\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        \n",
    "    # Setup pipeline with exclusion capability\n",
    "    rad_pipeline, df = setup_rad_pipeline_with_exclusion(filepath)\n",
    "    \n",
    "    # Randomly select examples\n",
    "    total_examples = len(df)\n",
    "    selected_indices = random.sample(range(total_examples), min(num_examples, total_examples))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx in selected_indices:\n",
    "        # Get the example\n",
    "        finding = df['findings'].iloc[idx]\n",
    "        actual_conclusion = df['conclusions_and_recommendations'].iloc[idx]\n",
    "        \n",
    "        # Set the current example to be excluded from retrieval\n",
    "        rad_pipeline.retriever.set_excluded_indices([idx])\n",
    "        \n",
    "        # Run inference\n",
    "        result = rad_pipeline(finding)\n",
    "        \n",
    "        # Clear exclusion for next iteration\n",
    "        rad_pipeline.retriever.clear_excluded_indices()\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'index': idx,\n",
    "            'finding': finding,\n",
    "            'actual_conclusion': actual_conclusion,\n",
    "            'generated_conclusion': result['generated_conclusion'],\n",
    "            'similar_examples': result['similar_examples']\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = '../../data/vector_veterinary_imaging_2.json'\n",
    "    \n",
    "    # Run evaluation on 5 random examples\n",
    "    results_df = run_evaluation_with_exclusion(filepath, num_examples=5, seed=42)\n",
    "    \n",
    "    # Print results\n",
    "    for idx, row in results_df.iterrows():\n",
    "        print(f\"\\nExample {idx + 1}:\")\n",
    "        print(\"Finding:\")\n",
    "        print(row['finding'])\n",
    "        print(\"\\nActual Conclusion:\")\n",
    "        print(row['actual_conclusion'])\n",
    "        print(\"\\nGenerated Conclusion:\")\n",
    "        print(row['generated_conclusion'])\n",
    "        print(\"\\nSimilar Examples Used:\")\n",
    "        for i, example in enumerate(row['similar_examples'], 1):\n",
    "            print(f\"\\nReference {i} (Similarity Score: {example['score']:.3f}):\")\n",
    "            print(f\"Finding: {example['finding']}\")\n",
    "            print(f\"Conclusion: {example['conclusion']}\")\n",
    "        print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
