{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to focus on just the generative portion of the model and test new models as they are available. We will start by testing Marco o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_examples(data: List[Dict], index: int) -> Tuple[str, List[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Extract the base finding and reference examples from the evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries containing the evaluation results\n",
    "        index: Index of the item to extract\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - Base finding string\n",
    "            - List of tuples, each containing (finding, conclusion) pairs from reference examples\n",
    "    \"\"\"\n",
    "    item = data[index]\n",
    "    base_finding = item['finding']\n",
    "    \n",
    "    reference_examples = [\n",
    "        (example['finding'], example['conclusion'])\n",
    "        for example in item['reference_examples']\n",
    "    ]\n",
    "    \n",
    "    return base_finding, reference_examples\n",
    "\n",
    "def create_prompt(\n",
    "    base_finding: str,\n",
    "    reference_examples: List[Tuple[str, str]],\n",
    "    base_prompt: Optional[str] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a formatted prompt combining the base finding and reference examples.\n",
    "    \n",
    "    Args:\n",
    "        base_finding: The primary finding to generate conclusions for\n",
    "        reference_examples: List of (finding, conclusion) pairs to use as examples\n",
    "        base_prompt: Optional custom instructions for the prompt\n",
    "        \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    if base_prompt is None:\n",
    "        base_prompt = \"\"\"Please generate the \"Conclusions\" section of a medical report based on the provided \"Findings\" section. \n",
    "Below are several example pairs of \"Findings\" and their corresponding \"Conclusions\" to help guide the format and style of your response.\n",
    "\n",
    "After reviewing these examples, please generate an appropriate conclusion for the new finding provided at the end.\"\"\"\n",
    "\n",
    "    # Format reference examples\n",
    "    examples_text = \"\\n\\n\".join([\n",
    "        f\"Example {i+1}:\\n\"\n",
    "        f\"Findings:\\n{finding}\\n\\n\"\n",
    "        f\"Conclusion:\\n{conclusion}\"\n",
    "        for i, (finding, conclusion) in enumerate(reference_examples)\n",
    "    ])\n",
    "    \n",
    "    # Combine all parts\n",
    "    full_prompt = f\"\"\"{base_prompt}\n",
    "\n",
    "{examples_text}\n",
    "\n",
    "New Finding to generate conclusion for:\n",
    "{base_finding}\"\"\"\n",
    "    \n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"evaluation_results.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_finding, reference_examples = extract_examples(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = create_prompt(base_finding, reference_examples)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ollama_inference(\n",
    "    prompt: str,\n",
    "    model: str = \"mistral\",\n",
    "    system_prompt: Optional[str] = None,\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: Optional[int] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run inference using an Ollama model.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The input text to process\n",
    "        model: Name of the Ollama model to use (default: \"mistral\")\n",
    "        system_prompt: Optional system prompt to set context\n",
    "        temperature: Sampling temperature (default: 0.7)\n",
    "        max_tokens: Maximum tokens to generate (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Generated text response\n",
    "        \n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If the API call fails\n",
    "    \"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    if system_prompt:\n",
    "        payload[\"system\"] = system_prompt\n",
    "    if max_tokens:\n",
    "        payload[\"max_tokens\"] = max_tokens\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Failed to get response from Ollama: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an AI radiology assistant, helping to generate conclusions from findings in radiology reports. Please make sure that all output is in english.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = run_ollama_inference(prompt=prompt, model='marco-o1', system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative implementation using the ollama package\n",
    "def run_ollama_client(\n",
    "    prompt: str,\n",
    "    model: str = \"mistral\",\n",
    "    system_prompt: Optional[str] = None,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run inference using the Ollama client package.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The input text to process\n",
    "        model: Name of the Ollama model to use (default: \"mistral\")\n",
    "        system_prompt: Optional system prompt to set context\n",
    "        temperature: Sampling temperature (default: 0.7)\n",
    "        \n",
    "    Returns:\n",
    "        Generated text response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import ollama\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install ollama package: pip install ollama\")\n",
    "    \n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            system=system_prompt if system_prompt else None,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response['response']\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to get response from Ollama: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
