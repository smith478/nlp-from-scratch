{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ea0a9-ff9c-4604-8dfb-b60cb5302f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5897ae6f-e9c8-49a4-8daf-75cac3a39ad9",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a459825-a810-4341-9c33-88e3ccff93a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45df8ff-d3d7-4db6-8d76-cbe735e3eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_dir, \"google_gemma-2-2b-it_model_labeled_20240925.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08164412-7402-499f-aaad-2d6d46dcad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f87b4-a62b-4b95-8fba-73d98a916b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3caafe-e007-4c9f-a61a-dec0dc262057",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'case_identifier', \n",
    "    'findings', \n",
    "    'conclusions_and_recommendations', \n",
    "    'pulmonary edema', \n",
    "    'consolidation', \n",
    "    'pleural effusion', \n",
    "    'pneumothorax',\n",
    "    'cardiomegaly'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3555ddb-da4f-46d3-876e-31f96daf1cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b7acd-488f-4d00-a9db-0ca27458c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429cdb57-d013-41b0-a73d-3c2628921fc1",
   "metadata": {},
   "source": [
    "## Get dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69134f98-90a4-4d9f-80f0-da1ddf2789ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormalities = ['pulmonary edema', 'consolidation', 'pleural effusion', 'pneumothorax', 'cardiomegaly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b114fc-abd2-4774-bd45-b3291a848de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum (count) for each abnormality\n",
    "abnormality_counts = df[abnormalities].sum()\n",
    "\n",
    "# Display the results\n",
    "print(abnormality_counts)\n",
    "\n",
    "# Calculate the percentage of reports with each abnormality\n",
    "total_reports = len(df)\n",
    "abnormality_percentages = (abnormality_counts / total_reports) * 100\n",
    "\n",
    "# Display the percentages\n",
    "print(\"\\nPercentage of reports with each abnormality:\")\n",
    "print(abnormality_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa785f-77a0-4775-be0e-2cb127ef2815",
   "metadata": {},
   "source": [
    "## Create data loaders\n",
    "\n",
    "- Note that the text messages have different lengths; if we want to combine multiple training examples in a batch, we have to either\n",
    "  1. truncate all messages to the length of the shortest message in the dataset or batch\n",
    "  2. pad all messages to the length of the longest message in the dataset or batch\n",
    "\n",
    "- We choose option 2 and pad all messages to the longest message in the dataset\n",
    "- For that, we use `<|endoftext|>` as a padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c1555a-3b87-4813-868a-97aabaeaa3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    # Shuffle the entire DataFrame\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # Split the DataFrame\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(df, 0.7, 0.1)\n",
    "# Test size is implied to be 0.2 as the remainder\n",
    "\n",
    "train_df.to_csv(os.path.join(data_dir, \"train.csv\"), index=None)\n",
    "validation_df.to_csv(os.path.join(data_dir, \"validation.csv\"), index=None)\n",
    "test_df.to_csv(os.path.join(data_dir, \"test.csv\"), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd828c52-e42f-42ef-8d66-fc1992f4ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f728e88-e3ad-4f91-ac20-bad1105dc13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadiologyDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.abnormalities = ['pulmonary edema', 'consolidation', 'pleural effusion', 'pneumothorax', 'cardiomegaly']\n",
    "\n",
    "        # Convert abnormality columns to numeric type\n",
    "        for abnormality in self.abnormalities:\n",
    "            self.data[abnormality] = pd.to_numeric(self.data[abnormality], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"conclusions_and_recommendations\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # Truncate sequences if they are longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # Pad sequences to the longest sequence\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        labels = self.data.iloc[index][self.abnormalities].values.astype(np.float32)\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(labels, dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14a5ec-9dd3-4607-809d-740efb36c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RadiologyDataset(\n",
    "    csv_file=os.path.join(data_dir, \"train.csv\"),\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9f9f7-a738-42b9-975c-316969ed7cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = RadiologyDataset(\n",
    "    csv_file=os.path.join(data_dir, \"validation.csv\"),\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = RadiologyDataset(\n",
    "    csv_file=os.path.join(data_dir, \"test.csv\"),\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc65f84-f972-4732-86b5-326f353a90c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50d795a-9338-40c3-80c1-b695de59f4c4",
   "metadata": {},
   "source": [
    "- As a verification step, we iterate through the data loaders and ensure that the batches contain 8 training examples each, where each training example consists of 525 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706b6bf9-2774-43bf-9d22-a21427c9b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions:\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa11ba-2cd1-475d-aa69-cab3c231edc2",
   "metadata": {},
   "source": [
    "- Lastly, let's print the total number of batches in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91239f5-48a1-4789-a469-979e032b2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e9eba-1a5b-4f05-b4d5-33fc3ac982d9",
   "metadata": {},
   "source": [
    "# Define GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4837c32e-18c7-4019-8d76-4dc54122e1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
