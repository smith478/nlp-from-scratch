# BERT From Scratch

Here we will give instructions on building BERT from scratch. For the task we will focus on masked language modeling (MLM). It has been observed (RoBERTa?) that MLM is more effective than next sentence prediction (NSP) for pretraining. In fact it seems that NSP may hurt the performance of the model.