{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b60dd-20ce-4b6e-bc76-7b214c57daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1bf16-b05b-4d25-9981-af60a5ab220f",
   "metadata": {},
   "source": [
    "If we look at the weights of GPT-2 we can see the token and position embedding and we can see that it has a vocab size of 50257 and a context length of 1024 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd6e49-3a60-4032-a478-167bc4287775",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M #1.5B you need to use gpt2-xl\n",
    "sd_hf = model_hf.state_dict()\n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770e062-0644-4252-b162-b694d6560a3a",
   "metadata": {},
   "source": [
    "Let's look at the first few positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2af130-f5b8-4734-a1ba-01f74879f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_hf[\"transformer.wpe.weight\"].view(-1)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86adea0b-79f9-4c8f-aee2-1a44148941de",
   "metadata": {},
   "source": [
    "Next we can plot them. Every row represents a fixed position in our context window from 0 to 1023. The model uses these to understand the relative positions of the tokens and attend to them depending on their position, not just their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e67b18-8199-4032-8921-ab48c49d65e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(sd_hf[\"transformer.wpe.weight\"], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ff8d9-44c0-4b22-9aa8-2941e4eaab0f",
   "metadata": {},
   "source": [
    "When we look into an individual columns we can see how they react to different positions. You can see that the green channel becomes more active for positions more in the middle (above ~250 and below ~800). The fact that they're more jagged indicates that the model is not fully trained. After the model has been more trained, you would expect these to be more smooth.\n",
    "\n",
    "Note that in the original transformer paper the positional embedding weights were fixed using sin and cosine curves of different frequencies, however in GPT-2 they are learned weights. It is interesting that they recover these periodic wave like structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc8d87-f046-4673-83c9-329d345e673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 150])\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 200])\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7f58c-263d-4e14-bcab-abd3e73a6bd5",
   "metadata": {},
   "source": [
    "We can visualize any of the other weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed8fac-4a85-49e1-aa56-8a11b2891a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sd_hf[\"transformer.h.1.attn.c_attn.weight\"][:300,:300], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9560018b-deca-4536-b98f-d9bc602c3dd5",
   "metadata": {},
   "source": [
    "Our main interest here is to play with inference on the model with the weights that we loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef232033-4e80-4043-95e5-cc7f6d2422ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850bdff2-343e-4290-8321-cf3b8b521586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
