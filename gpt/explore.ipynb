{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b60dd-20ce-4b6e-bc76-7b214c57daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1bf16-b05b-4d25-9981-af60a5ab220f",
   "metadata": {},
   "source": [
    "If we look at the weights of GPT-2 we can see the token and position embedding and we can see that it has a vocab size of 50257 and a context length of 1024 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd6e49-3a60-4032-a478-167bc4287775",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M #1.5B you need to use gpt2-xl\n",
    "sd_hf = model_hf.state_dict()\n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770e062-0644-4252-b162-b694d6560a3a",
   "metadata": {},
   "source": [
    "Let's look at the first few positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2af130-f5b8-4734-a1ba-01f74879f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_hf[\"transformer.wpe.weight\"].view(-1)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86adea0b-79f9-4c8f-aee2-1a44148941de",
   "metadata": {},
   "source": [
    "Next we can plot them. Every row represents a fixed position in our context window from 0 to 1023. The model uses these to understand the relative positions of the tokens and attend to them depending on their position, not just their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e67b18-8199-4032-8921-ab48c49d65e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(sd_hf[\"transformer.wpe.weight\"], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ff8d9-44c0-4b22-9aa8-2941e4eaab0f",
   "metadata": {},
   "source": [
    "When we look into an individual columns we can see how they react to different positions. You can see that the green channel becomes more active for positions more in the middle (above ~250 and below ~800). The fact that they're more jagged indicates that the model is not fully trained. After the model has been more trained, you would expect these to be more smooth.\n",
    "\n",
    "Note that in the original transformer paper the positional embedding weights were fixed using sin and cosine curves of different frequencies, however in GPT-2 they are learned weights. It is interesting that they recover these periodic wave like structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc8d87-f046-4673-83c9-329d345e673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 150])\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 200])\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7f58c-263d-4e14-bcab-abd3e73a6bd5",
   "metadata": {},
   "source": [
    "We can visualize any of the other weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed8fac-4a85-49e1-aa56-8a11b2891a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sd_hf[\"transformer.h.1.attn.c_attn.weight\"][:300,:300], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9560018b-deca-4536-b98f-d9bc602c3dd5",
   "metadata": {},
   "source": [
    "Our main interest here is to play with inference on the model with the weights that we loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef232033-4e80-4043-95e5-cc7f6d2422ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850bdff2-343e-4290-8321-cf3b8b521586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's instead sample manually\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M\n",
    "model.eval()\n",
    "# model.to('cuda')\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "tokens = [15496, 11, 314, 1101, 257, 3303, 2746, 11] # \"Hello, I'm a language model,\"\n",
    "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
    "tokens = tokens.unsqueeze(0).repeat(5, 1) # (5, 8)\n",
    "# x = tokens.to('cuda')\n",
    "x = tokens.to(device)\n",
    "\n",
    "# generate!\n",
    "while x.size(1) < 30: # max_length=30\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)[0] # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# print the generated text\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "for i in range(5):\n",
    "    tokens = x[i, :30].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b03ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "data = text[:1000] # first 1,000 characters\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3148f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(data)\n",
    "print(tokens[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "buf = torch.tensor(tokens[:24 + 1])\n",
    "x = buf[:-1].view(4, 6)\n",
    "y = buf[1:].view(4, 6)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e6cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "label-extractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
