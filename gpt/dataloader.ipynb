{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be713d74-b151-4ec4-aa30-a5fa04890613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4df9e-67f8-480d-a831-390a6f761a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "##############################\n",
    "# Download data if necessary\n",
    "##############################\n",
    "\n",
    "file_path = \"../data/the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad3ea0c-2b59-4728-b6d4-569cb76ff4cc",
   "metadata": {},
   "source": [
    "## Data loader for txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a34f5-e132-485c-9fb2-6d1460dd74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded_text = tokenizer.encode(raw_text)\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "context_length = 1024\n",
    "\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44acd5e7-4c95-4d04-b32d-202f58f76c0e",
   "metadata": {},
   "source": [
    "## Data loader for list of strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026566a-85d3-48ca-a970-12495e184106",
   "metadata": {},
   "source": [
    "Test end of text splitting between different articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061a8b8-8f06-4313-bb20-dc9358a76a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_decode_example(list_of_strings):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Get the token ID for <|endoftext|>\n",
    "    endoftext_token = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "    all_tokens = []\n",
    "    for text in list_of_strings:\n",
    "        # Encode the text\n",
    "        encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        all_tokens.extend(encoded + [endoftext_token])\n",
    "\n",
    "    # Decode the tokens\n",
    "    decoded = tokenizer.decode(all_tokens)\n",
    "\n",
    "    return all_tokens, decoded\n",
    "\n",
    "# Test the function\n",
    "string_sample = ['this is the first passage', 'this is the second']\n",
    "tokens, decoded = encode_and_decode_example(string_sample)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Decoded text:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd82fa2-0478-48a3-a091-054eaa5276b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, articles, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Get the token ID for <|endoftext|>\n",
    "        endoftext_token = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "        # Tokenize all articles with end-of-text token\n",
    "        all_tokens = []\n",
    "        for article in articles:\n",
    "            article_tokens = tokenizer.encode(article, allowed_special={\"<|endoftext|>\"})\n",
    "            all_tokens.extend(article_tokens + [endoftext_token])\n",
    "\n",
    "        # Use a sliding window to chunk the tokens into overlapping sequences of max_length\n",
    "        for i in range(0, len(all_tokens) - max_length, stride):\n",
    "            input_chunk = all_tokens[i:i + max_length]\n",
    "            target_chunk = all_tokens[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2e77d-aedd-453f-9a60-93b58d99fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_dataloader_v1(articles, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(articles, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "# Usage:\n",
    "# articles = ['this is the first article', 'this is the second article']\n",
    "# dataloader = create_dataloader_v1(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19257b47-ff66-4a7c-a4da-00c6a0f4fc70",
   "metadata": {},
   "source": [
    "## Load in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8615c-4295-4f74-9a89-efc003aca2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e24f1-9ca5-4eb1-ab38-7aaf3ec0766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sgm_to_dataframe(file_path: str) -> pd.DataFrame:\n",
    "    # Open and read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sgm_data = file.read()\n",
    "\n",
    "    # Parse the SGML data\n",
    "    soup = BeautifulSoup(sgm_data, 'html.parser')\n",
    "\n",
    "    # List to hold parsed data\n",
    "    data = []\n",
    "\n",
    "    # Iterate over each Reuters tag in the SGML\n",
    "    for reuters in soup.find_all('reuters'):\n",
    "        # Extract the NEWID attribute to serve as an ID\n",
    "        article_id = reuters.get('newid')\n",
    "\n",
    "        # Extract the BODY content\n",
    "        body = reuters.find('body')\n",
    "        body_text = body.get_text().strip() if body else ''\n",
    "\n",
    "        # Extract the TOPICS\n",
    "        topics = reuters.find('topics')\n",
    "        if topics:\n",
    "            # Get all topics listed under <D> tags\n",
    "            topics_list = [d.get_text().strip() for d in topics.find_all('d')]\n",
    "            # If there are topics, add a row for each topic\n",
    "            if topics_list:\n",
    "                for topic in topics_list:\n",
    "                    data.append({'ID': article_id, 'Topic': topic, 'Body': body_text})\n",
    "            else:\n",
    "                # If <topics> tag exists but is empty, add a row with empty string for Topic\n",
    "                data.append({'ID': article_id, 'Topic': '', 'Body': body_text})\n",
    "        else:\n",
    "            # If there's no <topics> tag, add a row with None for Topic\n",
    "            data.append({'ID': article_id, 'Topic': None, 'Body': body_text})\n",
    "\n",
    "    # Create a DataFrame from the parsed data\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7848d23-aa7d-48ae-a51f-9e86ca5788f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_sgm_to_dataframe('../data/reuters21578/reut2-000.sgm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006dd220-cf2a-4ba1-8b5a-b1f42c331c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = list(df['Body'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b21f4a-3772-49a2-873f-9fb223d71e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeae4dd-a932-4789-9cae-da4b106f625b",
   "metadata": {},
   "source": [
    "## Create data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a384ac4-bac8-487a-a67c-0091efa044d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "max_len = 1024\n",
    "context_length = max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34dc43c-e3bb-4922-adb9-fbb01d3c5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "token_embedding_layer = nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8bfc00-fd52-463e-a522-0c6d950eae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(articles, batch_size=8, max_length=max_length, stride=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da6bb9-2e2b-4f2f-834b-1e78029340ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_batch(x, y, n_samples=2):\n",
    "    for i in range(min(n_samples, len(x))):\n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        \n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        \n",
    "        # Decode and print the input sequence\n",
    "        input_text = tokenizer.decode(x[i].tolist())\n",
    "        print(f\"Input text: {input_text}\")\n",
    "        print(f\"Input encoding: {x[i].tolist()}\")\n",
    "        \n",
    "        # Decode and print the target sequence\n",
    "        target_text = tokenizer.decode(y[i].tolist())\n",
    "        print(f\"Target text: {target_text}\")\n",
    "        print(f\"Target encoding: {y[i].tolist()}\")\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a2204-8a2d-47b8-9a4b-5aafc6f752ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSPECT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2000607-0b23-40b1-9180-a98226d0ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "\n",
    "    if INSPECT:\n",
    "        # Visual inspection\n",
    "        inspect_batch(x, y)\n",
    "\n",
    "    token_embeddings = token_embedding_layer(x)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1144cc-bd61-48e6-a389-c0903fae6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
